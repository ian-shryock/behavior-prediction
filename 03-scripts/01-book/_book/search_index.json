[["index.html", "Idiographic prediction of loneliness and procrastination Chapter 1 Workspace 1.1 Packages 1.2 Directory Path 1.3 Codebook 1.4 Demographics", " Idiographic prediction of loneliness and procrastination Emorie D Beck 2021-05-25 Chapter 1 Workspace 1.1 Packages library(knitr) # creating tables library(kableExtra) # formatting and exporting tables library(readxl) # read excel codebooks and documentation library(psych) # biscuit / biscwit library(glmnet) # elastic net regression library(glmnetUtils) # extension of basic elastic net with CV library(caret) # train and test for random forest library(vip) # variable importance library(Amelia) # multiple imputation (of time series) library(lubridate) # date wrangling library(gtable) # ggplot friendly tables library(grid) # ggplot friendly table rendering library(gridExtra) # more helpful ggplot friendly table updates library(plyr) # data wranging library(tidyverse) # data wrangling library(ggdist) # distributional plots library(ggridges) # more distributional plots library(cowplot) # flexibly arrange multiple ggplot objects library(tidymodels) # tidy model workflow and selection library(modeltime) # tidy models for time series library(furrr) # mapping many models in parallel 1.2 Directory Path # res_path &lt;- &quot;https://github.com/emoriebeck/big-five-prediction/blob/master&quot; res_path &lt;- &quot;~/Box/network/other projects/idio prediction&quot; 1.3 Codebook Each study has a separate codebook indexing matching, covariate, personality, and outcome variables. Moreover, these codebooks contain information about the original scale of the variable, any recoding of the variable (including binarizing outcomes, changing the scale, and removing missing data), reverse coding of scale variables, categories, etc. # list of all codebook sheets sheets &lt;- sprintf(&quot;%s/01-codebooks/codebook.xlsx&quot;, res_path) %&gt;% excel_sheets() # function for reading in sheets read_fun &lt;- function(x){ sprintf(&quot;%s/01-codebooks/codebook.xlsx&quot;, res_path) %&gt;% read_xlsx(., sheet = x) } # read in sheets and index source codebook &lt;- tibble( sheet = sheets, codebook = map(sheet, read_fun) ) ipcs_codebook &lt;- (codebook %&gt;% filter(sheet == &quot;codebook&quot;))$codebook[[1]] ipcs_codebook ## # A tibble: 98 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BFI-2 E scblty Sociabil… Was out… Liker… 1. Is outgoin… E1 no Extravers… ## 2 BFI-2 E scblty Sociabil… Was tal… Liker… 46. Is talkati… E2 no Extravers… ## 3 BFI-2 E scblty Sociabil… Tended … Liker… 16r. Tends to b… E3 yes Extravers… ## 4 BFI-2 E scblty Sociabil… Was som… Liker… 31r. Is sometim… E4 yes Extravers… ## 5 BFI-2 E assert Assertiv… Had an … Liker… 6. Has an ass… E5 no Extravers… ## 6 BFI-2 E assert Assertiv… Was dom… Liker… 21. Is dominan… E6 no Extravers… ## 7 BFI-2 E assert Assertiv… Found i… Liker… 36r. Finds it h… E7 yes Extravers… ## 8 BFI-2 E assert Assertiv… Preferr… Liker… 51r. Prefers to… E8 yes Extravers… ## 9 BFI-2 E enerL… Energy L… Was ful… Liker… 41. Is full of… E9 no Extravers… ## 10 BFI-2 E enerL… Energy L… Showed … Liker… 56. Shows a lo… E10 no Extravers… ## # … with 88 more rows, and 1 more variable: long_name &lt;chr&gt; outcomes &lt;- ipcs_codebook %&gt;% filter(category == &quot;outcome&quot;) %&gt;% select(trait, long_name) ftrs &lt;- codebook$codebook[[3]] 1.3.1 Measures Participants responded to a large battery of trait and ESM measures as part of the larger study. The present study focuses on ESM measures whose use we preregistered. A full list of the collected measures for the study can be found in supplementary codebooks in the online materials on the OSF and GitHub. The measures collected at each wave were identical. ESM measures were used to estimate idiographic personality prediction models. 1.3.1.1 ESM Measures 1.3.1.1.1 Personality Personality was assessed using the full BFI-2 (Soto &amp; John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 “disagree strongly” to 5 “agree strongly.” Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants. ipcs_codebook %&gt;% filter(category == &quot;BFI-2&quot;) ## # A tibble: 60 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BFI-2 E scblty Sociabil… Was out… Liker… 1. Is outgoin… E1 no Extravers… ## 2 BFI-2 E scblty Sociabil… Was tal… Liker… 46. Is talkati… E2 no Extravers… ## 3 BFI-2 E scblty Sociabil… Tended … Liker… 16r. Tends to b… E3 yes Extravers… ## 4 BFI-2 E scblty Sociabil… Was som… Liker… 31r. Is sometim… E4 yes Extravers… ## 5 BFI-2 E assert Assertiv… Had an … Liker… 6. Has an ass… E5 no Extravers… ## 6 BFI-2 E assert Assertiv… Was dom… Liker… 21. Is dominan… E6 no Extravers… ## 7 BFI-2 E assert Assertiv… Found i… Liker… 36r. Finds it h… E7 yes Extravers… ## 8 BFI-2 E assert Assertiv… Preferr… Liker… 51r. Prefers to… E8 yes Extravers… ## 9 BFI-2 E enerL… Energy L… Was ful… Liker… 41. Is full of… E9 no Extravers… ## 10 BFI-2 E enerL… Energy L… Showed … Liker… 56. Shows a lo… E10 no Extravers… ## # … with 50 more rows, and 1 more variable: long_name &lt;chr&gt; 1.3.1.1.2 Affect Items capturing affect were initially pulled from the PANAS-X (Watson &amp; Clark, 1994). In order to reduce redundancy, these were cross-referenced with the BFI-2 and duplicated items (e.g., “excited” were only asked once. Because we were not interested in scale score but in items, we further had research participants examine remaining items and asked them to indicate items that were not relevant to their experience. Finally, we added two “neutral” affect-related terms – goal-directed and purposeful. Each of these were rated on a 1 “disagree strongly” to 5 “agree strongly.” ipcs_codebook %&gt;% filter(category == &quot;Affect&quot;) ## # A tibble: 10 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Affect angry angry angry Angry Likert… &lt;NA&gt; &lt;NA&gt; A1 no Angry ## 2 Affect afraid afra… afraid Afraid Likert… &lt;NA&gt; &lt;NA&gt; A3 no Afraid ## 3 Affect happy happy happy Happy Likert… &lt;NA&gt; &lt;NA&gt; A5 no Happy ## 4 Affect excit… exci… excited Excited Likert… &lt;NA&gt; &lt;NA&gt; A7 no Excited ## 5 Affect proud proud proud Proud Likert… &lt;NA&gt; &lt;NA&gt; A9 no Proud ## 6 Affect guilty guil… guilty Guilty Likert… &lt;NA&gt; &lt;NA&gt; A10 no Guilty ## 7 Affect atten… atte… attenti… Attenti… Likert… &lt;NA&gt; &lt;NA&gt; A11 no Attentive ## 8 Affect conte… cont… content Content Likert… &lt;NA&gt; &lt;NA&gt; A12 no Content ## 9 Affect purpo… purp… purpose… Purpose… Likert… &lt;NA&gt; &lt;NA&gt; A13 no Purposeful ## 10 Affect goald… goal… goaldir Goal-di… Likert… &lt;NA&gt; &lt;NA&gt; A14 no Goal-dire… ## # … with 1 more variable: long_name &lt;chr&gt; 1.3.1.1.3 Binary Situations Binary situation indicators were derived by asking undergraduate research assistants to provide list of the common social, academic, and personal situations in which they tended to find themselves. From these, we derived a list of 19 unique situations. Separate items for arguing with or interacting with friends or relatives were composited in overall argument and interaction items. Participants checked a box for each event that occurred in the last hour (1 = occurred, 0 = did not occur). ipcs_codebook %&gt;% filter(category == &quot;sit&quot;) ## # A tibble: 18 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 sit study study study Was stud… 0 = … &lt;NA&gt; &lt;NA&gt; sit_01 no Studying ## 2 sit argum… argum… argFrnd Had an a… 0 = … &lt;NA&gt; &lt;NA&gt; sit_02 no Argument ## 3 sit argum… argum… argFam Had an a… 0 = … &lt;NA&gt; &lt;NA&gt; sit_03 no Argument ## 4 sit inter… inter… IntFrnd Interact… 0 = … &lt;NA&gt; &lt;NA&gt; sit_04 no Interacted ## 5 sit inter… inter… IntFam Interact… 0 = … &lt;NA&gt; &lt;NA&gt; sit_05 no Interacted ## 6 sit lostS… lostS… lostSmt… Lost som… 0 = … &lt;NA&gt; &lt;NA&gt; sit_06 no Lost some… ## 7 sit late late late Was late… 0 = … &lt;NA&gt; &lt;NA&gt; sit_07 no Late ## 8 sit frgtS… frgtS… frgtSmt… Forgot s… 0 = … &lt;NA&gt; &lt;NA&gt; sit_08 no Forgot so… ## 9 sit brdSWk brdSWk brdSWk Was bore… 0 = … &lt;NA&gt; &lt;NA&gt; sit_09 no Bored wit… ## 10 sit excSWk excSWk excSWk Was exci… 0 = … &lt;NA&gt; &lt;NA&gt; sit_10 no Excited a… ## 11 sit AnxSWk AnxSWk AnxSWk Was anxi… 0 = … &lt;NA&gt; &lt;NA&gt; sit_11 no Anxious a… ## 12 sit tired tired tired Felt tir… 0 = … &lt;NA&gt; &lt;NA&gt; sit_12 no Tired ## 13 sit sick sick sick Felt sick 0 = … &lt;NA&gt; &lt;NA&gt; sit_13 no Sick ## 14 sit sleep… sleep… sleeping Was slee… 0 = … &lt;NA&gt; &lt;NA&gt; sit_15 no Sleeping ## 15 sit class class class Was in c… 0 = … &lt;NA&gt; &lt;NA&gt; sit_16 no In Class ## 16 sit music music music Was list… 0 = … &lt;NA&gt; &lt;NA&gt; sit_17 no Listening… ## 17 sit inter… inter… internet Was on t… 0 = … &lt;NA&gt; &lt;NA&gt; sit_18 no On the in… ## 18 sit TV TV TV Was watc… 0 = … &lt;NA&gt; &lt;NA&gt; sit_19 no Watching … ## # … with 1 more variable: long_name &lt;chr&gt; 1.3.1.1.4 DIAMONDS Situation Features Psychological features of situations were measured using the ultra brief version of the “Situational Eight” DIAMONDS (Duty, Intellect, Adversity, Mating, pOsitivity, Negativity, Deception, and Sociality) scale (S8-I; Rauthmann &amp; Sherman, 2015). Items were measured on a 3-point scale from 1 “not at all” to 3 “totally.” ipcs_codebook %&gt;% filter(category == &quot;S8-I&quot;) ## # A tibble: 8 x 12 ## category trait facet itemname old_desc scale orig_scale_num description orig_itemname reverse_code long_trait ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 S8-I Duty Duty Duty Work has … 1 (=… &lt;NA&gt; &lt;NA&gt; D1 no Duty ## 2 S8-I Intel… Intel… Intelle… Deep thin… 1 (=… &lt;NA&gt; &lt;NA&gt; D2 no Intellect ## 3 S8-I Adver… Adver… Adversi… Somebody … 1 (=… &lt;NA&gt; &lt;NA&gt; D3 no Adversity ## 4 S8-I Mating Mating Mating Potential… 1 (=… &lt;NA&gt; &lt;NA&gt; D4 no Mating ## 5 S8-I pOsit… pOsit… pOsitiv… The situa… 1 (=… &lt;NA&gt; &lt;NA&gt; D5 no pOsitivity ## 6 S8-I Negat… Negat… Negativ… The situa… 1 (=… &lt;NA&gt; &lt;NA&gt; D6 no Negativity ## 7 S8-I Decep… Decep… Decepti… Somebody … 1 (=… &lt;NA&gt; &lt;NA&gt; D7 no Deception ## 8 S8-I Socia… Socia… Sociabi… Social in… 1 (=… &lt;NA&gt; &lt;NA&gt; D8 no Sociabili… ## # … with 1 more variable: long_name &lt;chr&gt; 1.3.1.1.5 Timing Features The final set of features were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (Fisher &amp; Soyster, 2019; . To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = {cumulative time}_t and 1 period sine = {cumulative time}_t). 1.3.2 Procedure Participants in this study were drawn from a larger personality study. All responded to two types of surveys: trait and state (Experience Sampling Method; ESM) measures, for which they were paid separately. Participants completed three waves of trait measures and two waves of state measures. For the first two waves, trait surveys were collected immediately before beginning the ESM protocol. 1.3.2.1 Main Sample For the main sample, participants were recruited from the psychology subject pool at Washington University in St. Louis. Participants were told that the study posted on the recruitment website was the first wave of a longer longitudinal study they would be offered the opportunity to take part in. Participants were brought into the lab between October 2018 and December 2019, where a research assistant or the first author explained the study procedure to them and walked them through the consent procedure. If they consented, participants were led to a room where they could fill out a form to opt into the ESM portion of the study. They then completed baseline trait measures using the Qualtrics Survey Platform. After, the participants were debriefed, paid $10 in cash and, if they opted into the ESM portion of the study, the ESM survey procedure was explained to them. Participants then received ESM surveys four times per day for two weeks (target n = 56). The survey platform was built by the first author using the jsPsych library (De Leeuw, 2015). Additional JavaScript controllers were written for the purpose of this study and are available on the first author’s GitHub. Start times were based on times that participants indicated they would like to receive their first survey based on their personal wake times. Surveys were sent every 4 hours, meaning that the surveys spanned a 12-hour period from the start time participants indicated. Participants received their first survey at their chosen time on the Monday following their in-lab session. They were compensated $.50 for each survey completed for a maximum of $28. To incentivize responding, participants who completed at least 50 surveys received a “bonus” for a total compensation of $30, which was distributed as an Amazon Gift Card. 1.3.3 Analytic Plan The present study tested three methods of machine learning classification models, some of which have been used for idiographic prediction in other studies (Fisher &amp; Soyster, 2019; Kaiser &amp; Butter, 2020): (1) Elastic Net Regression (Friedman, Hastie, &amp; Tibshirani, 2010), (2) The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT; Elleman, McDougald, Condon, &amp; Revelle, 2020), and (3) Random Forest Models (Kim et al., 2019). Because we have a large number of indicators to test, each of the methods used have variable selection features and, in some instances, other methods for reducing overfitting, as detailed below. To both reduce the number of indicators used in each test and to test which group of indicators are the most predictive of procrastination and loneliness, we will also test these in several sets: (1) Personality indicators (15), (2) Affective indicators (10), (3) Binary situation indicators (16), (4) DIAMONDS situation indicators (8), (5) Psychological indicators (personality + affect) (25), (6), Situation indicators (binary + DIAMONDS) (24), and (7) Full set (personality + affect + binary situations + DIAMONDS) (49). We will additionally test each of these with and without the 18 timing indicators, for a total set of 14 combinations of the 67 features. In each of these methods, we used cumulative rolling origin forecast validation, which was comprised of the first 75% of the time series, and held out the remaining 25% of the data set for the test set. In the rolling origin forecast validation, we used the first one-third of the time series as the initial set, five observations as the validation set, and set skip to one, which roughly resulted in 10-15 rolling origin “folds.” Out of sample prediction was tested based on classification error and area under the ROC (receive operating characteristic) curve (AUC). Classification error is a simple estimate of the percentage of the test sample that was correctly classified by the model. In addition, the AUC will capture the trade-off between sensitivity and specificity across a threshold. In the present study, we used an AUC threshold of .5, which indicates binary classification at chance levels. ROC visualizations plot 1 - specificity (i.e. false positive rate: false positives / (false positives + true negatives)) against sensitivity (i.e. true positive rate: true positives / (true positives + false positives)). 1.4 Demographics 1.4.0.1 Trait participants &lt;- googlesheets4::sheets_read(&quot;https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit?usp=sharing&quot;, sheet = &quot;ESM&quot;) %&gt;% select(SID, Name, Email) %&gt;% mutate(new = seq(1, n(), 1), new = ifelse(new &lt; 10, paste(&quot;0&quot;, new, sep = &quot;&quot;), new)) 1 old_names &lt;- trait_codebook$`New #` # wave 1 trait baseline &lt;- sprintf(&quot;%s/04-data/01-raw-data/baseline_05.07.20.csv&quot;, res_path) %&gt;% read_csv() %&gt;% filter(!row_number() %in% c(1,2) &amp; !is.na(SID) &amp; SID %in% participants$SID) %&gt;% select(SID, StartDate, gender, YOB, race, ethnicity) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% mutate(wave = 1, gender = factor(gender, c(1,2), c(&quot;Male&quot;, &quot;Female&quot;)), YOB = substr(YOB, nchar(YOB)-4+1, nchar(YOB)), race = mapvalues(race, 1:7, c(0,1,3,2,3,3,3)), ethnicity = ifelse(!is.na(ethnicity), 3, NA), race = ifelse(is.na(ethnicity), race, ifelse(ethnicity == 3, ethnicity))) %&gt;% select(-ethnicity) save(baseline, file = sprintf(&quot;%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData&quot;, res_path)) load(sprintf(&quot;%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData&quot;, res_path)) dem &lt;- baseline %&gt;% select(SID:race) %&gt;% mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB), StartDate = as.Date(ymd_hms(StartDate)), race = factor(race, 0:3, c(&quot;White&quot;, &quot;Black&quot;, &quot;Asian&quot;, &quot;Other&quot;))) %&gt;% select(-YOB) dem %&gt;% summarize(n = length(unique(SID)), gender = sprintf(&quot;%i (%.2f%%)&quot;,sum(gender == &quot;Female&quot;), sum(gender == &quot;Female&quot;)/n()*100), age = sprintf(&quot;%.2f (%.2f)&quot;, mean(age, na.rm = T), sd(age, na.rm = T)), white = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;White&quot;, na.rm = T) , sum(race == &quot;White&quot;, na.rm = T)/n()*100), black = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Black&quot;, na.rm = T) , sum(race == &quot;Black&quot;, na.rm = T)/n()*100), asian = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Asian&quot;, na.rm = T) , sum(race == &quot;Asian&quot;, na.rm = T)/n()*100), other = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Other&quot;, na.rm = T) , sum(race == &quot;Other&quot;, na.rm = T)/n()*100), StartDate = sprintf(&quot;%s (%s - %s)&quot;, median(StartDate), min(StartDate), max(StartDate))) ## # A tibble: 1 x 8 ## n gender age white black asian other StartDate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 208 154 (71.96%) 19.51 (1.27) 69 (32.24%) 34 (15.89%) 67 (31.31%) 30 (14.02… 2019-03-29 (2018-10-17 - 2019-12-… dem %&gt;% kable(., &quot;html&quot; , col.names = c(&quot;ID&quot;, &quot;Start Date&quot;, &quot;Gender&quot;, &quot;Race/Ethnicity&quot;, &quot;Age&quot;) , align = rep(&quot;c&quot;, 5) , caption = &quot;&lt;strong&gt;Table S1&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Descriptive Statistics of Participants at Baseline&lt;em&gt;&quot;) %&gt;% kable_styling(full_width = F) %&gt;% scroll_box(height = &quot;900px&quot;) Table 1.1: Table S1Descriptive Statistics of Participants at Baseline ID Start Date Gender Race/Ethnicity Age 02 2018-10-17 Female White 18 01 2018-10-17 Female Black 19 03 2018-10-17 Female Asian 19 04 2018-10-18 Male Other 19 05 2018-10-18 Male White 19 06 2018-10-18 Female Asian 20 07 2018-10-18 Female Black 20 08 2018-10-18 Female Black 18 09 2018-10-18 Female 18 10 2018-10-19 Female 19 11 2018-10-19 Female Asian 18 12 2018-10-19 Female White 20 13 2018-10-19 Female White 18 14 2018-10-19 Female Black 19 16 2018-10-19 Female White 18 15 2018-10-19 Female 20 17 2018-10-19 Male Asian 18 18 2018-10-22 Female White 19 19 2018-10-22 Female Black 20 20 2018-10-22 Female Asian 18 21 2018-10-22 Female Asian 19 22 2018-10-22 Female Black 19 23 2018-10-22 Male White 21 24 2018-10-22 Male Black 20 25 2018-10-22 Female White 18 27 2018-10-23 Female 18 26 2018-10-23 Female 18 28 2018-10-23 Female Other 21 29 2018-10-23 Female Asian 19 30 2018-10-23 Female Asian 20 31 2018-10-24 Female White 18 32 2018-10-24 Female Black 20 33 2018-10-24 Female Asian 18 34 2018-10-24 Female Black 19 35 2018-10-26 Female Black 18 36 2018-10-29 Female Asian 21 37 2018-10-29 Male Other 18 38 2018-10-29 Male Asian 19 36 2018-10-29 Female Other 20 37 2018-10-29 Female Asian 18 41 2018-10-29 Male White 19 38 2018-10-29 Female Black 19 43 2018-10-29 Female White 18 44 2018-10-30 Female Asian 18 45 2018-11-01 Female 18 46 2018-11-01 Female Asian 22 48 2018-11-01 Female Asian 21 47 2018-11-01 Male Asian 23 49 2018-11-02 Female Asian 20 51 2018-11-02 Female White 20 50 2018-11-02 Female Other 19 52 2018-11-05 Male Other 21 53 2018-11-05 Female Asian 19 52 2018-11-05 Male Asian 21 53 2018-11-05 Female White 19 56 2018-11-05 Male Asian 18 58 2018-11-05 Female Asian 21 57 2018-11-05 Female Asian 59 2018-11-06 Female Asian 21 60 2018-11-06 Male White 20 61 2018-11-06 Male White 18 62 2018-11-06 Male Black 18 63 2018-11-06 Female White 20 64 2018-11-07 Female Other 21 65 2018-11-07 Male White 20 67 2018-11-07 Female Black 19 66 2018-11-07 Male White 20 68 2018-11-07 Female Asian 18 69 2018-11-07 Female Asian 18 70 2018-11-08 Female 19 72 2018-11-08 Female Other 18 71 2018-11-08 Female Other 19 74 2018-11-08 Female Other 22 73 2018-11-08 Female White 18 75 2018-11-08 Female Asian 19 76 2018-11-08 Female Black 20 77 2018-11-08 Male Black 21 79 2018-11-08 Male Asian 18 80 2018-11-08 Female White 21 82 2018-11-09 Female Black 20 81 2018-11-09 Female Asian 22 83 2018-11-09 Female White 18 84 2018-11-09 Female White 20 85 2018-11-14 Female White 18 86 2018-11-14 Female Asian 21 87 2018-11-14 Female Asian 21 89 2018-11-14 Female White 19 88 2018-11-14 Female Asian 18 90 2018-11-20 Female Other 21 91 2018-11-20 Female Black 21 93 2018-11-28 Female White 20 92 2018-11-28 Male Black 20 94 2018-11-28 Female Asian 21 95 2018-11-28 Female Black 21 96 2018-11-28 Female White 21 97 2018-11-28 Male Asian 19 98 2018-11-29 Female Asian 18 99 2018-11-29 Female Other 21 100 2018-11-29 Female Black 19 101 2018-11-29 Female White 18 102 2018-11-29 Female White 19 103 2019-03-15 Female White 20 104 2019-03-15 Female 22 106 2019-03-22 Female Asian 21 105 2019-03-22 Female White 20 107 2019-03-22 Female White 19 109 2019-03-29 Female White 19 108 2019-03-29 Female White 20 111 2019-04-05 Female White 19 110 2019-04-05 Male Other 22 112 2019-04-05 Female Black 113 2019-04-05 Female Asian 23 114 2019-04-05 Male White 20 116 2019-04-12 Female White 21 115 2019-04-12 Female White 19 118 2019-04-12 Female Asian 21 117 2019-04-12 Female White 20 119 2019-04-12 Male Asian 19 121 2019-04-12 Female White 20 122 2019-04-12 Male White 20 123 2019-04-12 Female Asian 20 124 2019-04-12 Female Black 23 126 2019-04-19 Male Other 22 125 2019-04-19 Female Asian 21 128 2019-04-19 Male Black 23 127 2019-04-19 Female White 21 129 2019-04-19 Female Other 20 130 2019-04-19 Female White 19 131 2019-04-19 Male White 20 133 2019-04-26 Male Asian 20 132 2019-04-26 Female White 21 134 2019-04-26 Female Asian 136 2019-09-11 Male Asian 20 135 2019-09-11 Male White 19 138 2019-09-13 Female White 19 137 2019-09-13 Female White 19 139 2019-09-17 Female Black 19 141 2019-09-18 Female White 21 142 2019-09-13 Male Asian 18 143 2019-09-19 Female 18 146 2019-09-19 Female Asian 20 148 2019-09-20 Female Black 20 147 2019-09-20 Female White 20 149 2019-09-20 Female White 22 150 2019-09-20 Female Asian 18 151 2019-09-20 Female Other 18 152 2019-09-20 Male White 18 153 2019-09-27 Female Asian 21 154 2019-09-27 Male White 18 155 2019-09-27 Male White 19 156 2019-09-27 Male White 20 157 2019-09-27 Female White 21 158 2019-09-27 Female Asian 21 159 2019-09-27 Female Other 18 160 2019-09-27 Female Asian 18 161 2019-09-27 Female White 18 162 2019-09-27 Female Black 19 164 2019-10-04 Female 18 163 2019-10-04 Male Black 19 165 2019-10-04 Female White 22 166 2019-10-04 Male Other 19 168 2019-10-04 Female Asian 19 167 2019-10-04 Male Asian 19 169 2019-10-18 Male Asian 21 170 2019-10-18 Female Asian 19 171 2019-10-18 Male Other 18 172 2019-10-18 Female Other 19 174 2019-10-18 Male White 19 173 2019-10-18 Female Asian 18 175 2019-10-28 Male Black 19 176 2019-10-28 Male Other 19 177 2019-10-30 Male Asian 21 179 2019-11-02 Male Other 21 181 2019-11-02 Male Black 21 180 2019-11-02 Female White 20 182 2019-11-03 Female Black 19 184 2019-11-03 Female Asian 20 183 2019-11-03 Female Black 21 185 2019-11-07 Female Other 19 186 2019-11-07 Female Asian 21 188 2019-11-08 Female Other 18 187 2019-11-08 Male White 20 190 2019-11-08 Male White 21 189 2019-11-08 Female Other 20 192 2019-11-08 Female Black 18 191 2019-11-08 Male Other 18 199 2019-11-11 Female Asian 19 200 2019-11-11 Female Asian 18 201 2019-11-11 Female Asian 19 202 2019-11-11 Male Asian 18 203 2019-11-14 Male White 18 204 2019-11-14 Female Other 20 205 2019-11-14 Female Asian 19 206 2019-11-15 Female White 19 197 2019-11-15 Female 18 207 2019-11-15 Female Other 18 193 2019-11-15 Male White 18 196 2019-11-15 Female White 21 195 2019-11-15 Male Black 21 197 2019-11-15 Male White 18 209 2019-11-20 Female Asian 19 210 2019-11-20 Female 18 211 2019-11-20 Female 19 212 2019-11-20 Male White 20 213 2019-11-22 Male Asian 19 214 2019-11-22 Female Other 19 216 2019-11-22 Female Asian 19 215 2019-11-22 Female White 19 217 2019-12-02 Male 19 218 2019-12-03 Male Asian 19 219 2019-12-05 Male White 19 220 2019-12-05 Female Other 20 221 2019-12-05 Female Asian 21 222 2019-12-05 Female Black 20 "],["cleaning.html", "Chapter 2 Data Cleaning 2.1 Pre-Share Cleaning 2.2 ESM Data Setup 2.3 Setup for Idiographic Machine Learning Models 2.4 Demographics", " Chapter 2 Data Cleaning 2.1 Pre-Share Cleaning For the purposes of this study, we are only sharing the data used in this study (for both items and participants). ESM data were pre-cleaned because data imported from jsPsych are in a format not easily interpretable within R. An R script that shows the full procedure for extracting jsPscyh data is available on the OSF page for this study. Trait data were downloaded from Qualtrics and directly imported and cleaned as shown below. In addition, in order to anonymize participants, we have changed their ID’s using our master list, which we cannot make available because it contains identifying information. participants &lt;- googlesheets4::sheets_read(&quot;https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit#gid=16299281&quot;, sheet = &quot;ESM&quot;) %&gt;% select(SID, Name, Email) %&gt;% mutate(new = seq(1, nrow(.), 1), new = ifelse(new &lt; 10, paste(&quot;0&quot;, new, sep = &quot;&quot;), new)) 1 # wave 1 esm load(sprintf(&quot;%s/04-data/01-raw-data/clean_data_w1_2020-06-08.RData&quot;, res_path)) # combine waves bfi &lt;- BFI %&gt;% distinct() %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(orig_itemname = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, orig_itemname, reverse_code)) old.names &lt;- ipcs_codebook$orig_itemname emo &lt;- emotion %&gt;% select(-trait, -facet) %&gt;% mutate(item = str_remove_all(item, &quot;E_&quot;)) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(trait = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, reverse_code)) sit &lt;- sit %&gt;% filter(item %in% old.names) %&gt;% select(-trait, -facet, -answer) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(orig_itemname = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, orig_itemname, reverse_code)) ds8 &lt;- DS8 %&gt;% select(-trait, -facet, -answer) %&gt;% mutate(SID = mapvalues(SID, participants$SID, participants$new)) %&gt;% rename(orig_itemname = item) %&gt;% left_join(ipcs_codebook %&gt;% select(category, trait, facet, itemname, orig_itemname, reverse_code)) save(bfi, emo, sit, ds8, file = sprintf(&quot;%s/04-data/esm_cleaned_combined_2021-04-07.RData&quot;, res_path)) rm(list = ls()) train_fun &lt;- function(x) { if(length(unique(x[!is.na(x)]))==1){ replace &lt;- c(0,1)[!0:1 %in% unique(x[!is.na(x)])[1]] x[sample(1:length(x), 1)] &lt;- replace } else if (any(table(x) == 1)){ replace &lt;- c(0,1)[which(table(x) &lt;= 1)] x[sample(1:length(x), 1)] &lt;- replace } x } test_fun &lt;- function(x) { if(length(unique(x[!is.na(x)]))==1){ replace &lt;- c(0,1)[!0:1 %in% unique(x[!is.na(x)])[1]] x[sample(1:length(x), 1)] &lt;- replace } x } Now, we’ll load in the cleaned and de-identified data. load(sprintf(&quot;%s/04-data/01-raw-data/esm_cleaned_combined_2021-04-07.RData&quot;, res_path)) 2.2 ESM Data Setup Next, we need to make sure that all time information for IPCS is available. Specifically, this will allow us to control for overnight periods and unequal spacing between measurement occasions. 2.2.1 Timing First, we need to create empty rows in the data where there are missing assessments from the four target surveys per day as well as for the overnight periods. The function below uses the time stamp to figure out which blocks are missing and add those empty rows by indexing the time stamp of collected surveys as well as participants chosen start times. missing_fun &lt;- function(d){ first_day &lt;- unique(d$StartDate) # get first day hourBlock &lt;- unique(d$`Hour Block 1`) # get first hour block max_day &lt;- max(d$Day); max_day &lt;- ifelse(max_day &lt; 14, 14, max_day) # get number of days d2 &lt;- d %&gt;% #mutate(StartDate = ifelse(is.na(StartDate), min(Date, na.), StartDate)) full_join(crossing( Day = seq(0,max_day,1), HourBlock = 1:6, StartDate = first_day, `Hour Block 1` = hourBlock)) %&gt;% # cross existing data with &quot;perfect&quot; data arrange(Day, HourBlock) %&gt;% mutate(Date = StartDate + Day, Hour = ifelse(is.na(Hour), `Hour Block 1` + (HourBlock-1)*4, Hour), Minute = ifelse(is.na(Minute), &quot;00&quot;, Minute)) d2$Date[d2$Hour &gt; 23] &lt;- d2$Date[d2$Hour &gt; 23] + 1 # some day blocks span days d2$Hour[d2$Hour &gt; 23] &lt;- d2$Hour[d2$Hour &gt; 23] - 24 # some day blocks span days d2 &lt;- d2 %&gt;% mutate( Full_Date = sprintf(&quot;%s %s:%s&quot;, as.character(Date), Hour, Minute)) %&gt;% select(-`Hour Block 1`, -StartDate) } # create a data frame of timing info ipcs_times &lt;- emo %&gt;% select(SID, StartDate, Date, Hour, Minute, Day, `Hour Block 1`, HourBlock) %&gt;% distinct() %&gt;% mutate(Minute = str_remove_all(Minute, &quot;.csv&quot;), Minute = ifelse(as.numeric(Minute) &lt; 10, sprintf(&quot;0%s&quot;, Minute), Minute)) %&gt;% arrange(SID, Date) %&gt;% group_by(SID) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = map(data, missing_fun)) %&gt;% unnest(data) %&gt;% arrange(SID, Date, Hour) %&gt;% group_by(SID) %&gt;% mutate(all_beeps = seq(1, n(), 1)) %&gt;% ungroup() Here’s the result ipcs_times ## # A tibble: 23,615 x 8 ## SID Date Hour Minute Day HourBlock Full_Date all_beeps ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 2018-10-22 15 23 0 1 2018-10-22 15:23 1 ## 2 01 2018-10-22 19 00 0 2 2018-10-22 19:00 2 ## 3 01 2018-10-22 23 23 0 3 2018-10-22 23:23 3 ## 4 01 2018-10-22 23 25 0 3 2018-10-22 23:25 4 ## 5 01 2018-10-23 3 00 0 4 2018-10-23 3:00 5 ## 6 01 2018-10-23 7 00 0 5 2018-10-23 7:00 6 ## 7 01 2018-10-23 11 00 0 6 2018-10-23 11:00 7 ## 8 01 2018-10-23 17 50 1 1 2018-10-23 17:50 8 ## 9 01 2018-10-23 19 39 1 2 2018-10-23 19:39 9 ## 10 01 2018-10-23 23 00 1 3 2018-10-23 23:00 10 ## # … with 23,605 more rows 2.2.2 Personality Now it’s time to wrangle the personality data. As a reminder, personality was assessed using the full BFI-2 (Soto &amp; John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 “disagree strongly” to 5 “agree strongly.” Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants. 2.2.2.1 Wrangle Raw Data # join with codebook, reverse code, composite within facets and spread to wide format bfi_wide &lt;- bfi %&gt;% select(SID, Date, Hour, Minute, trait, facet, value = responses2, reverse_code) %&gt;% mutate(Minute = str_remove_all(Minute, &quot;.csv&quot;), Minute = ifelse(as.numeric(Minute) &lt; 10, sprintf(&quot;0%s&quot;, Minute), Minute), Full_Date = sprintf(&quot;%s %s:%s&quot;, as.character(Date), Hour, Minute)) %&gt;% mutate(value = as.numeric(value), value = ifelse(!is.na(reverse_code) &amp; reverse_code == &quot;yes&quot;, reverse.code(-1, value, mini = 1, maxi = 5), value)) %&gt;% select(-reverse_code) %&gt;% group_by(SID, trait, facet, Full_Date) %&gt;% summarize(value = mean(value, na.rm = T)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = c(&quot;trait&quot;, &quot;facet&quot;) , values_from = &quot;value&quot; , names_sep = &quot;_&quot;) %&gt;% group_by(SID) %&gt;% arrange(SID, lubridate::ymd_hm(Full_Date)) %&gt;% mutate(all_beeps = seq(1, n(), 1)) %&gt;% ungroup() 2.2.2.2 Multiple Imputation These data were collected using a planned missing design, so we need to impute data for the planned missing components. # run MI bfi_mi &lt;- data.frame(unclass(bfi_wide %&gt;% select(-Full_Date))) set.seed(5) bfi_mi &lt;- amelia(bfi_mi, m = 1, ts = &quot;all_beeps&quot;, cs = &quot;SID&quot;)$imputations[[1]] %&gt;% as_tibble() %&gt;% full_join(bfi_wide %&gt;% select(SID, Full_Date, all_beeps)) %&gt;% select(-all_beeps) ## -- Imputation 1 -- ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 bfi_mi ## # A tibble: 8,672 x 17 ## SID agreeableness_Co… agreeableness_Re… agreeableness_T… conscientiousnes… conscientiousnes… conscientiousnes… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 1.45 1 3 4 4 4.79 ## 2 01 2.43 3 2 1 1 3.68 ## 3 01 2.86 1.5 1.70 1.14 3.03 3 ## 4 01 3.39 2 4 4.33 4 4 ## 5 01 5 2 3.85 3.04 3 3.04 ## 6 01 4 4.59 4 1.91 5 5 ## 7 01 2 1 3.88 2.18 3 1.61 ## 8 01 2 6.30 2 4 2.96 5 ## 9 01 3.26 2.80 3 3.17 2 2 ## 10 01 4.5 3.23 2.39 3.5 1.63 3.09 ## # … with 8,662 more rows, and 10 more variables: extraversion_Assertiveness &lt;dbl&gt;, ## # extraversion_Energy.Level &lt;dbl&gt;, extraversion_Sociability &lt;dbl&gt;, neuroticism_Anxiety &lt;dbl&gt;, ## # neuroticism_Depression &lt;dbl&gt;, neuroticism_Emotional.Volatility &lt;dbl&gt;, openness_Aesthetic.Sensitivity &lt;dbl&gt;, ## # openness_Creative.Imagination &lt;dbl&gt;, openness_Intellectual.Curiosity &lt;dbl&gt;, Full_Date &lt;chr&gt; Personality data: bfi_mi ## # A tibble: 8,672 x 17 ## SID agreeableness_Co… agreeableness_Re… agreeableness_T… conscientiousnes… conscientiousnes… conscientiousnes… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 1.45 1 3 4 4 4.79 ## 2 01 2.43 3 2 1 1 3.68 ## 3 01 2.86 1.5 1.70 1.14 3.03 3 ## 4 01 3.39 2 4 4.33 4 4 ## 5 01 5 2 3.85 3.04 3 3.04 ## 6 01 4 4.59 4 1.91 5 5 ## 7 01 2 1 3.88 2.18 3 1.61 ## 8 01 2 6.30 2 4 2.96 5 ## 9 01 3.26 2.80 3 3.17 2 2 ## 10 01 4.5 3.23 2.39 3.5 1.63 3.09 ## # … with 8,662 more rows, and 10 more variables: extraversion_Assertiveness &lt;dbl&gt;, ## # extraversion_Energy.Level &lt;dbl&gt;, extraversion_Sociability &lt;dbl&gt;, neuroticism_Anxiety &lt;dbl&gt;, ## # neuroticism_Depression &lt;dbl&gt;, neuroticism_Emotional.Volatility &lt;dbl&gt;, openness_Aesthetic.Sensitivity &lt;dbl&gt;, ## # openness_Creative.Imagination &lt;dbl&gt;, openness_Intellectual.Curiosity &lt;dbl&gt;, Full_Date &lt;chr&gt; Now back to long format bfi_long &lt;- bfi_mi %&gt;% pivot_longer(cols = c(-SID, -Full_Date) , names_to = c(&quot;trait&quot;, &quot;facet&quot;) , values_to = &quot;value&quot; , names_sep = &quot;_&quot;) %&gt;% mutate(category = &quot;BFI-2&quot;) 2.2.3 Other Measured Features Emotion and Situation (Binary and DIAMONDS) Data (not planned missing, so no need to impute): features &lt;- emo %&gt;% full_join(sit) %&gt;% full_join(ds8) %&gt;% select(SID, category, trait, facet, itemname, Date, Hour, Minute, Day, HourBlock, value = responses2) %&gt;% mutate(Minute = str_remove_all(Minute, &quot;.csv&quot;), Minute = ifelse(as.numeric(Minute) &lt; 10, sprintf(&quot;0%s&quot;, Minute), Minute), Full_Date = sprintf(&quot;%s %s:%s&quot;, as.character(Date), Hour, Minute), value = as.numeric(value)) %&gt;% group_by(SID, category, trait, facet, Full_Date) %&gt;% summarize(value = max(value)) %&gt;% ungroup() features ## # A tibble: 312,010 x 6 ## SID category trait facet Full_Date value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 01 Affect afraid afraid 2018-10-22 15:23 2 ## 2 01 Affect afraid afraid 2018-10-22 23:23 3 ## 3 01 Affect afraid afraid 2018-10-22 23:25 3 ## 4 01 Affect afraid afraid 2018-10-23 17:50 4 ## 5 01 Affect afraid afraid 2018-10-23 19:39 2 ## 6 01 Affect afraid afraid 2018-10-24 0:00 2 ## 7 01 Affect afraid afraid 2018-10-24 11:44 3 ## 8 01 Affect afraid afraid 2018-10-24 15:37 2 ## 9 01 Affect afraid afraid 2018-10-24 20:46 3 ## 10 01 Affect afraid afraid 2018-10-25 21:07 4 ## # … with 312,000 more rows 2.2.4 Timing Features Finally, we’ll create the timing features. These were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (e.g., Fisher &amp; Soyster, 2019). To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = {cumulative time}_t and 1 period sine = {cumulative time}_t). time_features &lt;- ipcs_times %&gt;% mutate(wkday = wday(Full_Date, label = T) , Mon = ifelse(wkday == &quot;Mon&quot;, 1, 0) , Tue = ifelse(wkday == &quot;Tue&quot;, 1, 0) , Wed = ifelse(wkday == &quot;Wed&quot;, 1, 0) , Thu = ifelse(wkday == &quot;Thu&quot;, 1, 0) , Fri = ifelse(wkday == &quot;Fri&quot;, 1, 0) , Sat = ifelse(wkday == &quot;Sat&quot;, 1, 0) , Sun = ifelse(wkday == &quot;Sun&quot;, 1, 0) , morning = ifelse(Hour &gt;= 5 &amp; Hour &lt; 11, 1, 0) , midday = ifelse(Hour &gt;= 11 &amp; Hour &lt; 17, 1, 0) , evening = ifelse(Hour &gt;= 5 &amp; Hour &lt; 22, 1, 0) , night = ifelse(Hour &gt;= 22 &amp; Hour &lt; 5, 1, 0)) %&gt;% ## sequential time differences for each persn group_by(SID) %&gt;% mutate(tdif = as.numeric(difftime(ymd_hm(Full_Date), lag(ymd_hm(Full_Date)), units = &quot;hours&quot;))) %&gt;% filter(is.na(tdif) | tdif &gt; 1) %&gt;% mutate(tdif = as.numeric(difftime(ymd_hm(Full_Date), lag(ymd_hm(Full_Date)), units = &quot;hours&quot;)) , tdif = ifelse(is.na(tdif), 0, tdif) , cumsumT = cumsum(tdif)) %&gt;% ungroup() %&gt;% ## timing variables mutate(linear = as.numeric(scale(cumsumT)) , quad = linear^2 , cub = linear^3 , sin1p = sin(((2*pi)/24)*cumsumT) , sin2p = sin(((2*pi)/12)*cumsumT) , cos1p = cos(((2*pi)/24)*cumsumT) , cos2p = cos(((2*pi)/12)*cumsumT) ) %&gt;% ## keep key variables and reshape select(SID, Full_Date, Mon:night, linear:cos2p) %&gt;% pivot_longer(cols = c(-SID, -Full_Date) , names_to = &quot;trait&quot; , values_to = &quot;value&quot;) %&gt;% mutate(category = &quot;time&quot; , facet = trait) time_features ## # A tibble: 413,928 x 6 ## SID Full_Date trait value category facet ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01 2018-10-22 15:23 Mon 1 time Mon ## 2 01 2018-10-22 15:23 Tue 0 time Tue ## 3 01 2018-10-22 15:23 Wed 0 time Wed ## 4 01 2018-10-22 15:23 Thu 0 time Thu ## 5 01 2018-10-22 15:23 Fri 0 time Fri ## 6 01 2018-10-22 15:23 Sat 0 time Sat ## 7 01 2018-10-22 15:23 Sun 0 time Sun ## 8 01 2018-10-22 15:23 morning 0 time morning ## 9 01 2018-10-22 15:23 midday 1 time midday ## 10 01 2018-10-22 15:23 evening 1 time evening ## # … with 413,918 more rows 2.2.5 Combine Features Now, let’s bring the personality, affect/situation/DIAMONDS, and timing features together. all_features &lt;- bfi_long %&gt;% full_join(features) %&gt;% full_join(ipcs_times %&gt;% select(SID, Full_Date)) %&gt;% full_join(time_features) %&gt;% arrange(SID, category, trait, facet, Full_Date) all_features ## # A tibble: 871,011 x 6 ## SID Full_Date trait facet value category ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 01 2018-10-22 15:23 afraid afraid 2 Affect ## 2 01 2018-10-22 23:23 afraid afraid 3 Affect ## 3 01 2018-10-22 23:25 afraid afraid 3 Affect ## 4 01 2018-10-23 17:50 afraid afraid 4 Affect ## 5 01 2018-10-23 19:39 afraid afraid 2 Affect ## 6 01 2018-10-24 0:00 afraid afraid 2 Affect ## 7 01 2018-10-24 11:44 afraid afraid 3 Affect ## 8 01 2018-10-24 15:37 afraid afraid 2 Affect ## 9 01 2018-10-24 20:46 afraid afraid 3 Affect ## 10 01 2018-10-25 21:07 afraid afraid 4 Affect ## # … with 871,001 more rows 2.3 Setup for Idiographic Machine Learning Models The last step is the most important. We need to: Separate the data for each outcome, participant, and feature set combination. In addition, the outcomes need to be lagged such that same time point features will be predicting “future” behavior. Moreover, the data must be split into training (first 75%) and test sets (last 25%). As we do this, we will also remove participants who have no variance in the outcome in either training or test sets as we can’t (statistically) predict things without variance (even if no variance suggests a good prediction!). The feature sets are as follows: Psychological: Big Five (BFI-2) Psychological: Affect Psychological: Big Five + Affect Situations: Binary Situations: DIAMONDS Situations: Binary + DIAMONDS Full: Big Five + Affect + Binary + DIAMONDS Each of these will be tested with and without the timing features for a total number of 14 feature sets. For now, I’m not going to run the chunk below because it takes a long time. All the resulting files can be found in the online materials: 04-data/02-raw-data: data before being split into training and test 04-data/03-train-data: training data for each participant x outcome x feature set combination (14) 04-data/04-test-data: test data for each participant x outcome x feature set combination (14) save_fun &lt;- function(d, group, set, outcome, SID, time){ print(paste(SID, outcome, group, set, time)) d_split &lt;- initial_time_split(d, prop = 0.75) d_train &lt;- training(d_split) d_test &lt;- testing(d_split) if(any(table(d_train$o_value) &lt; 2) | sd(d_train$o_value) == 0) { return(NA) # no variance == can&#39;t use that participant } else { d_train &lt;- d_train %&gt;% mutate_at(vars(one_of(dummy_vars)), train_fun) %&gt;% mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor) ret &lt;- F # this is indexing if there were any other issues or concerns to be aware of } if(length(unique(d_test$o_value[!is.na(d_test$o_value)])) == 1){ d_test &lt;- d_test %&gt;% mutate_at(vars(one_of(dummy_vars)), test_fun) %&gt;% mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor) ret &lt;- c(ret, T) } else { d_test &lt;- d_test %&gt;% mutate_at(vars(one_of(c(dummy_vars, time_dummy))), factor) ret &lt;- c(ret, F) } d &lt;- d_train %&gt;% full_join(d_test) %&gt;% arrange(Full_Date) d_split &lt;- initial_time_split(d, prop = 0.75) d_train &lt;- training(d_split) d_test &lt;- testing(d_split) save(d, file = sprintf(&quot;%s/04-data/02-model-data/%s_%s_%s_%s_%s.RData&quot; , res_path, SID, outcome, group, set, time)) save(d_train, file = sprintf(&quot;%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData&quot; , res_path, SID, outcome, group, set, time)) save(d_test, d_split, file = sprintf(&quot;%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData&quot; , res_path, SID, outcome, group, set, time)) # return(T) if(any(ret == T)) ret &lt;- T else ret &lt;- F return(ret) # } } factor_fun &lt;- function(x){if(is.numeric(x)){diff(range(x, na.rm = T)) %in% 1:2} else{F}} dummy_vars &lt;- c(&quot;o_value&quot;, &quot;argument&quot;, &quot;interacted&quot;, &quot;lostSmthng&quot; , &quot;late&quot;, &quot;frgtSmthng&quot;, &quot;brdSWk&quot;, &quot;excSWk&quot;, &quot;AnxSWk&quot; , &quot;tired&quot;, &quot;sick&quot;, &quot;sleeping&quot;, &quot;class&quot; , &quot;music&quot;, &quot;internet&quot;, &quot;TV&quot;, &quot;study&quot;) time_dummy &lt;- c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot;) data_fun &lt;- function(group, set, outcome, time){ if(set != &quot;all&quot;) groups &lt;- set if(set == &quot;all&quot;) { if(group == &quot;psychological&quot;) groups &lt;- c(&quot;BFI-2&quot;, &quot;Affect&quot;) else if(group == &quot;situations&quot;) groups &lt;- c(&quot;S8-I&quot;, &quot;sit&quot;) else groups &lt;- c(&quot;BFI-2&quot;, &quot;Affect&quot;, &quot;S8-I&quot;, &quot;sit&quot;) } if(time == &quot;time&quot;) groups &lt;- c(groups, &quot;time&quot;) out &lt;- all_features %&gt;% filter(trait == outcome) %&gt;% select(SID, Full_Date, value) %&gt;% group_by(SID) %&gt;% mutate(o_value = lag(value)) %&gt;% ungroup() %&gt;% select(-value) d &lt;- all_features %&gt;% filter(category %in% groups) %&gt;% mutate(name = ifelse(category == &quot;BFI-2&quot;, paste(trait, facet, sep = &quot;_&quot;), trait)) %&gt;% select(SID, Full_Date, name, value) %&gt;% distinct() %&gt;% pivot_wider(names_from = &quot;name&quot; , values_from = &quot;value&quot;) %&gt;% full_join(out) %&gt;% filter(complete.cases(.)) d %&gt;% group_by(SID) %&gt;% filter(n() &gt;= 40) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = pmap(list(data, group, set, outcome, SID, time), possibly(save_fun, NA_real_))) %&gt;% unnest(data) } nested_data &lt;- tribble( ~group, ~set, &quot;psychological&quot;, &quot;BFI-2&quot; , &quot;psychological&quot;, &quot;Affect&quot;, &quot;psychological&quot;, &quot;all&quot; , &quot;situations&quot; , &quot;sit&quot; , &quot;situations&quot; , &quot;S8-I&quot; , &quot;situations&quot; , &quot;all&quot; , &quot;full&quot; , &quot;all&quot; , ) %&gt;% full_join(crossing(group = c(&quot;psychological&quot;, &quot;situations&quot;, &quot;full&quot;) , time = c(&quot;no time&quot;, &quot;time&quot;) , outcome = c(&quot;prcrst&quot;, &quot;lonely&quot;))) %&gt;% mutate(data = pmap(list(group, set, outcome, time), possibly(data_fun, NA_real_))) 2.4 Demographics load(sprintf(&quot;%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData&quot;, res_path)) dem &lt;- baseline %&gt;% select(SID:race) %&gt;% mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB), StartDate = as.Date(ymd_hms(StartDate)), race = factor(race, 0:3, c(&quot;White&quot;, &quot;Black&quot;, &quot;Asian&quot;, &quot;Other&quot;))) %&gt;% select(-YOB) prelim_dem &lt;- all_features %&gt;% filter(category %in% c(&quot;Affect&quot;, &quot;BFI-2&quot;, &quot;sit&quot;, &quot;SI-8&quot;)) %&gt;% group_by(SID, Full_Date, trait, facet, category) %&gt;% summarize(value = mean(value, na.rm = T)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = c(&quot;category&quot;, &quot;trait&quot;, &quot;facet&quot;) , values_from = value) %&gt;% filter(complete.cases(.)) prelim_dem %&gt;% group_by(SID) %&gt;% tally() %&gt;% ungroup() %&gt;% left_join(dem) %&gt;% summarize(N = length(unique(SID)), n = sprintf(&quot;%.2f (%.2f; %i-%i&quot;, mean(n), sd(n), min(n), max(n)), gender = sprintf(&quot;%i (%.2f%%)&quot;,sum(gender == &quot;Female&quot;, na.rm = T), sum(gender == &quot;Female&quot;, na.rm = T)/n()*100), age = sprintf(&quot;%.2f (%.2f)&quot;, mean(age, na.rm = T), sd(age, na.rm = T)), white = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;White&quot;, na.rm = T) , sum(race == &quot;White&quot;, na.rm = T)/n()*100), black = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Black&quot;, na.rm = T) , sum(race == &quot;Black&quot;, na.rm = T)/n()*100), asian = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Asian&quot;, na.rm = T) , sum(race == &quot;Asian&quot;, na.rm = T)/n()*100), other = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Other&quot;, na.rm = T) , sum(race == &quot;Other&quot;, na.rm = T)/n()*100), StartDate = sprintf(&quot;%s (%s - %s)&quot;, median(StartDate), min(StartDate), max(StartDate))) ## # A tibble: 1 x 9 ## N n gender age white black asian other StartDate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 199 42.23 (24.01; 1-158 144 (70.59%) 19.52 (1.24) 65 (31.86%) 30 (14.71%) 60 (29.41%) 28 (13.73%) NA (NA - NA) final_dem &lt;- prelim_dem %&gt;% group_by(SID) %&gt;% filter(n() &gt;= 40) %&gt;% tally() %&gt;% ungroup() %&gt;% left_join(dem) unique(ldply(str_split(list.files(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models&quot;, res_path)), pattern = &quot;_&quot;), function(x) x[1]))$V1 ## [1] &quot;01&quot; &quot;02&quot; &quot;03&quot; &quot;05&quot; &quot;08&quot; &quot;09&quot; &quot;10&quot; &quot;103&quot; &quot;105&quot; &quot;106&quot; &quot;107&quot; &quot;108&quot; &quot;11&quot; &quot;110&quot; &quot;111&quot; &quot;112&quot; &quot;113&quot; &quot;118&quot; ## [19] &quot;119&quot; &quot;121&quot; &quot;123&quot; &quot;124&quot; &quot;126&quot; &quot;129&quot; &quot;133&quot; &quot;135&quot; &quot;146&quot; &quot;148&quot; &quot;149&quot; &quot;15&quot; &quot;150&quot; &quot;152&quot; &quot;154&quot; &quot;155&quot; &quot;156&quot; &quot;157&quot; ## [37] &quot;158&quot; &quot;159&quot; &quot;160&quot; &quot;162&quot; &quot;164&quot; &quot;165&quot; &quot;166&quot; &quot;167&quot; &quot;168&quot; &quot;169&quot; &quot;17&quot; &quot;170&quot; &quot;171&quot; &quot;174&quot; &quot;177&quot; &quot;18&quot; &quot;185&quot; &quot;186&quot; ## [55] &quot;188&quot; &quot;189&quot; &quot;190&quot; &quot;192&quot; &quot;195&quot; &quot;196&quot; &quot;20&quot; &quot;201&quot; &quot;206&quot; &quot;207&quot; &quot;21&quot; &quot;211&quot; &quot;212&quot; &quot;214&quot; &quot;216&quot; &quot;22&quot; &quot;220&quot; &quot;25&quot; ## [73] &quot;26&quot; &quot;27&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; &quot;33&quot; &quot;35&quot; &quot;36&quot; &quot;38&quot; &quot;40&quot; &quot;43&quot; &quot;44&quot; &quot;49&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; &quot;56&quot; &quot;58&quot; ## [91] &quot;61&quot; &quot;63&quot; &quot;66&quot; &quot;67&quot; &quot;68&quot; &quot;71&quot; &quot;73&quot; &quot;74&quot; &quot;77&quot; &quot;78&quot; &quot;81&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;88&quot; final_dem %&gt;% filter(SID %in% unique(ldply(str_split(list.files(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models&quot;, res_path)), pattern = &quot;_&quot;), function(x) x[1]))$V1) %&gt;% summarize(N = length(unique(SID)), n = sprintf(&quot;%.2f (%.2f; %i-%i&quot;, mean(n), sd(n), min(n), max(n)), gender = sprintf(&quot;%i (%.2f%%)&quot;,sum(gender == &quot;Female&quot;, na.rm = T), sum(gender == &quot;Female&quot;, na.rm = T)/n()*100), age = sprintf(&quot;%.2f (%.2f)&quot;, mean(age, na.rm = T), sd(age, na.rm = T)), white = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;White&quot;, na.rm = T) , sum(race == &quot;White&quot;, na.rm = T)/n()*100), black = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Black&quot;, na.rm = T) , sum(race == &quot;Black&quot;, na.rm = T)/n()*100), asian = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Asian&quot;, na.rm = T) , sum(race == &quot;Asian&quot;, na.rm = T)/n()*100), other = sprintf(&quot;%i (%.2f%%)&quot; , sum(race == &quot;Other&quot;, na.rm = T) , sum(race == &quot;Other&quot;, na.rm = T)/n()*100), StartDate = sprintf(&quot;%s (%s - %s)&quot;, median(StartDate), min(StartDate), max(StartDate))) ## # A tibble: 1 x 9 ## N n gender age white black asian other StartDate ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 99 57.41 (16.33; 40-109 75 (72.82%) 19.49 (1.31) 32 (31.07%) 14 (13.59%) 33 (32.04%) 16 (15.53%) NA (NA - NA) rm(list = ls()[!ls() %in% c(&quot;codebook&quot;, &quot;ipcs_codebook&quot;, &quot;res_path&quot;, &quot;sheets&quot;, &quot;outcomes&quot;, &quot;ftrs&quot;)]) "],["elastic-net.html", "Chapter 3 Elastic Net 3.1 Functions 3.2 Run Models", " Chapter 3 Elastic Net Elastic Net Regression (ENR) proceeds from the observation that typical OLS-based regression minimizes bias but may have great variance. Using L1 (Ridge) and L2 (LASSO) approaches, which apply penalties to model estimates, ENR attempts to balance the trade-off between bias and variance by choosing the best penalties that minimize an information criterion or prediction error. Together, these both shrink coefficients and help with feature selection by forcing some of the coefficients to be zero. Because there are a large number of values the regularization parameter λ can take on, the typical solution is to use a cross-validation to test a number of possible \\(\\lambda\\) penalty values and choose the one with the one that matches a criterion like minimizing prediction error. In the present study, we used the tidymodels package in R to estimate the ENR models by calling the logistic_reg(), setting the engine as “glmnet”, and the mode as “classification”. The parameters tuned via rolling origin forecast validation were penalty and mixture, which were each set to 10 values. Next, we used the select_best() function with the method set to “accuracy” to allow the algorithm to automatically pick the best combination of penalty and mixture that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of penalty and mixture and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the collect_metrics() function. 3.1 Functions dummy_vars &lt;- c(&quot;o_value&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot;, &quot;argument&quot; , &quot;interacted&quot;, &quot;lostSmthng&quot;, &quot;late&quot;, &quot;frgtSmthng&quot;, &quot;brdSWk&quot; , &quot;excSWk&quot;, &quot;AnxSWk&quot;, &quot;tired&quot;, &quot;sick&quot;, &quot;sleeping&quot;, &quot;class&quot; , &quot;music&quot;, &quot;internet&quot;, &quot;TV&quot;, &quot;study&quot;) time_vars &lt;- c(&quot;sin2p&quot;, &quot;sin1p&quot;, &quot;cos2p&quot;, &quot;cos1p&quot;) c_fun &lt;- function(m){ # final model characteristics lambda &lt;- min(m$fit$lambda) coefs &lt;- stats::coef(m$fit, s = lambda) coefs &lt;- coefs[, 1L, drop = TRUE] coefs &lt;- coefs[setdiff(x = names(coefs), y = &quot;(Intercept)&quot;)] return(coefs) } elnet_fun &lt;- function(sid, outcome, group, set, time){ # load the data load(sprintf(&quot;%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) d_train &lt;- d_train %&gt;% arrange(Full_Date) %&gt;% select(-Full_Date) #%&gt;% # mutate_at(vars(-o_value), ~as.numeric(as.character(.))) init &lt;- ceiling(nrow(d_train)/3) d_train_cv &lt;- rolling_origin( d_train, initial = init, assess = 5, skip = 1, cumulative = TRUE ) no_zv_fun &lt;- function(x){ if(is.numeric(x)) sd(x, na.rm = T) != 0 else length(table(x)) &gt; 1 } # set up the cross-valiation folds # set.seed(234) # d_train_cv &lt;- vfold_cv(d_train, v = 10) # set up the data and formula mod_recipe &lt;- recipe( o_value ~ . , data = d_train ) %&gt;% step_zv(all_predictors(), -all_outcomes()) %&gt;% step_nzv(all_predictors(), unique_cut = 35) %&gt;% step_dummy(one_of(dummy_vars), -all_outcomes()) %&gt;% step_normalize(-one_of(dummy_vars), -one_of(time_vars)) #%&gt;% # estimate the means and standard deviations # prep(training = d_train, retain = TRUE) # set up the model specifications tune_spec &lt;- logistic_reg( penalty = tune() , mixture = tune() ) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% set_mode(&quot;classification&quot;) # set up the ranges for the tuning functions elnet_grid &lt;- grid_regular(penalty() , mixture() , levels = 10) # set up the workflow: combine modeling spec with modeling recipe set.seed(345) elnet_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_recipe(mod_recipe) # combine the workflow, and grid to a final tuning model elnet_res &lt;- elnet_wf %&gt;% tune_grid( resamples = d_train_cv , grid = elnet_grid , control = control_resamples(save_pred = T) ) save(elnet_res, file = sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s.RData&quot;, # res_path, sid, outcome, group, set)) # plot the metrics across tuning parameters p &lt;- elnet_res %&gt;% collect_metrics() %&gt;% ggplot(aes(penalty, mean, color = mixture)) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_gradient(low = &quot;gray90&quot;, high = &quot;red&quot;) + theme_classic() ggsave(p, file = sprintf(&quot;%s/05-results/01-glmnet/02-tuning-figures/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 8) # load(sprintf(&quot;%s/05-results/01-glmnet/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, # res_path, sid, outcome, group, set, time)) # select the best model based on AUC best_elnet &lt;- elnet_res %&gt;% # select_best(&quot;roc_auc&quot;) select_best(&quot;accuracy&quot;) # set up the workflow for the best model final_wf &lt;- elnet_wf %&gt;% finalize_workflow(best_elnet) # run the final best model on the training data and save final_elnet &lt;- final_wf %&gt;% fit(data = d_train) final_m &lt;- final_elnet %&gt;% pull_workflow_fit() final_coefs &lt;- c_fun(final_m) best_elnet &lt;- best_elnet %&gt;% mutate(nvars = length(final_coefs[final_coefs != 0])) save(final_coefs, best_elnet, file = sprintf(&quot;%s/05-results/01-glmnet/07-final-model-param/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load the test data load(sprintf(&quot;%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # d_split$data$o_value &lt;- factor(d_split$data$o_value) # d_split$data&lt;-d_split$data %&gt;% # mutate_at(vars(-Full_Date, -o_value), ~as.numeric(as.character(.))) # run the final fit workflow of the training and test data together final_fit &lt;- final_wf %&gt;% last_fit(d_split) save(final_elnet, final_fit , file = sprintf(&quot;%s/05-results/01-glmnet/03-final-training-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # final metrics (accuracy and roc) final_metrics &lt;- final_fit %&gt;% collect_metrics(summarize = T) save(final_metrics , file = sprintf(&quot;%s/05-results/01-glmnet/06-final-model-performance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # variable importance final_var_imp &lt;- final_elnet %&gt;% pull_workflow_fit() %&gt;% vi() %&gt;% slice_max(Importance, n = 10) save(final_var_imp , file = sprintf(&quot;%s/05-results/01-glmnet/05-variable-importance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # roc plot p_roc &lt;- final_fit %&gt;% collect_predictions() %&gt;% roc_curve(.pred_0, truth = o_value) %&gt;% autoplot() + labs(title = sprintf(&quot;Participant %s: %s, %s, %s, %s&quot; , sid, outcome, group, set, time)) ggsave(p_roc, file = sprintf(&quot;%s/05-results/01-glmnet/04-roc-curves/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) rm(list = c(&quot;final_var_imp&quot;, &quot;final_metrics&quot;, &quot;final_wf&quot;, &quot;final_elnet&quot;, &quot;final_fit&quot; , &quot;best_elnet&quot;, &quot;elnet_res&quot;, &quot;elnet_wf&quot;, &quot;elnet_grid&quot;, &quot;tune_spec&quot;, &quot;mod_recipe&quot; , &quot;p&quot;, &quot;p_roc&quot;, &quot;d_split&quot;, &quot;d_test&quot;, &quot;d_train&quot;, &quot;d_train_cv&quot;)) gc() return(T) } 3.2 Run Models plan(multisession(workers = 12L)) elnet_mods &lt;- tibble( file = sprintf(&quot;%s/04-data/03-train-data&quot;, res_path) %&gt;% list.files() ) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;)) %&gt;% mutate(mod = future_pmap( list(SID, outcome, group, set, time) , safely(elnet_fun, NA_real_) , .progress = T , .options = future_options( globals = c(&quot;res_path&quot;, &quot;dummy_vars&quot;, &quot;c_fun&quot;, &quot;time_vars&quot;) , packages = c(&quot;plyr&quot;, &quot;tidyverse&quot;, &quot;glmnet&quot;, &quot;tidymodels&quot;, &quot;vip&quot;) ) ) ) closeAllConnections() "],["biscwit.html", "Chapter 4 BISCWIT 4.1 Functions 4.2 Run Models", " Chapter 4 BISCWIT The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT) is a correlation-based machine learning technique. The technique, which we modified to be compatible with rolling origin validation rather than k-fold cross validation as implemented in the best.scales() function in the psych package in R, proceeds as follows. First, pairwise correlations between predictors and outcome(s) are calculated using rolling origin forecast validation where the number of best items is varied. Second, the number of items in the final model is determined by finding the average correlation across the rolling origin training sets with the validation sets for each number of items and choosing the one with the highest average correlation. Third, the final training model is constructed by running the procedure on the full training set with the number of items determined by the validation procedure. Fourth, weighted sum scores of the features for both the training and test sets are extracted. Finally, accuracy and AUC are calculated. Accuracy was estimated by (1) scaling the scores in the test set by the mean and standard deviation of scores in the final training model, (2) multiplying the scaled scores by the correlation between the training scores and the training outcome and the standard deviation of the training outcome and adding the mean of the training data outcome to this. This resulted in predicted outcomes for the test set. As in other classification models, values of .5 and above were considered to predict a “1” while values less than .5 were considered to predict a “0.” Accuracy was determined by comparing the predicted value with the actual test value and averaging the number correct. AUC was calculated using the predicted values and the test set values for the outcome. In the present study, we will use the bestScales() function from the psych package to create the models. Weighted scores were calculated by extracting the correlations from the best scales object and using it in the scoreWtd() function to create the correlation weighted scores. AUC was calculated using the vi() function in the vip package. Variable importance was determined by the items with the highest correlation with the outcome in the final training model. 4.1 Functions biscwit_call &lt;- function(x, nitem){ ## format the ro training set x_train &lt;- training(x) %&gt;% select(o_value, everything()) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% unclass() %&gt;% data.frame() ## call the best scales function with nitem set from outer loop bs &lt;- bestScales( x = x_train , criteria = &quot;o_value&quot; , n.item = nitem , n.iter = 1 ) ## get the multiple R&#39;s (weights for biscwit v biscuit) # the second line sets the items not found by biscuit to be &quot;best&quot; # to be 0 mR &lt;- bs$R mR[!names(mR) %in% str_remove(bs$best.keys$o_value, &quot;-&quot;)] &lt;- 0 mR &lt;- as.matrix(mR); colnames(mR) &lt;- &quot;o_value&quot; ## format the test set x_test &lt;- testing(x) %&gt;% select(o_value, everything()) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% # select(one_of(names(mR))) %&gt;% unclass() %&gt;% data.frame() ## score the test set using the correlations as weights scores &lt;- scoreWtd(mR, x_test) ## calculate the correlation between the weighted score and the test y r &lt;- cor(scores, as.numeric(as.character(testing(x)$o_value))) ## return the biscuit object and the test correlations (criterion) return(list(bs = bs, r = r)) } biscwit_fun &lt;- function(sid, outcome, group, set, time){ # load the data load(sprintf(&quot;%s/04-data/03-train-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## format the training data x_train &lt;- d_train %&gt;% arrange(Full_Date) %&gt;% select(-Full_Date) %&gt;% select(o_value, everything()) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% unclass() %&gt;% data.frame() ## create the rolling_origin training and validation sets init &lt;- ceiling(nrow(d_train)/3) d_train_cv &lt;- rolling_origin( x_train, initial = init, assess = 5, skip = 1, cumulative = TRUE ) ## run biscwit on the ro ## run for nitem = 3 to the number of x training columns in increments of 3 tune_res &lt;- d_train_cv %&gt;% mutate(nitem = map(splits, ~seq(3, ncol(training(.)) - 1, 3))) %&gt;% unnest(nitem) %&gt;% mutate(cv = map2(splits, nitem, possibly(biscwit_call, NA_real_))) %&gt;% ## run the biscuit procedure # mutate(cv = map2(splits, nitem, biscwit_call)) %&gt;% ## run the biscuit procedure filter(!is.na(cv)) %&gt;% mutate(error = map_dbl(cv, ~(.)$r)) ## correlation is the training &quot;error&quot; (higher = lower error) save(tune_res, file = sprintf(&quot;%s/05-results/02-biscwit/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # plot the metrics across tuning parameters p &lt;- tune_res %&gt;% group_by(nitem) %&gt;% summarize(merror = fisherz2r(mean(fisherz(error), na.rm = T))) %&gt;% arrange(desc(abs(merror))) %&gt;% ggplot(aes(x = nitem, y = merror)) + scale_alpha_continuous(range=c(.4,1))+ geom_point(aes(alpha = abs(merror)), color = &quot;red&quot;) + labs(x = &quot;n items&quot;, y = &quot;Correlation&quot;, &quot;Correlation Strength (absolute value)&quot;) + theme_classic() ggsave(p, file = sprintf(&quot;%s/05-results/02-biscwit/02-tuning-figures/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) # find the best model by averaging correlations across ro sets # and choosing the one with the highest average correlation best_biscwit &lt;- tune_res %&gt;% group_by(nitem) %&gt;% summarize(merror = fisherz2r(mean(fisherz(error), na.rm = T))) %&gt;% arrange(desc(abs(merror))) %&gt;% slice_head(n = 1) ## rerun biscuit on the full training set with the best nitem final_biscwit &lt;- bestScales( x = x_train , criteria = &quot;o_value&quot; , n.item = best_biscwit$nitem , n.iter = 1 ) save(final_biscwit , file = sprintf(&quot;%s/05-results/02-biscwit/03-final-training-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # get the correlations for weighting # the second line sets the items not found by biscuit to be &quot;best&quot; # to be 0 mR &lt;- final_biscwit$R; mR &lt;- mR[!names(mR) == &quot;o_value&quot;] mR[!names(mR) %in% str_remove(final_biscwit$best.keys$o_value, &quot;-&quot;)] &lt;- 0 mR &lt;- as.matrix(mR); colnames(mR) &lt;- &quot;o_value&quot; final_m &lt;- final_biscwit final_coefs &lt;- mR best_biscwit &lt;- best_biscwit %&gt;% mutate(nvars = length(mR[mR != 0])) save(final_coefs, best_biscwit, file = sprintf(&quot;%s/05-results/02-biscwit/07-final-model-param/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load the test data load(sprintf(&quot;%s/04-data/04-test-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## format the test set x_test &lt;- d_test %&gt;% arrange(Full_Date) %&gt;% select(o_value, everything(), -Full_Date) %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% unclass() %&gt;% data.frame() # adapted from Elleman, McDougald, Condon, &amp; Revelle (2020) # Step 1. Score the train X. scores_best &lt;- scoreWtd(mR, x_train) # Step 2. Score the test X. scores_best_test &lt;- scoreWtd(mR, x_test) # Step 3. Get the diagnostic info from the train y&#39;s scoring. cor(scores_best, x_train$o_value, use = &quot;pairwise&quot;) ## mean and sd of training scores for observations 1 to t. mean_biscuit &lt;- mean(scores_best, na.rm = T) sd_biscuit &lt;- sd( scores_best, na.rm = T) # performance in training set multiple_R &lt;- cor(scores_best, x_train$o_value, use = &quot;pairwise&quot;) # descriptives of the outcome variable in the training set mean_data &lt;- mean(x_train$o_value, na.rm = T) sd_data &lt;- sd( x_train$o_value, na.rm = T) # Step 4. Apply the diagnostic info to the test y&#39;s scoring. # Standardardize scores in the test set using the training set # subtract average training score from each score in the test set and divide by SD scores_best.z_test &lt;- c((scores_best_test - mean_biscuit) / sd_biscuit) # scores_best.z_test_vector &lt;- c(scores_best.z_test) # reverse transform using mean and SD of outcome in the training set # and the correlation between scores and outcome in the training set value_test &lt;- ( c(multiple_R) * scores_best.z_test * sd_data ) + mean_data # Step 5. You have your predicted test values. Look at them to make sure they make sense!! temp &lt;- data.frame(crit_actual = x_test$o_value, crit_predicted = value_test) # Get the accuracy. Since the reverse standardization should make these raw, # we&#39;ll just essentially round these values to 0 or 1 (whichever is closer) # to determine accuracy temp_diagnostics &lt;- as_tibble(temp) %&gt;% mutate( accuracy = ifelse((crit_predicted &gt;= .5 &amp; crit_actual == 1) | (crit_predicted &lt; .5 &amp; crit_actual == 0), 1, 0) ) # Best model results, testing data ## Getting and saving final metrics ## First get average accuracy in the test set value_accuracy &lt;- mean(temp_diagnostics$accuracy, na.rm = T) ## Next get AUC&#39;s. Building in a failsafe in case in the case of no variance ## in the test set, which would result in an error and break the code ## formatted to match output from tidymodels collect_metrics() for consistency if(sd(temp$crit_actual) != 0){ roc &lt;- roc_auc(temp %&gt;% mutate(crit_actual = as.factor(crit_actual)) , truth = crit_actual , crit_predicted) } else { roc &lt;- tibble( .metric = &quot;roc_auc&quot; , .estimator = &quot;binary&quot; , .estimate = NA_real_ ) } ## reformat accuracy and join with AUC final_metrics &lt;- roc %&gt;% bind_rows(tibble( .metric = &quot;accuracy&quot; , .estimator = &quot;binary&quot; , .estimate = value_accuracy)) save(final_metrics , file = sprintf(&quot;%s/05-results/02-biscwit/06-final-model-performance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## variable importance ## reformatted to match the output of the vi() function in the vip package final_var_imp &lt;- data.frame(Importance = final_biscwit$R) %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% mutate(Sign = ifelse(sign(Importance) == 1, &quot;POS&quot;, &quot;NEG&quot;)) %&gt;% as_tibble() %&gt;% arrange(desc(abs(Importance))) %&gt;% slice_max(abs(Importance), n = 10) save(final_var_imp , file = sprintf(&quot;%s/05-results/02-biscwit/05-variable-importance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) ## roc curve of test data p_roc &lt;- roc_curve( temp %&gt;% mutate(crit_actual = as.factor(crit_actual)) , truth = crit_actual , crit_predicted ) %&gt;% autoplot() + labs(title = sprintf(&quot;Participant %s: %s, %s, %s, %s&quot; , sid, outcome, group, set, time)) ggsave(p_roc, file = sprintf(&quot;%s/05-results/02-biscwit/04-roc-curves/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) rm(list = ls()) gc() return(T) } 4.2 Run Models done &lt;- tibble(file = list.files(sprintf(&quot;%s/05-results/02-biscwit/06-final-model-performance&quot;, res_path)), done = &quot;done&quot;) plan(multisession(workers = 12L)) res &lt;- tibble( file = sprintf(&quot;%s/04-data/02-model-data&quot;, res_path) %&gt;% list.files() ) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;) , mod = future_pmap( list(SID, outcome, group, set, time) , possibly(biscwit_fun, NA_real_) , .progress = T , .options = future_options( globals = c(&quot;res_path&quot;, &quot;biscwit_call&quot;) , packages = c(&quot;plyr&quot;, &quot;tidyverse&quot;, &quot;psych&quot;, &quot;tidymodels&quot;, &quot;vip&quot;) ) ) ) closeAllConnections() "],["random-forest.html", "Chapter 5 Random Forest 5.1 Set Up Data", " Chapter 5 Random Forest Random forest models are a variant of decision tree classification algorithms that additionally draw on bagging (i.e. ensemble methods; bootstrapping with aggregation) methods. The name itself hints at how it different from classic decision trees; we have a forest instead of a tree. In so doing, random forest models draw upon a large number of decision trees of varying depth that are then aggregated. The random part comes from two sources. First, each tree in the forest in trained on a data set drawn with replacement from the training dataset (i.e. bootstrapped) as part of the bagging procedure. Observations left out of each model are termed out-of-bag observations (and collectively, the out-of-bag dataset). These are used to evaluate the performance of the tree. Second, the features used in each tree are also randomly drawn from the full of features. Each of these trees, based on their data generates a prediction given new data. The final prediction is based off the ensemble of trees – that is, the decision made by a majority of the trees (i.e. aggregation). Importantly, because of the random feature selection part of the procedure, random forest can also provide estimates of variable importance, indicating which features are critical to making a less error-prone classification. Because random forest using bagging (i.e. bootstrapping with aggregation), we will have to perform a series of steps that make bootstrapping appropriate with time series data: differencing, Box-Cox transformations, and time-delay embedding. Essentially, differencing and Box-Cox transformations stabilize the mean and variance, respectively, to make the time series stationary (Priestley, 1988), and time-delay embedding quite literally embeds the sequence of the time series into predictor variables, which in effect preserves the order of the times series (Von Oertzen &amp; Boker, 2010). We can easily back transform forecasts to their original scale. In the present study, we used the tidymodels package in R to estimate the random forest models by calling the rand_forest(), setting the engine as “ranger”, with importance = “permutation” in order to extract variable importance, and the mode as “classification”. The parameters tuned via rolling origin forecast validation were mtry (i.e. the number of predictors that will be randomly sampled at each split when creating tree models) and min_n (i.e. the minimum number of data points in a node that is required for the node to be split further), which were each set to 10 values. Next, we used the select_best() function with the method set to “accuracy” to allow the algorithm to automatically pick the best combination of mtry and min_n that maximized classification accuracy. Next, we fit the final training model using the full training set and the best combination of mtry and min_n and tested the model using the training set. To evaluate the efficacy of the model, we extracted the classification accuracy rate (0-1) and the AUC using the collect_metrics() function. 5.1 Set Up Data 5.1.1 Differencing and Box Cox dummy_vars &lt;- c(&quot;o_value&quot;, &quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot;, &quot;argument&quot; , &quot;interacted&quot;, &quot;lostSmthng&quot;, &quot;late&quot;, &quot;frgtSmthng&quot;, &quot;brdSWk&quot; , &quot;excSWk&quot;, &quot;AnxSWk&quot;, &quot;tired&quot;, &quot;sick&quot;, &quot;sleeping&quot;, &quot;class&quot; , &quot;music&quot;, &quot;internet&quot;, &quot;TV&quot;, &quot;study&quot;) time_vars &lt;- c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;, &quot;Sat&quot;, &quot;Sun&quot; , &quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;, &quot;night&quot; , &quot;sin2p&quot;, &quot;sin1p&quot;, &quot;cos2p&quot;, &quot;cos1p&quot; , &quot;cub&quot;, &quot;linear&quot;, &quot;quad&quot;) rf_fun &lt;- function(sid, outcome, group, set, time){ load(sprintf(&quot;%s/04-data/02-model-data/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # differencing and box-cox d &lt;- d %&gt;% mutate_if(is.factor, ~as.numeric(as.character(.))) %&gt;% mutate_at(vars(-one_of(c(dummy_vars, time_vars)), -Full_Date), log) %&gt;% mutate_at(vars(-one_of(c(dummy_vars, time_vars)), -Full_Date), ~. - lag(.)) # time delay embedding d_mbd &lt;- map(d %&gt;% select(-Full_Date, -one_of(time_vars)), ~embed(., 2)) %&gt;% ldply(.) %&gt;% group_by(.id) %&gt;% mutate(beep = 1:n()) %&gt;% ungroup() %&gt;% pivot_wider(names_from = &quot;.id&quot; , names_glue = &quot;{.id}_{.value}&quot; , values_from = c(&quot;1&quot;, &quot;2&quot;)) %&gt;% bind_cols(d[-1,] %&gt;% select(Full_Date)) %&gt;% select(-beep) d_mbd &lt;- d_mbd %&gt;% full_join(d %&gt;% select(Full_Date, one_of(time_vars))) %&gt;% mutate_at(vars(contains(dummy_vars)), factor) %&gt;% # mutate(o_value_1 = factor(o_value_1)) %&gt;% drop_na() # training and test sets d_split &lt;- initial_time_split(d_mbd, prop = 0.75) d_train &lt;- training(d_split) d_test &lt;- testing(d_split) d_train &lt;- d_train %&gt;% arrange(lubridate::ymd_hm(Full_Date)) %&gt;% select(-Full_Date) ## create the rolling_origin training and validation sets init &lt;- ceiling(nrow(d_train)/3) # set up the cross-valiation folds d_train_cv &lt;- rolling_origin( d_train, initial = init, assess = 5, skip = 1, cumulative = TRUE ) # set up the data and formula mod_recipe &lt;- recipe( o_value_1 ~ . , data = d_train ) %&gt;% step_zv(all_numeric(), contains(dummy_vars), contains(time_vars)) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% step_nzv(all_predictors(), unique_cut = 35) #%&gt;% # estimate the means and standard deviations # prep(training = d_train, retain = TRUE) # set up the model specifications tune_spec &lt;- rand_forest( mtry = tune() , trees = 1000 , min_n = tune() ) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;permutation&quot;) %&gt;% set_mode(&quot;classification&quot;) # set up the workflow: combine modeling spec with modeling recipe set.seed(345) rf_wf &lt;- workflow() %&gt;% add_model(tune_spec) %&gt;% add_recipe(mod_recipe) # set up the ranges for the tuning functions set.seed(345) tune_res &lt;- tune_grid( rf_wf , resamples = d_train_cv , grid = 20 ) save(tune_res, file = sprintf(&quot;%s/05-results/03-rf/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # load(sprintf(&quot;%s/05-results/03-rf/01-tuning-models/%s_%s_%s_%s_%s.RData&quot;, # res_path, sid, outcome, group, set, time)) # plot the metrics across tuning parameters p &lt;- tune_res %&gt;% collect_metrics() %&gt;% ggplot(aes(mtry, mean, color = min_n)) + geom_point(size = 2) + facet_wrap(~ .metric, scales = &quot;free&quot;, nrow = 2) + scale_x_log10(labels = scales::label_number()) + scale_color_gradient(low = &quot;gray90&quot;, high = &quot;red&quot;) + theme_classic() ggsave(p, file = sprintf(&quot;%s/05-results/03-rf/02-tuning-figures/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 8) # select the best model based on AUC best_rf &lt;- tune_res %&gt;% # select_best(&quot;roc_auc&quot;) select_best(&quot;accuracy&quot;) # set up the workflow for the best model final_wf &lt;- rf_wf %&gt;% finalize_workflow(best_rf) # run the final best model on the training data and save final_rf &lt;- final_wf %&gt;% fit(data = d_train) final_m &lt;- final_rf %&gt;% pull_workflow_fit() final_coefs &lt;- final_m$fit$variable.importance best_rf &lt;- best_rf %&gt;% mutate(nvars = length(final_coefs[final_coefs != 0])) save(final_coefs, best_rf, file = sprintf(&quot;%s/05-results/03-rf/07-final-model-param/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # run the final fit workflow of the training and test data together final_fit &lt;- final_wf %&gt;% last_fit(d_split) save(final_rf, final_fit , file = sprintf(&quot;%s/05-results/03-rf/03-final-training-models/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # final metrics (accuracy and roc) final_metrics &lt;- final_fit %&gt;% collect_metrics(summarize = T) save(final_metrics , file = sprintf(&quot;%s/05-results/03-rf/06-final-model-performance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # variable importance final_var_imp &lt;- final_rf %&gt;% pull_workflow_fit() %&gt;% vi() %&gt;% slice_max(Importance, n = 10) save(final_var_imp , file = sprintf(&quot;%s/05-results/03-rf/05-variable-importance/%s_%s_%s_%s_%s.RData&quot;, res_path, sid, outcome, group, set, time)) # roc plot p_roc &lt;- final_fit %&gt;% collect_predictions() %&gt;% roc_curve(.pred_0, truth = o_value_1) %&gt;% autoplot() + labs(title = sprintf(&quot;Participant %s: %s, %s, %s, %s&quot; , sid, outcome, group, set, time)) ggsave(p_roc, file = sprintf(&quot;%s/05-results/03-rf/04-roc-curves/%s_%s_%s_%s_%s.png&quot;, res_path, sid, outcome, group, set, time) , width = 5, height = 5) rm(list = c(&quot;final_var_imp&quot;, &quot;final_metrics&quot;, &quot;final_wf&quot;, &quot;final_rf&quot;, &quot;final_fit&quot; , &quot;best_rf&quot;, &quot;tune_res&quot;, &quot;rf_wf&quot;, &quot;tune_spec&quot;, &quot;mod_recipe&quot; , &quot;p&quot;, &quot;p_roc&quot;, &quot;d_split&quot;, &quot;d_test&quot;, &quot;d_train&quot;, &quot;d_train_cv&quot;)) gc() return(T) } 5.1.2 Run Models plan(multisession(workers = 12L)) rf_res &lt;- tibble( file = sprintf(&quot;%s/04-data/02-model-data&quot;, res_path) %&gt;% list.files() ) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;)) %&gt;% mutate( mod = future_pmap( list(SID, outcome, group, set, time) , safely(rf_fun, NA_real_) , .progress = T , .options = future_options( globals = c(&quot;res_path&quot;, &quot;dummy_vars&quot;, &quot;time_vars&quot;) , packages = c(&quot;plyr&quot;, &quot;tidyverse&quot;, &quot;glmnet&quot;, &quot;tidymodels&quot;, &quot;vip&quot;) ) ) ) closeAllConnections() "],["summarizing-models.html", "Chapter 6 Summarizing Models 6.1 Question 1: Can we predict procrastination and loneliness? 6.2 Question 2: Are there individual differences in the idiographic range of prediction across people? 6.3 Question 3: Do Psychological, Situational, or Full Feature Sets Perform Best? 6.4 Question 4: Which features are most associated with Procrastination and Loneliness? 6.5 Question 5: Do people vary in the which features are most important?", " Chapter 6 Summarizing Models Now that all the models have been run, the next step is to take various metrics and results from the models and format them into tables and figures that are more understable than thousands of model objects. 6.1 Question 1: Can we predict procrastination and loneliness? 6.1.1 Performance Metrics To begin, we’ll pull the performance metrics – classification accuracy and area under the receiver operating curve (AUC) – to determine and display: Overall Model Performance Participant Specific Model Performance (e.g., did certain feature sets perform differently) Participants best models (in terms of accuracy and AUC) along with summaries of such accuracy and AUC, the feature set, etc. The first thing we need to do is load in the final model performance metrics – that is, the accuracy and AUC of the model chosen via rolling-origin validation on the test / holdout set. loadRData &lt;- function(fileName, type, model){ #loads an RData file, and returns it path &lt;- sprintf(&quot;%s/05-results/%s/06-final-model-performance/%s&quot;, res_path, model, fileName) load(path) get(ls()[grepl(type, ls())]) } sum_res &lt;- tibble( model = c(&quot;01-glmnet&quot;, &quot;02-biscwit&quot;, &quot;03-rf&quot;) ) %&gt;% mutate(file = map(model, ~sprintf(&quot;%s/05-results/%s/06-final-model-performance&quot;, res_path, .) %&gt;% list.files())) %&gt;% unnest(file) %&gt;% mutate(data = map2(file, model, ~loadRData(.x, &quot;final_metrics&quot;, .y))) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;) , model = str_remove_all(model, &quot;[0-9 -]&quot;)); sum_res ## # A tibble: 5,485 x 7 ## model SID outcome group set time data ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 glmnet 01 prcrst full all no time &lt;tibble[,4] [2 × 4]&gt; ## 2 glmnet 01 prcrst psychological Affect no time &lt;tibble[,4] [2 × 4]&gt; ## 3 glmnet 01 prcrst psychological all no time &lt;tibble[,4] [2 × 4]&gt; ## 4 glmnet 01 prcrst psychological BFI-2 no time &lt;tibble[,4] [2 × 4]&gt; ## 5 glmnet 01 prcrst situations all no time &lt;tibble[,4] [2 × 4]&gt; ## 6 glmnet 01 prcrst situations S8-I no time &lt;tibble[,4] [2 × 4]&gt; ## 7 glmnet 01 prcrst situations sit no time &lt;tibble[,4] [2 × 4]&gt; ## 8 glmnet 02 prcrst full all no time &lt;tibble[,4] [2 × 4]&gt; ## 9 glmnet 02 prcrst full all time &lt;tibble[,4] [2 × 4]&gt; ## 10 glmnet 02 prcrst psychological Affect no time &lt;tibble[,4] [2 × 4]&gt; ## # … with 5,475 more rows Which looks something like this: sum_res %&gt;% unnest(data) ## # A tibble: 10,970 x 10 ## model SID outcome group set time .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 glmnet 01 prcrst full all no time accuracy binary 0.909 Preprocessor1_Model1 ## 2 glmnet 01 prcrst full all no time roc_auc binary 0.4 Preprocessor1_Model1 ## 3 glmnet 01 prcrst psychological Affect no time accuracy binary 0.909 Preprocessor1_Model1 ## 4 glmnet 01 prcrst psychological Affect no time roc_auc binary 0.8 Preprocessor1_Model1 ## 5 glmnet 01 prcrst psychological all no time accuracy binary 0.909 Preprocessor1_Model1 ## 6 glmnet 01 prcrst psychological all no time roc_auc binary 0.4 Preprocessor1_Model1 ## 7 glmnet 01 prcrst psychological BFI-2 no time accuracy binary 0.917 Preprocessor1_Model1 ## 8 glmnet 01 prcrst psychological BFI-2 no time roc_auc binary 0.5 Preprocessor1_Model1 ## 9 glmnet 01 prcrst situations all no time accuracy binary 0.818 Preprocessor1_Model1 ## 10 glmnet 01 prcrst situations all no time roc_auc binary 0.7 Preprocessor1_Model1 ## # … with 10,960 more rows loadRData &lt;- function(fileName, type, model){ #loads an RData file, and returns it path &lt;- sprintf(&quot;%s/05-results/%s/07-final-model-param/%s&quot;, res_path, model, fileName) load(path) get(ls()[grepl(type, ls())]) } param_res &lt;- tibble( model = c(&quot;01-glmnet&quot;, &quot;02-biscwit&quot;, &quot;03-rf&quot;) ) %&gt;% mutate(file = map(model, ~sprintf(&quot;%s/05-results/%s/07-final-model-param&quot;, res_path, .) %&gt;% list.files())) %&gt;% unnest(file) %&gt;% mutate(params = map2(file, model, ~loadRData(.x, &quot;best&quot;, .y)) , coefs = map2(file, model, ~loadRData(.x, &quot;coef&quot;, .y))) %&gt;% separate(file, c(&quot;SID&quot;, &quot;outcome&quot;, &quot;group&quot;, &quot;set&quot;, &quot;time&quot;), sep = &quot;_&quot;) %&gt;% mutate(time = str_remove_all(time, &quot;.RData&quot;) , model = str_remove_all(model, &quot;[0-9 -]&quot;)) 6.1.2 Classification Accuracy and AUC for all Models Now that we’ve loaded in the results, the first thing that we’ll do is create tables on the performance of each model for all the tested feature sets. The goal here is less to make any specific argument with the results and more to just document them in a nice table format that is easier to read. perf_tab_fun &lt;- function(d, outcome, group, set, time){ # format groups, time, and outcomes to nice names g &lt;- str_to_title(group); s &lt;- str_to_title(set) tm &lt;- if(time == &quot;time&quot;) &quot;With Time&quot; else &quot;Without Time&quot; o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) # create the caption cap &lt;- sprintf(&quot;&lt;strong&gt;Table SX&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Performance Metrics of the %s (%s) Feature Set %s Predicting %s&quot;, g, s, tm, o) # call kable to create the html table tab &lt;- d %&gt;% kable(. , &quot;html&quot; , col.names = c(&quot;ID&quot;, rep(c(&quot;Accuracy&quot;, &quot;AUC&quot;), times = 3)) , align = c(&quot;r&quot;, rep(&quot;c&quot;, 6)) , digits = 2 , caption = cap ) %&gt;% kable_styling(full_width = F) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2)) # save the table to files save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/01-participant-metrics/%s_%s_%s_%s.html&quot;, res_path, outcome, group, set, time)) # return the table object return(tab) } sum_res_tab &lt;- sum_res %&gt;% unnest(data) %&gt;% select(-.estimator, -.config) %&gt;% pivot_wider(names_from = c(&quot;model&quot;, &quot;.metric&quot;) , values_from = &quot;.estimate&quot;) %&gt;% group_by(outcome, group, set, time) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = pmap(list(data, outcome, group, set, time), perf_tab_fun)) sum_res_tab ## # A tibble: 28 x 6 ## outcome group set time data tab ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 prcrst full all no time &lt;tibble[,7] [83 × 7]&gt; &lt;kablExtr [1]&gt; ## 2 prcrst psychological Affect no time &lt;tibble[,7] [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 3 prcrst psychological all no time &lt;tibble[,7] [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 4 prcrst psychological BFI-2 no time &lt;tibble[,7] [98 × 7]&gt; &lt;kablExtr [1]&gt; ## 5 prcrst situations all no time &lt;tibble[,7] [83 × 7]&gt; &lt;kablExtr [1]&gt; ## 6 prcrst situations S8-I no time &lt;tibble[,7] [83 × 7]&gt; &lt;kablExtr [1]&gt; ## 7 prcrst situations sit no time &lt;tibble[,7] [98 × 7]&gt; &lt;kablExtr [1]&gt; ## 8 prcrst full all time &lt;tibble[,7] [75 × 7]&gt; &lt;kablExtr [1]&gt; ## 9 prcrst psychological Affect time &lt;tibble[,7] [81 × 7]&gt; &lt;kablExtr [1]&gt; ## 10 prcrst psychological all time &lt;tibble[,7] [81 × 7]&gt; &lt;kablExtr [1]&gt; ## # … with 18 more rows Now I’ll print the tables in different tabs below. Here, I’m only showing the set with combined features for each category for parsimony. The full results are in the online materials under 05-results/04-tables/01-participant-metrics. 6.1.2.1 Procrastination As is clear in each of these tables, overall accuracy and AUC are quite high although there are quite stark individual differences in them across people. AUC tended to be lower than accuracy on average. However, at the individual level, there are some individuals who had AUC scores higher than accuracy. tmp &lt;- sum_res_tab %&gt;% filter(set == &quot;all&quot; &amp; outcome == &quot;prcrst&quot;) %&gt;% mutate(group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) for(i in 1:nrow(tmp)){ cat(&#39; \\n\\n##### &#39;, tmp$group[i], &#39;\\n\\n &#39;, sep =&quot;&quot;) tmp$tab[[i]] %&gt;% scroll_box(height = &quot;500px&quot;) %&gt;% print() } ## ## ## ##### Full, No Time ## ## ## ## ##### Psychological, No Time ## ## ## ## ##### Situations, No Time ## ## ## ## ##### Full, Time ## ## ## ## ##### Psychological, Time ## ## ## ## ##### Situations, Time ## ## 6.1.2.2 Loneliness As with procrastination, the loneliness tables indicate that overall accuracy and AUC are quite high although there are quite stark individual differences in them across people. AUC tended to be lower than accuracy on average. However, at the individual level, there are some individuals who had AUC scores higher than accuracy. tmp &lt;- sum_res_tab %&gt;% filter(set == &quot;all&quot; &amp; outcome == &quot;lonely&quot;) %&gt;% mutate(group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) for(i in 1:nrow(tmp)){ cat(&#39; \\n\\n##### &#39;, tmp$group[i], &#39;\\n\\n &#39;, sep =&quot;&quot;) tmp$tab[[i]] %&gt;% scroll_box(height = &quot;750px&quot;) %&gt;% print() } 6.1.2.2.1 Full, No Time 6.1.2.2.2 Full, Time 6.1.2.2.3 Psychological, No Time 6.1.2.2.4 Psychological, Time 6.1.2.2.5 Situations, No Time 6.1.2.2.6 Situations, Time 6.1.3 Best Models Next, to get a more concise indication of how these models are performing, we will choose the best model in terms of accuracy and AUC for each participant, outcome, and model combination. best_mods &lt;- sum_res %&gt;% unnest(data) %&gt;% filter(set == &quot;all&quot;) %&gt;% group_by(SID, outcome, .metric, model) %&gt;% filter(!is.na(.estimate) &amp; .estimate != 1) %&gt;% arrange(desc(.estimate), model, group) %&gt;% slice_head(n = 1) %&gt;% ungroup(); best_mods ## # A tibble: 817 x 10 ## model SID outcome group set time .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 biscwit 01 prcrst full all no time accuracy binary 0.909 &lt;NA&gt; ## 2 glmnet 01 prcrst full all no time accuracy binary 0.909 Preprocessor1_Model1 ## 3 rf 01 prcrst psychological all no time accuracy binary 0.909 Preprocessor1_Model1 ## 4 biscwit 01 prcrst situations all no time roc_auc binary 0.85 &lt;NA&gt; ## 5 glmnet 01 prcrst situations all no time roc_auc binary 0.7 Preprocessor1_Model1 ## 6 rf 01 prcrst full all no time roc_auc binary 0.556 Preprocessor1_Model1 ## 7 biscwit 02 prcrst full all time accuracy binary 0.769 &lt;NA&gt; ## 8 glmnet 02 prcrst full all time accuracy binary 0.846 Preprocessor1_Model1 ## 9 rf 02 prcrst full all no time accuracy binary 0.833 Preprocessor1_Model1 ## 10 biscwit 02 prcrst situations all no time roc_auc binary 0.705 &lt;NA&gt; ## # … with 807 more rows 6.1.3.1 Participant Summaries (Table) Now that we have participants best models, the first thing we’ll do is create a summary of just how well participants’ best models actually performed. These Supplementary Tables will be split by outcome (procrastination, loneliness) and metric (accuracy, AUC) with each row giving details on which feature set was chosen for each method and what the accuracy or AUC for that method was. px_bm_fun &lt;- function(d, outcome, metric){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) m &lt;- if(metric == &quot;accuracy&quot;) &quot;Accuracy&quot; else &quot;AUC&quot; cap &lt;- sprintf(&quot;&lt;strong&gt;Table SX&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Feature Set and %s for Predicting %s for Each Participant&#39;s Best Model&lt;/em&gt;&quot;, m, o) tab &lt;- d %&gt;% select(SID, contains(&quot;glmnet&quot;), contains(&quot;biscwit&quot;), contains(&quot;rf&quot;)) %&gt;% kable(. , &quot;html&quot; , digits = 2 , col.names = c(&quot;ID&quot;, rep(c(&quot;Feature Set&quot;, m), times = 3)) , align = c(&quot;r&quot;, rep(c(&quot;l&quot;, &quot;c&quot;), times = 3)) , cap = cap ) %&gt;% kable_styling(full_width = F) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2)) save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/02-participant-best-models/%s_%s.html&quot;, res_path, outcome, metric)) return(tab) } px_best_mods &lt;- best_mods %&gt;% select(-.estimator, -.config, -set) %&gt;% mutate_at(vars(group, time), str_to_title) %&gt;% unite(group, group, time, sep = &quot;, &quot;) %&gt;% pivot_wider(names_from = &quot;model&quot; , values_from = c(&quot;group&quot;, &quot;.estimate&quot;) , names_glue = &quot;{model}_{.value}&quot;) %&gt;% group_by(outcome, .metric) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = pmap(list(data, outcome, .metric), px_bm_fun)); px_best_mods ## # A tibble: 4 x 4 ## outcome .metric data tab ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 prcrst accuracy &lt;tibble[,7] [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 2 prcrst roc_auc &lt;tibble[,7] [89 × 7]&gt; &lt;kablExtr [1]&gt; ## 3 lonely accuracy &lt;tibble[,7] [51 × 7]&gt; &lt;kablExtr [1]&gt; ## 4 lonely roc_auc &lt;tibble[,7] [51 × 7]&gt; &lt;kablExtr [1]&gt; Now, let’s print each of these tables and see what they demonstrate. 6.1.3.1.1 Procrastination, Accuracy Across people, accuracy was, on average (and median), very high. Accuracy estimates across models tended to be very similar for each person, which indicates that the models seemed to perform similarly. px_best_mods$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and Accuracy for Predicting Procrastinating for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set Accuracy Feature Set Accuracy Feature Set Accuracy 01 Full, No Time 0.91 Full, No Time 0.91 Psychological, No Time 0.91 02 Full, Time 0.85 Full, Time 0.77 Full, No Time 0.83 05 Full, No Time 0.92 Full, No Time 0.92 Psychological, No Time 0.92 09 Psychological, No Time 0.91 Psychological, No Time 0.91 Psychological, No Time 0.90 10 Psychological, No Time 0.82 Psychological, No Time 0.78 103 Full, No Time 0.62 Situations, No Time 0.69 Full, No Time 0.64 105 Full, No Time 0.92 Situations, Time 0.92 Full, No Time 0.92 106 Full, Time 0.88 Full, Time 0.81 Full, Time 0.87 107 Full, No Time 0.93 Psychological, No Time 0.93 Situations, No Time 0.93 108 Full, No Time 0.70 Psychological, No Time 0.70 Situations, No Time 0.58 110 Full, No Time 0.92 Full, Time 0.92 Situations, No Time 0.92 111 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 112 Psychological, Time 0.77 Full, Time 0.69 Psychological, Time 0.83 113 Full, No Time 0.90 118 Psychological, Time 0.80 Psychological, Time 0.80 Psychological, No Time 0.62 119 Psychological, Time 0.64 Full, No Time 0.64 Full, No Time 0.57 123 Situations, No Time 0.87 Psychological, Time 0.67 Full, No Time 0.79 124 Psychological, No Time 0.64 Full, No Time 0.64 Psychological, No Time 0.60 126 Psychological, Time 0.58 Psychological, Time 0.58 Full, Time 0.73 129 Psychological, No Time 0.50 Situations, No Time 0.67 Situations, No Time 0.83 133 Full, No Time 0.90 Full, No Time 0.90 Full, No Time 0.90 135 Full, No Time 0.93 Full, Time 0.93 Situations, No Time 0.93 146 Full, Time 0.67 Full, Time 0.67 Full, Time 0.67 148 Full, No Time 0.58 Situations, Time 0.67 Situations, No Time 0.64 149 Full, No Time 0.58 Full, No Time 0.58 Situations, No Time 0.63 150 Situations, No Time 0.88 Full, No Time 0.72 Situations, No Time 0.83 152 Full, No Time 0.88 Full, No Time 0.88 Situations, No Time 0.88 155 Situations, Time 0.67 Psychological, Time 0.67 Full, No Time 0.59 156 Full, No Time 0.48 Psychological, Time 0.52 Situations, No Time 0.55 157 Full, No Time 0.90 Full, No Time 0.90 Full, No Time 0.95 158 Situations, No Time 0.73 Full, No Time 0.60 Situations, No Time 0.93 159 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 160 Full, No Time 0.89 Full, No Time 0.89 Full, No Time 0.89 162 Full, No Time 0.93 Psychological, No Time 0.93 Situations, Time 0.92 164 Full, No Time 0.96 Situations, No Time 0.96 165 Full, No Time 0.64 Full, Time 0.73 Situations, No Time 0.70 166 Full, No Time 0.67 Full, No Time 0.67 Situations, No Time 0.82 167 Situations, Time 0.73 Full, No Time 0.64 Full, No Time 0.78 168 Full, No Time 0.62 Full, Time 0.62 Full, No Time 0.85 169 Psychological, No Time 0.94 Psychological, No Time 0.94 Psychological, No Time 0.93 17 Psychological, No Time 0.64 Psychological, Time 0.62 Situations, Time 0.77 170 Psychological, Time 0.94 Full, No Time 0.85 Full, No Time 0.83 171 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 174 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 185 Full, No Time 0.95 Psychological, Time 0.95 Full, No Time 0.95 186 Psychological, No Time 0.80 Full, No Time 0.80 Situations, No Time 0.71 188 Full, No Time 0.92 Full, No Time 0.83 Full, No Time 0.92 189 Psychological, Time 0.83 Situations, No Time 0.83 Situations, No Time 0.82 190 Psychological, No Time 0.93 Psychological, No Time 0.93 Psychological, Time 0.93 192 Full, No Time 0.92 Full, No Time 0.92 Full, No Time 0.92 195 Full, No Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 196 Full, No Time 0.83 Psychological, No Time 0.75 Situations, No Time 0.83 20 Full, No Time 0.79 Psychological, No Time 0.79 Situations, No Time 0.79 206 Full, No Time 0.89 Full, No Time 0.89 Situations, No Time 0.88 207 Full, No Time 0.86 Full, No Time 0.86 Full, No Time 0.85 21 Situations, No Time 0.60 Situations, No Time 0.67 Full, Time 0.57 211 Psychological, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 212 Full, Time 0.95 Psychological, No Time 0.95 Situations, No Time 0.95 214 Full, Time 0.94 Full, No Time 0.94 Situations, No Time 0.93 216 Full, No Time 0.78 Full, No Time 0.78 Full, No Time 0.83 22 Full, No Time 0.71 Full, No Time 0.79 Full, No Time 0.82 220 Situations, No Time 0.92 Psychological, Time 0.73 Full, No Time 0.91 25 Full, No Time 0.75 Psychological, No Time 0.67 Situations, No Time 0.75 26 Full, No Time 0.81 Full, Time 0.79 Full, Time 0.85 27 Full, No Time 0.83 Full, No Time 0.75 Full, Time 0.89 30 Full, Time 0.92 Full, Time 0.92 Full, Time 0.92 31 Full, No Time 0.93 Full, No Time 0.93 Psychological, No Time 0.92 32 Full, No Time 0.86 Situations, Time 0.86 Situations, No Time 0.86 33 Full, No Time 0.91 Full, Time 0.91 Situations, No Time 0.91 35 Full, No Time 0.75 Full, No Time 0.67 Full, No Time 0.82 38 Psychological, No Time 0.73 Situations, No Time 0.67 Situations, No Time 0.64 40 Full, No Time 0.91 Full, No Time 0.91 Full, No Time 0.91 43 Full, No Time 0.91 Situations, No Time 0.91 Full, No Time 0.90 49 Psychological, No Time 0.64 Psychological, No Time 0.64 Psychological, No Time 0.50 51 Psychological, No Time 0.92 Situations, No Time 0.91 Psychological, No Time 0.91 52 Full, No Time 0.91 Full, No Time 0.91 Situations, No Time 0.91 53 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 61 Psychological, No Time 0.91 Psychological, No Time 0.91 Psychological, No Time 0.91 63 Full, No Time 0.91 Full, No Time 0.91 Full, No Time 0.91 66 Full, No Time 0.91 Situations, No Time 0.91 67 Full, No Time 0.64 Full, No Time 0.64 Situations, No Time 0.69 71 Situations, No Time 0.82 Psychological, No Time 0.91 Full, No Time 0.70 73 Full, No Time 0.93 Psychological, No Time 0.93 Full, No Time 0.92 74 Full, No Time 0.93 Situations, No Time 0.92 77 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 78 Full, No Time 0.91 Full, No Time 0.90 84 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.92 85 Full, No Time 0.85 86 Full, Time 0.92 Full, No Time 0.93 Full, No Time 0.93 6.1.3.1.2 Procrastination, AUC Like accuracy, AUC in predicting future procrastination was quite high across the full sample. However, unlike accuracy, AUC tended to vary within-person across models suggesting that the accuracy results may have been somewhat misleading in suggesting that the models seemed to perform similarly. To better understand how, however, we will have to examine the features in more detail, as we will later in Questions 4 and 5. px_best_mods$tab[[2]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and AUC for Predicting Procrastinating for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set AUC Feature Set AUC Feature Set AUC 01 Situations, No Time 0.70 Situations, No Time 0.85 Full, No Time 0.56 02 Psychological, No Time 0.50 Situations, No Time 0.70 Full, Time 0.50 05 Full, No Time 0.91 Situations, No Time 0.95 Full, No Time 0.50 09 Psychological, No Time 0.20 Psychological, Time 0.70 Psychological, No Time 0.11 10 Psychological, No Time 0.56 Psychological, Time 0.57 103 Psychological, No Time 0.78 Full, No Time 0.57 Situations, No Time 0.72 105 Full, No Time 0.92 Full, No Time 0.42 Situations, No Time 0.82 106 Full, No Time 0.71 Situations, No Time 0.50 Psychological, Time 0.69 107 Situations, No Time 0.86 Full, No Time 0.86 Psychological, No Time 0.83 108 Full, No Time 0.67 Situations, Time 0.59 Psychological, Time 0.65 110 Full, No Time 0.82 Full, No Time 0.64 Situations, Time 0.50 111 Full, Time 0.67 Situations, Time 0.89 Psychological, No Time 0.88 112 Situations, Time 0.70 Psychological, Time 0.87 Psychological, Time 0.85 113 Situations, No Time 0.56 118 Psychological, Time 0.71 Psychological, No Time 0.29 Psychological, No Time 0.47 119 Psychological, No Time 0.69 Situations, Time 0.60 Psychological, Time 0.50 123 Situations, No Time 0.64 Psychological, No Time 0.69 Situations, Time 0.70 124 Psychological, No Time 0.50 Psychological, Time 0.79 Situations, No Time 0.58 126 Situations, Time 0.46 Full, No Time 0.77 Full, No Time 0.75 129 Situations, No Time 0.56 Psychological, Time 0.75 Situations, No Time 0.91 133 Full, No Time 0.33 Psychological, No Time 0.89 Full, No Time 0.89 135 Full, No Time 0.77 Situations, Time 0.77 Psychological, No Time 0.83 146 Psychological, No Time 0.54 Situations, Time 0.76 Situations, No Time 0.54 148 Situations, Time 0.49 Psychological, No Time 0.77 Situations, No Time 0.71 149 Psychological, No Time 0.44 Psychological, Time 0.75 Situations, No Time 0.75 150 Psychological, No Time 0.72 Situations, No Time 0.54 Situations, No Time 0.89 152 Situations, No Time 0.76 Psychological, Time 0.83 Situations, No Time 0.86 155 Situations, Time 0.71 Situations, Time 0.58 Situations, No Time 0.79 156 Situations, No Time 0.50 Situations, No Time 0.68 Situations, No Time 0.79 157 Situations, No Time 0.45 Psychological, No Time 0.79 Situations, Time 0.78 158 Full, Time 0.64 Situations, No Time 0.69 Situations, Time 0.85 159 Full, Time 0.50 Situations, No Time 0.69 Situations, No Time 0.67 160 Situations, No Time 0.94 Psychological, Time 0.84 Full, No Time 0.94 162 Situations, No Time 0.92 Full, Time 0.75 Full, Time 0.89 164 Full, No Time 0.91 Full, No Time 0.95 165 Situations, No Time 0.64 Full, No Time 0.71 Full, Time 0.95 166 Full, No Time 0.81 Psychological, No Time 0.53 Situations, Time 0.79 167 Situations, Time 0.77 Situations, Time 0.47 Full, No Time 0.67 168 Full, No Time 0.50 Full, No Time 0.70 Full, No Time 0.90 169 Psychological, No Time 0.80 Psychological, Time 0.40 Psychological, No Time 0.93 17 Psychological, No Time 0.88 Situations, No Time 0.58 Full, Time 0.78 170 Situations, No Time 0.91 Psychological, No Time 0.35 Situations, No Time 0.86 171 Full, Time 0.91 Full, Time 0.91 Situations, No Time 0.62 174 Situations, No Time 0.98 Psychological, Time 0.88 Situations, No Time 0.96 185 Situations, No Time 0.84 Situations, Time 0.89 Situations, No Time 0.53 186 Full, Time 0.93 Situations, No Time 0.16 Situations, Time 0.92 188 Situations, Time 0.55 Psychological, Time 0.91 Situations, No Time 0.55 189 Psychological, No Time 0.35 Full, Time 0.85 Situations, No Time 0.89 190 Psychological, Time 0.93 Psychological, No Time 0.79 Psychological, Time 0.86 192 Situations, Time 0.75 Psychological, No Time 0.83 Situations, No Time 0.67 195 Full, No Time 0.62 Full, No Time 0.75 Psychological, No Time 0.38 196 Full, No Time 0.50 Situations, Time 0.70 Full, No Time 0.67 20 Psychological, No Time 0.67 Situations, No Time 0.85 Situations, No Time 0.88 206 Psychological, No Time 0.41 Situations, Time 0.81 Situations, No Time 0.50 207 Situations, No Time 0.79 Full, Time 0.86 Full, No Time 0.91 21 Situations, Time 0.59 Psychological, No Time 0.56 Psychological, Time 0.52 211 Psychological, No Time 0.81 Psychological, No Time 0.88 Situations, No Time 0.56 212 Full, No Time 0.80 Psychological, No Time 0.55 Full, Time 0.78 214 Full, Time 0.87 Psychological, Time 0.87 Situations, No Time 0.93 216 Situations, No Time 0.68 Full, No Time 0.75 Situations, Time 0.49 22 Full, No Time 0.50 Psychological, Time 0.51 Full, No Time 0.50 220 Situations, No Time 0.50 Full, Time 0.90 Full, No Time 0.50 25 Full, No Time 0.78 Full, No Time 0.59 Full, No Time 0.50 26 Psychological, Time 0.67 Psychological, Time 0.70 Full, No Time 0.81 27 Situations, No Time 0.89 Psychological, Time 0.71 Situations, No Time 0.86 30 Psychological, Time 0.83 Full, No Time 0.79 Full, No Time 0.95 31 Full, No Time 0.92 Situations, No Time 0.81 Situations, Time 0.67 32 Situations, No Time 0.79 Psychological, No Time 0.54 Psychological, No Time 0.55 33 Full, No Time 0.80 Full, No Time 0.85 Psychological, Time 0.88 35 Psychological, No Time 0.67 Situations, No Time 0.43 Situations, No Time 0.61 38 Situations, No Time 0.44 Full, Time 0.73 Psychological, Time 0.55 40 Psychological, Time 0.70 Full, No Time 0.90 Situations, Time 0.60 43 Psychological, No Time 0.70 Situations, No Time 0.70 Situations, No Time 0.89 49 Psychological, No Time 0.87 Psychological, No Time 0.10 Psychological, No Time 0.50 51 Full, No Time 0.70 Full, Time 0.80 Full, Time 0.56 52 Full, No Time 0.90 Situations, Time 0.80 Full, Time 0.62 53 Full, No Time 0.79 Situations, No Time 0.64 Psychological, Time 0.50 61 Psychological, Time 0.80 Psychological, Time 0.70 Psychological, No Time 0.70 63 Situations, Time 0.80 Full, No Time 0.90 Psychological, No Time 0.80 66 Psychological, No Time 0.90 Full, Time 0.71 67 Psychological, No Time 0.44 Psychological, Time 0.98 Psychological, No Time 0.50 71 Full, Time 0.92 Full, Time 0.96 Situations, Time 0.71 73 Psychological, No Time 0.31 Full, Time 0.92 Psychological, No Time 0.75 74 Full, No Time 0.85 Situations, Time 0.92 77 Psychological, No Time 0.22 Full, No Time 0.78 Full, No Time 0.71 78 Situations, No Time 0.50 84 Psychological, Time 0.42 Full, No Time 0.88 Situations, No Time 0.83 85 Situations, No Time 0.61 86 Psychological, Time 0.92 Full, No Time 0.77 Full, No Time 0.85 6.1.3.1.3 Loneliness, Accuracy Across people, accuracy in predicting future loneliness was, on average (and median), very high. Accuracy estimates across models tended to be very similar for each person, which indicates that the models seemed to perform similarly. There are some exceptions to this, but the magnitude of these differences remains relatively small (magnitude of about .1 at most). px_best_mods$tab[[3]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and Accuracy for Predicting Lonely for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set Accuracy Feature Set Accuracy Feature Set Accuracy 08 Full, No Time 0.83 Full, No Time 0.92 Full, No Time 0.82 10 Psychological, No Time 0.91 Psychological, No Time 0.91 Psychological, No Time 0.89 103 Full, No Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 105 Psychological, No Time 0.80 Psychological, No Time 0.80 Situations, No Time 0.92 106 Full, Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 108 Full, Time 0.80 Psychological, No Time 0.80 Full, No Time 0.81 112 Full, No Time 0.69 Full, No Time 0.69 Situations, No Time 0.69 118 Psychological, Time 0.80 Psychological, Time 0.70 Psychological, Time 0.62 119 Full, No Time 0.93 Full, No Time 0.93 Full, No Time 0.93 123 Full, No Time 0.93 Full, No Time 0.93 Full, No Time 0.93 129 Psychological, No Time 0.67 Situations, No Time 0.67 Situations, Time 0.75 133 Full, No Time 0.90 Full, No Time 0.90 Full, No Time 0.90 135 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 146 Psychological, Time 0.94 Psychological, No Time 0.95 Psychological, No Time 0.95 149 Full, No Time 0.95 Full, No Time 0.95 Situations, No Time 0.95 150 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 152 Psychological, Time 0.96 Full, No Time 0.92 Situations, No Time 0.92 154 Full, Time 0.94 Full, No Time 0.94 Situations, No Time 0.94 155 Situations, Time 0.61 Psychological, No Time 0.78 Full, Time 0.71 157 Full, No Time 0.90 Full, No Time 0.90 Situations, No Time 0.90 158 Full, Time 0.93 Full, No Time 0.93 Full, No Time 0.93 160 Full, No Time 0.83 Full, No Time 0.83 Full, No Time 0.83 164 Full, No Time 0.96 Full, No Time 0.96 Situations, No Time 0.96 167 Full, No Time 0.91 Full, No Time 0.91 Situations, No Time 0.90 168 Psychological, No Time 0.69 Psychological, Time 0.77 Full, Time 0.92 170 Psychological, No Time 0.95 Psychological, No Time 0.95 Psychological, No Time 0.94 18 Full, No Time 0.95 Full, No Time 0.95 Situations, No Time 0.94 185 Full, No Time 0.65 Full, No Time 0.70 Full, No Time 0.65 186 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 196 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.92 20 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 201 Situations, No Time 0.82 Situations, No Time 0.82 Situations, No Time 0.91 206 Full, No Time 0.89 Full, No Time 0.89 Situations, No Time 0.88 207 Full, No Time 0.86 Psychological, No Time 0.86 Full, No Time 0.85 211 Psychological, No Time 0.81 Full, No Time 0.81 Full, No Time 0.83 212 Full, No Time 0.95 Full, No Time 0.95 Situations, No Time 0.95 214 Full, No Time 0.75 Full, No Time 0.75 Full, No Time 0.75 220 Full, No Time 0.92 Full, No Time 0.92 Full, No Time 0.91 25 Full, No Time 0.75 Psychological, No Time 0.75 Full, No Time 0.70 27 Full, No Time 0.83 Psychological, Time 0.82 Situations, No Time 0.83 32 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 35 Full, No Time 0.83 Psychological, No Time 0.75 Full, No Time 0.82 36 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.91 43 Situations, No Time 0.91 Situations, No Time 0.91 Situations, No Time 0.90 51 Situations, No Time 0.82 Psychological, Time 0.75 Psychological, No Time 0.73 53 Full, No Time 0.93 Full, No Time 0.93 Situations, No Time 0.93 63 Full, No Time 0.91 Full, No Time 0.91 Full, No Time 0.91 67 Situations, No Time 0.93 71 Full, No Time 0.91 Full, No Time 0.91 Situations, No Time 0.90 73 Full, No Time 0.93 Full, No Time 0.93 Full, No Time 0.92 84 Full, No Time 0.92 Full, No Time 0.92 Situations, No Time 0.92 6.1.3.1.4 Loneliness, AUC Like accuracy, AUC in predicting future loneliness was quite high across the full sample. However, unlike accuracy, AUC tended to vary within-person across models suggesting that the accuracy results may have been somewhat misleading in suggesting that the models seemed to perform similarly. To better understand how, however, we will have to examine the features in more detail, as we will later in Questions 4 and 5. px_best_mods$tab[[4]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.1: Table SXFeature Set and AUC for Predicting Lonely for Each Participant’s Best Model Elastic Net BISCWIT Random Forest ID Feature Set AUC Feature Set AUC Feature Set AUC 08 Full, Time 0.80 Situations, Time 0.30 Full, Time 0.78 10 Psychological, Time 0.80 Psychological, No Time 0.70 Psychological, No Time 0.88 103 Situations, Time 0.93 Psychological, No Time 0.87 Psychological, Time 0.92 105 Psychological, Time 0.72 Full, No Time 0.53 Situations, No Time 0.90 106 Psychological, Time 0.73 Full, No Time 0.97 Psychological, No Time 0.93 108 Psychological, No Time 0.50 Situations, No Time 0.86 Situations, Time 0.32 112 Psychological, Time 0.64 Situations, No Time 0.54 Situations, Time 0.75 118 Psychological, Time 0.44 Psychological, No Time 0.54 Psychological, Time 0.73 119 Situations, No Time 0.77 Psychological, Time 0.77 Full, Time 0.92 123 Full, No Time 0.93 Situations, No Time 0.50 Full, No Time 0.92 129 Full, No Time 0.86 Situations, No Time 0.29 Situations, No Time 0.97 133 Situations, No Time 0.33 Situations, No Time 0.94 Situations, No Time 0.89 135 Psychological, Time 0.92 Full, No Time 0.88 Situations, No Time 0.62 146 Psychological, No Time 0.89 Situations, Time 0.75 Situations, Time 0.50 149 Psychological, No Time 0.72 Situations, Time 0.94 Full, Time 0.86 150 Situations, No Time 0.83 Psychological, No Time 0.75 Full, No Time 0.95 152 Full, No Time 0.91 Situations, Time 0.34 Full, No Time 0.86 154 Situations, Time 0.93 Psychological, No Time 0.94 Full, No Time 0.86 155 Situations, No Time 0.48 Situations, No Time 0.64 Psychological, Time 0.86 157 Full, No Time 0.37 Full, Time 0.89 Full, No Time 0.94 158 Psychological, Time 0.93 Situations, No Time 0.89 Full, Time 0.92 160 Situations, No Time 0.78 Full, Time 0.42 Full, Time 0.79 164 Psychological, Time 0.86 Situations, Time 0.91 Full, No Time 0.70 167 Situations, Time 0.90 Full, No Time 0.60 Psychological, No Time 0.50 168 Situations, No Time 0.73 Psychological, No Time 0.48 Psychological, Time 0.97 170 Psychological, No Time 0.94 Psychological, No Time 0.22 Psychological, No Time 0.81 18 Situations, Time 0.72 Psychological, No Time 0.94 Situations, No Time 0.76 185 Psychological, No Time 0.52 Psychological, Time 0.68 Full, No Time 0.64 186 Full, Time 0.86 Situations, Time 0.93 Psychological, No Time 0.92 196 Psychological, No Time 0.82 Situations, No Time 0.41 Full, No Time 0.80 20 Full, No Time 0.08 Full, No Time 0.92 Situations, No Time 0.46 201 Psychological, No Time 0.83 Situations, No Time 0.69 Full, No Time 0.88 206 Psychological, No Time 0.69 Situations, No Time 0.72 Psychological, No Time 0.64 207 Situations, Time 0.95 Psychological, No Time 0.67 Full, No Time 0.86 211 Situations, Time 0.43 Full, Time 0.82 Psychological, Time 0.46 212 Situations, No Time 0.72 Psychological, Time 0.89 Full, No Time 0.78 214 Situations, No Time 0.58 Psychological, Time 0.48 Situations, Time 0.64 220 Full, No Time 0.64 Situations, Time 0.80 Psychological, Time 0.70 25 Situations, No Time 0.74 Full, No Time 0.59 Situations, No Time 0.74 27 Full, Time 0.83 Situations, No Time 0.62 Full, No Time 0.50 32 Full, Time 0.77 Psychological, No Time 0.92 Psychological, No Time 0.80 35 Psychological, No Time 0.75 Situations, No Time 0.85 Full, No Time 0.78 36 Situations, Time 0.60 Situations, No Time 0.91 Situations, No Time 0.70 43 Full, No Time 0.83 Psychological, No Time 0.67 Situations, No Time 0.88 51 Full, Time 0.83 Situations, Time 0.42 Situations, No Time 0.57 53 Situations, No Time 0.93 Full, No Time 0.75 Situations, No Time 0.93 63 Situations, Time 0.80 Full, No Time 0.90 Psychological, No Time 0.90 67 Situations, Time 0.54 71 Psychological, Time 0.60 Full, No Time 0.90 Situations, No Time 0.56 73 Psychological, No Time 0.92 Situations, No Time 0.92 Full, Time 0.83 84 Psychological, No Time 0.75 Situations, No Time 0.67 Psychological, No Time 0.80 6.1.3.2 Classification Accuracy and AUC 6.1.3.2.1 Table Similar to how we created tables for each outcome, feature set, and metric in the first section, we will next create a single, similar table for participants best models, summarizing the mean, standard deviation, median, and range for each outcome, method, and metric. In the manuscript, this will be summarized in a figure, but I’m still creating the table for ease of access. bm_tab &lt;- best_mods %&gt;% group_by(model, outcome, .metric) %&gt;% summarize_at(vars(.estimate), lst(mean, sd, median, min, max, n=~sum(!is.na(.)))) %&gt;% ungroup() %&gt;% mutate(sd = ifelse(sd &lt; .01, &quot;&lt;.01&quot;, sprintf(&quot;%.2f&quot;, sd)), mean = sprintf(&quot;%.2f (%s)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max), median = sprintf(&quot;%.2f&quot;, median), model = factor(model, levels = c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;)), .metric = factor(.metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;)) ) %&gt;% select(-sd, -min, -max) %&gt;% pivot_wider(names_from = &quot;outcome&quot; , values_from = c(mean, median, range, n) , names_glue = &quot;{outcome}_{.value}&quot;) %&gt;% arrange(model, .metric) %&gt;% select(.metric, contains(&quot;lonely&quot;), contains(&quot;prcrst&quot;)) %&gt;% kable(. , &quot;html&quot; , escape = F , col.names = c(&quot;Metric&quot;, rep(c(&quot;&lt;em&gt;M&lt;/em&gt; (&lt;em&gt;SD&lt;/em&gt;)&quot;, &quot;Median&quot;, &quot;Range&quot;, &quot;&lt;em&gt;N&lt;/em&gt;&quot;), times = 2)) , align = c(&quot;r&quot;, rep(&quot;c&quot;,8)) , cap = &quot;&lt;strong&gt;Table X&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Descriptive Statistics of Model Performance Across of the Best Performing Model for Each Participant&lt;/em&gt;&quot; ) %&gt;% kable_styling(full_width = F) %&gt;% kableExtra::group_rows(&quot;Elastic Net&quot;, 1, 2) %&gt;% kableExtra::group_rows(&quot;BISCWIT&quot;, 3, 4) %&gt;% kableExtra::group_rows(&quot;Random Forest&quot;, 5, 6) %&gt;% add_header_above(c(&quot; &quot; = 1, &quot;Loneliness&quot; = 4, &quot;Procrastination&quot; = 4)) %&gt;% footnote(&quot;Accuracy = Classification accuracy; AUC = Area under the receiver operating characteristic (ROC) curve.&quot;) save_kable(bm_tab, file = sprintf(&quot;%s/05-results/04-tables/01-best-summary.html&quot;, res_path)) bm_tab Table 6.2: Table XDescriptive Statistics of Model Performance Across of the Best Performing Model for Each Participant Loneliness Procrastination Metric M (SD) Median Range N M (SD) Median Range N Elastic Net Accuracy 0.87 (0.09) 0.91 0.61-0.96 50 0.82 (0.13) 0.88 0.48-0.96 82 AUC 0.74 (0.19) 0.77 0.08-0.95 50 0.69 (0.19) 0.70 0.20-0.98 82 BISCWIT Accuracy 0.87 (0.08) 0.92 0.67-0.96 51 0.82 (0.12) 0.88 0.52-0.96 89 AUC 0.71 (0.21) 0.75 0.22-0.97 51 0.71 (0.18) 0.75 0.10-0.98 89 Random Forest Accuracy 0.87 (0.09) 0.91 0.62-0.96 50 0.83 (0.12) 0.89 0.50-0.96 87 AUC 0.77 (0.16) 0.80 0.32-0.97 50 0.71 (0.17) 0.72 0.11-0.96 86 Note: Accuracy = Classification accuracy; AUC = Area under the receiver operating characteristic (ROC) curve. There are a few key takeaways from this table. First, accuracy across all models and outcomes was quite high, with mean accuracy of .87 (Median .91 to .92) for loneliness and between .82 and .83 (Median .88 to .89) for procrastination. Similarly, AUC was also well above the .5 threshold with means ranging from .70 to .76 (Median .75 to .80) for loneliness and .69 to .70 (Median .70 to .75) for procrastination. 6.1.3.2.2 Figure (Figure 1) Now, we’ll create distributions of the performance (accuracy, AUC) of participants’ best models and plot those along with the descriptive statistics that were created for the Supplementary Table in the previous section. This figure will become Figure 1 in the manuscript. p_dist_fun &lt;- function(d, outcome) { o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) d %&gt;% mutate(model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;) , c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)) , .metric = factor(.metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;)) , group = factor(str_to_title(group))) %&gt;% ggplot(aes(y = model, x = .estimate)) + scale_x_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + geom_density_ridges(aes(fill = model), alpha = .5) + stat_pointinterval() + geom_vline(aes(xintercept = .5), linetype = &quot;dashed&quot;) + labs(x = NULL, y = NULL, title = o) + facet_wrap(~.metric, scales = &quot;free&quot;, nrow = 2) + theme_classic() + theme(legend.position = &quot;none&quot; , axis.text = element_text(face = &quot;bold&quot;) , axis.title = element_text(face = &quot;bold&quot;) , strip.background = element_blank() , strip.text.y = element_blank() , plot.margin = margin(.1,.1,1,.1, unit = &quot;cm&quot;) , strip.text = element_text(face = &quot;bold&quot;, size = rel(1.2)) , plot.title = element_text(face = &quot;bold&quot;, size = rel(1.2), hjust = .5)) } bm_dist &lt;- best_mods %&gt;% group_by(outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = map2(data, outcome, p_dist_fun)) tab_fun &lt;- function(d){ tab &lt;- d %&gt;% select(-model) %&gt;% setNames(c(&quot;M (SD)&quot;, &quot;Median&quot;, &quot;N&quot;, &quot;Range&quot;)) %&gt;% tableGrob(rows = NULL , theme = ttheme_minimal(base_family = &quot;Times&quot;)) tab &lt;- gtable_add_grob(tab, grobs = segmentsGrob( # line across the bottom x0 = unit(0,&quot;npc&quot;), y0 = unit(0,&quot;npc&quot;), x1 = unit(1,&quot;npc&quot;), y1 = unit(0,&quot;npc&quot;), gp = gpar(lwd = 2.0)), t = 1, b = 1, l = 1, ncol(tab)) tab$grobs[1:4] &lt;- lapply(tab$grobs[1:4], function(x) {x$grobs[[1]]$gp$fontface = &quot;bold&quot;; return(x)}) return(tab) } bm_tbl &lt;- best_mods %&gt;% group_by(model, outcome, .metric) %&gt;% summarize_at(vars(.estimate), lst(mean, sd, median, min, max, n=~sum(!is.na(.)))) %&gt;% ungroup() %&gt;% mutate(sd = ifelse(sd &lt; .01, &quot;&lt;.01&quot;, sprintf(&quot;%.2f&quot;, sd)), mean = sprintf(&quot;%.2f (%s)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max), median = sprintf(&quot;%.2f&quot;, median), model = factor(model, levels = c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), labels = c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), .metric = factor(.metric, c(&quot;accuracy&quot;, &quot;roc_auc&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;)) ) %&gt;% select(-sd, -min, -max) %&gt;% arrange(outcome, .metric, model) %&gt;% group_by(outcome, .metric) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = map(data, tab_fun)) my_theme &lt;- function(...) { theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) } title_theme &lt;- calc_element(&quot;plot.title&quot;, my_theme()) ttl &lt;- ggdraw() + draw_label( &quot;Procrastination&quot;, fontfamily = title_theme$family, fontface = title_theme$face, size = title_theme$size ) bm_dist$p[[1]] &lt;- bm_dist$p[[1]] + labs(title = NULL) bm_tab1 &lt;- plot_grid(bm_tbl$tab[[3]], bm_tbl$tab[[4]], nrow = 2, rel_heights = c(.4, .4)) bm_prcrst &lt;- plot_grid(bm_dist$p[[1]], bm_tab1, ncol = 2) bm_prcrst &lt;- plot_grid(ttl, bm_prcrst, nrow = 2, rel_heights = c(.05,.95)) bm_dist$p[[2]] &lt;- bm_dist$p[[2]] + labs(title = NULL) + theme(axis.text.y = element_blank()) ttl &lt;- ggdraw() + draw_label( &quot;Loneliness&quot;, fontfamily = title_theme$family, fontface = title_theme$face, size = title_theme$size ) bm_tab2 &lt;- plot_grid(bm_tbl$tab[[1]], bm_tbl$tab[[2]], nrow = 2) bm_lonely &lt;- plot_grid(bm_dist$p[[2]], bm_tab2, ncol = 2, rel_widths = c(.4, .6)) bm_lonely &lt;- plot_grid(ttl, bm_lonely, nrow = 2, rel_heights = c(.05,.95)) bm_plot &lt;- plot_grid(bm_prcrst, bm_lonely, ncol = 2, rel_widths = c(.55, .45)); bm_plot Figure 6.1: Histograms of classification accuracy and Area Under the Receiver Operator Curve (AUC) for participants’ best models. ggsave(bm_plot, file = sprintf(&quot;%s/05-results/05-figures/fig-1-best-models.pdf&quot;, res_path) , width = 12, height = 5) Figure 1 presents histograms and descriptive statistics of accuracy and AUC across the full sample for each outcome and model. As is clear in the figure, predictive accuracy was high overall, with mean accuracy of .87 (Median .91 to .92) for loneliness and between .82 and .83 (Median .88 to .89) for procrastination. Similarly, AUC was also well above the .5 threshold with means ranging from .70 to .76 (Median .75 to .80) for loneliness and .69 to .70 (Median .70 to .75) for procrastination. 6.1.4 Tuning Parameters (Table) Next, I’m going to create tables that include the tuning parameters, features, and accuracy for each participants best model for each machine learning method and outcome as well as which feature set was used in their best model. px_tun_par_tab_fun &lt;- function(d, model, outcome){ if(model == &quot;glmnet&quot;){ cn &lt;- c(&quot;ID&quot;,&quot;Group&quot;, &quot;Penalty&quot;, &quot;Mixture&quot;, &quot;# Features&quot;, &quot;Accuracy&quot;) al &lt;- c(rep(&quot;r&quot;, 2), rep(&quot;c&quot;, 4)) tab &lt;- d %&gt;% mutate(group = str_to_title(paste(group, time, sep = &quot;, &quot;))) %&gt;% select(SID, group, penalty, mixture, nvars, .estimate) %&gt;% mutate(penalty = ifelse(penalty &lt; .01, sprintf(&quot;%.1e&quot;, penalty), sprintf(&quot;%.2f&quot;, penalty)) , .estimate = sprintf(&quot;%.2f&quot;, .estimate) , mixture = ifelse(mixture == 0, &quot;0&quot;, sprintf(&quot;%.2f&quot;, mixture))) } else if(model == &quot;rf&quot;){ cn &lt;- c(&quot;ID&quot;,&quot;Group&quot;, &quot;# Features Sampled&quot;, &quot;Min N for Split&quot;, &quot;# Features&quot;, &quot;Accuracy&quot;) al &lt;- c(rep(&quot;r&quot;, 2), rep(&quot;c&quot;, 4)) tab &lt;- d %&gt;% mutate(group = str_to_title(paste(group, time, sep = &quot;, &quot;))) %&gt;% select(SID, group, mtry, min_n, nvars, .estimate) %&gt;% mutate(.estimate = sprintf(&quot;%.2f&quot;, .estimate)) } else { cn &lt;- c(&quot;ID&quot;,&quot;Group&quot;, &quot;# Items&quot;, &quot;# Features&quot;, &quot;Accuracy&quot;) al &lt;- c(rep(&quot;r&quot;, 2), rep(&quot;c&quot;, 3)) tab &lt;- d %&gt;% mutate(group = str_to_title(paste(group, time, sep = &quot;, &quot;))) %&gt;% select(SID, group, nitem, nvars, .estimate) %&gt;% mutate(.estimate = sprintf(&quot;%.2f&quot;, .estimate)) } o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) m &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;rf&quot;, &quot;biscwit&quot;), c(&quot;Elastic Net&quot;, &quot;Random Forest&quot;, &quot;BISCWIT&quot;)) cap &lt;- sprintf(&quot;&lt;strong&gt;Table X&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Tuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants&#39; Best Model of %s Using %s&quot;, o, m) tab &lt;- tab %&gt;% kable(. , &quot;html&quot; , escape = &quot;F&quot; , col.names = cn , align = al , cap = cap ) %&gt;% kable_styling(full_width = F) save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/03-px-tuning-params/%s_%s.html&quot;, res_path, outcome, model)) return(tab) } tuning_param &lt;- param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config)) %&gt;% select(-coefs) %&gt;% group_by(outcome, model) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = map(data, ~(.) %&gt;% unnest(params) %&gt;% filter(.metric == &quot;accuracy&quot;)), tab = pmap(list(data, model, outcome), px_tun_par_tab_fun)) 6.1.4.1 Elastic Net Rather than splitting these by outcome and model, I’m going to do a broad discussion across outcomes. From the tables, a few things become clear – penalties tended to be quite low (near 0) or quite high (near 1). Indeed, of the 10 tested values, only 3 appeared: 0.0000000001, 0.08, and 1.00. For mixture, the most frequent value was 0, but there was was a also more variability than for penalty, with almost all of the 10 possible values being represented. The number of features tended to vary quite widely and does not appear to be a function of stronger penalties or mixture values. The tuning parameters also appear to have little effect on model accuracy. 6.1.4.1.1 Procrastination (tuning_param %&gt;% filter(model == &quot;glmnet&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Procrastinating Using Elastic Net ID Group Penalty Mixture # Features Accuracy 01 Full, No Time 1.0e-10 0 49 0.91 02 Full, Time 1.0e-10 0 64 0.85 05 Full, No Time 1.0e-10 0 46 0.92 09 Psychological, No Time 1.0e-10 0 25 0.91 103 Full, No Time 1.00 0.22 43 0.62 105 Full, No Time 0.08 0.78 19 0.92 106 Full, Time 1.00 0.11 57 0.88 107 Full, No Time 0.08 0.11 42 0.93 108 Full, No Time 0.08 0.11 43 0.70 110 Full, No Time 1.0e-10 0 45 0.92 111 Full, No Time 1.0e-10 0.11 44 0.90 112 Psychological, Time 1.0e-10 0 41 0.77 118 Psychological, Time 1.0e-10 1.00 18 0.80 119 Psychological, Time 1.0e-10 0 41 0.64 123 Situations, No Time 0.08 0.22 22 0.87 124 Psychological, No Time 1.00 0.33 24 0.64 126 Psychological, Time 1.0e-10 0 42 0.58 129 Psychological, No Time 1.0e-10 0.89 17 0.50 133 Full, No Time 1.0e-10 0 49 0.90 135 Full, No Time 1.0e-10 0 45 0.93 146 Full, Time 1.0e-10 0 62 0.67 148 Full, No Time 1.0e-10 0 47 0.58 149 Full, No Time 0.08 0.33 41 0.58 150 Situations, No Time 0.08 0.44 17 0.88 152 Full, No Time 1.00 0 45 0.88 155 Situations, Time 1.0e-10 0.11 36 0.67 156 Full, No Time 1.00 0 46 0.48 157 Full, No Time 1.0e-10 0 44 0.90 158 Situations, No Time 0.08 1.00 19 0.73 159 Full, No Time 1.0e-10 0 47 0.90 160 Full, No Time 1.0e-10 0 46 0.89 162 Full, No Time 1.0e-10 0 46 0.93 165 Full, No Time 1.0e-10 0 47 0.64 166 Full, No Time 1.0e-10 0 42 0.67 167 Situations, Time 1.0e-10 0.11 30 0.73 168 Full, No Time 1.00 0.11 45 0.62 169 Psychological, No Time 0.08 0 25 0.94 17 Psychological, No Time 1.0e-10 0 25 0.64 170 Psychological, Time 1.0e-10 0 41 0.94 171 Full, No Time 1.0e-10 0 28 0.96 174 Full, No Time 1.00 0 30 0.96 185 Full, No Time 1.00 0 45 0.95 186 Psychological, No Time 1.0e-10 0 24 0.80 188 Full, No Time 1.0e-10 0 49 0.92 189 Psychological, Time 1.0e-10 0 41 0.83 190 Psychological, No Time 1.00 0.11 25 0.93 192 Full, No Time 1.0e-10 0 48 0.92 195 Full, No Time 1.0e-10 0 47 0.94 196 Full, No Time 1.00 0.22 37 0.83 20 Full, No Time 1.0e-10 0 46 0.79 206 Full, No Time 1.0e-10 0 49 0.89 207 Full, No Time 1.0e-10 0 47 0.86 21 Situations, No Time 1.00 0 22 0.60 211 Psychological, No Time 1.00 0 25 0.96 212 Full, Time 1.00 0.11 59 0.95 214 Full, Time 1.0e-10 0 63 0.94 216 Full, No Time 1.0e-10 0 34 0.78 22 Full, No Time 1.00 0.22 40 0.71 220 Situations, No Time 1.00 0.22 23 0.92 25 Full, No Time 1.0e-10 0.78 28 0.75 26 Full, No Time 1.0e-10 0 48 0.81 27 Full, No Time 6.0e-03 0.89 28 0.83 30 Full, Time 1.0e-10 0 63 0.92 31 Full, No Time 1.0e-10 0 44 0.93 32 Full, No Time 1.0e-10 0 49 0.86 33 Full, No Time 1.0e-10 0 45 0.91 35 Full, No Time 1.0e-10 0 44 0.75 38 Psychological, No Time 0.08 0.11 25 0.73 40 Full, No Time 1.0e-10 0 46 0.91 43 Full, No Time 1.0e-10 0 47 0.91 49 Psychological, No Time 0.08 0 24 0.64 51 Psychological, No Time 1.00 0.22 23 0.92 52 Full, No Time 1.0e-10 0 47 0.91 53 Full, No Time 1.0e-10 0 47 0.93 61 Psychological, No Time 1.0e-10 0 25 0.91 63 Full, No Time 1.0e-10 0 49 0.91 67 Full, No Time 1.0e-10 0 48 0.64 71 Situations, No Time 1.0e-10 0 21 0.82 73 Full, No Time 1.00 0.11 43 0.93 77 Full, No Time 1.0e-10 0 48 0.90 84 Full, No Time 1.0e-10 0 47 0.92 86 Full, Time 1.0e-10 0 63 0.92 6.1.4.1.2 Loneliness (tuning_param %&gt;% filter(model == &quot;glmnet&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Lonely Using Elastic Net ID Group Penalty Mixture # Features Accuracy 08 Full, No Time 1.0e-10 0 48 0.83 10 Psychological, No Time 1.0e-10 0 23 0.91 103 Full, No Time 1.0e-10 0 47 0.94 105 Psychological, No Time 1.00 0 25 0.80 106 Full, Time 1.0e-10 0 62 0.94 108 Full, Time 1.0e-10 0 60 0.80 112 Full, No Time 1.0e-10 0 46 0.69 118 Psychological, Time 1.0e-10 0 41 0.80 119 Full, No Time 1.0e-10 0 48 0.93 123 Full, No Time 1.0e-10 0 46 0.93 129 Psychological, No Time 1.0e-10 0 25 0.67 133 Full, No Time 1.0e-10 0 49 0.90 135 Full, No Time 1.0e-10 0 45 0.93 146 Psychological, Time 1.0e-10 0 41 0.94 149 Full, No Time 1.0e-10 0 46 0.95 150 Full, No Time 1.00 0 42 0.96 152 Psychological, Time 1.0e-10 0 42 0.96 154 Full, Time 1.0e-10 0 64 0.94 155 Situations, Time 1.00 0.11 36 0.61 157 Full, No Time 1.0e-10 0 44 0.90 158 Full, Time 1.0e-10 0 60 0.93 160 Full, No Time 1.0e-10 0 46 0.83 164 Full, No Time 1.0e-10 0 42 0.96 167 Full, No Time 1.0e-10 0 47 0.91 168 Psychological, No Time 1.0e-10 0 25 0.69 170 Psychological, No Time 1.00 0 25 0.95 18 Full, No Time 0.08 0 47 0.95 185 Full, No Time 1.00 0 45 0.65 186 Full, No Time 1.0e-10 0 45 0.93 196 Full, No Time 0.08 0.67 27 0.92 20 Full, No Time 1.0e-10 0 46 0.93 201 Situations, No Time 1.0e-10 0 23 0.82 206 Full, No Time 1.00 0.11 48 0.89 207 Full, No Time 1.0e-10 0 47 0.86 211 Psychological, No Time 0.08 0 25 0.81 212 Full, No Time 1.0e-10 0 48 0.95 214 Full, No Time 1.0e-10 0 46 0.75 220 Full, No Time 1.0e-10 0 48 0.92 25 Full, No Time 0.08 1.00 19 0.75 27 Full, No Time 1.0e-10 0 49 0.83 32 Full, No Time 1.0e-10 0 49 0.93 35 Full, No Time 1.0e-10 0 44 0.83 36 Full, No Time 1.0e-10 0 48 0.92 43 Situations, No Time 1.0e-10 0.44 18 0.91 51 Situations, No Time 1.0e-10 0 22 0.82 53 Full, No Time 1.0e-10 0 47 0.93 63 Full, No Time 1.0e-10 0 49 0.91 71 Full, No Time 1.0e-10 0 46 0.91 73 Full, No Time 1.0e-10 0 49 0.93 84 Full, No Time 1.0e-10 0 47 0.92 6.1.4.2 BISCWIT 6.1.4.2.1 Procrastination The only turning parameter for BISCWIT was the number of items selected through rolling origin validation. As is clear in the table, relative to ENR, BISCWIT tended to select fewer features (e.g., Participant 01 had the full feature set with 22 features for BISCWIT but 49 features for ENR). Divergences in feature numbers are due to ties. As with ENR, the number of features did not appear to be related to the accuracy of the model and the was a wide range in which feature set produced the best model. (tuning_param %&gt;% filter(model == &quot;biscwit&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Procrastinating Using BISCWIT ID Group # Items # Features Accuracy 01 Full, No Time 21 22 0.91 02 Full, Time 3 5 0.77 05 Full, No Time 6 7 0.92 09 Psychological, No Time 3 4 0.91 10 Psychological, No Time 6 7 0.82 103 Situations, No Time 18 17 0.69 105 Situations, Time 3 4 0.92 106 Full, Time 6 7 0.81 107 Psychological, No Time 9 10 0.93 108 Psychological, No Time 21 13 0.70 110 Full, Time 6 7 0.92 111 Full, No Time 24 25 0.90 112 Full, Time 51 38 0.69 113 Full, No Time 3 4 0.90 118 Psychological, Time 3 4 0.80 119 Full, No Time 36 26 0.64 123 Psychological, Time 12 13 0.67 124 Full, No Time 24 25 0.64 126 Psychological, Time 30 26 0.58 129 Situations, No Time 18 19 0.67 133 Full, No Time 3 4 0.90 135 Full, Time 39 37 0.93 146 Full, Time 3 4 0.67 148 Situations, Time 3 4 0.67 149 Full, No Time 3 4 0.58 150 Full, No Time 3 4 0.72 152 Full, No Time 15 11 0.88 155 Psychological, Time 6 7 0.67 156 Psychological, Time 27 20 0.52 157 Full, No Time 3 4 0.90 158 Full, No Time 9 10 0.60 159 Full, No Time 6 7 0.90 160 Full, No Time 18 19 0.89 162 Psychological, No Time 3 4 0.93 164 Full, No Time 3 4 0.96 165 Full, Time 24 25 0.73 166 Full, No Time 36 25 0.67 167 Full, No Time 9 10 0.64 168 Full, Time 36 33 0.62 169 Psychological, No Time 9 10 0.94 17 Psychological, Time 9 10 0.62 170 Full, No Time 21 22 0.85 171 Full, No Time 3 4 0.96 174 Full, No Time 3 4 0.96 185 Psychological, Time 24 21 0.95 186 Full, No Time 15 16 0.80 188 Full, No Time 36 31 0.83 189 Situations, No Time 6 8 0.83 190 Psychological, No Time 12 13 0.93 192 Full, No Time 6 7 0.92 195 Full, No Time 18 19 0.94 196 Psychological, No Time 15 13 0.75 20 Psychological, No Time 3 4 0.79 206 Full, No Time 3 4 0.89 207 Full, No Time 3 4 0.86 21 Situations, No Time 9 10 0.67 211 Full, No Time 3 4 0.96 212 Psychological, No Time 18 13 0.95 214 Full, No Time 9 10 0.94 216 Full, No Time 6 7 0.78 22 Full, No Time 3 4 0.79 220 Psychological, Time 3 4 0.73 25 Psychological, No Time 6 7 0.67 26 Full, Time 36 34 0.79 27 Full, No Time 9 10 0.75 30 Full, Time 3 4 0.92 31 Full, No Time 9 10 0.93 32 Situations, Time 3 4 0.86 33 Full, Time 3 4 0.91 35 Full, No Time 15 16 0.67 38 Situations, No Time 3 4 0.67 40 Full, No Time 3 4 0.91 43 Situations, No Time 15 14 0.91 49 Psychological, No Time 3 4 0.64 51 Situations, No Time 3 4 0.91 52 Full, No Time 3 6 0.91 53 Full, No Time 15 16 0.93 61 Psychological, No Time 18 17 0.91 63 Full, No Time 15 16 0.91 66 Full, No Time 24 25 0.91 67 Full, No Time 15 16 0.64 71 Psychological, No Time 3 4 0.91 73 Psychological, No Time 6 7 0.93 74 Full, No Time 9 10 0.93 77 Full, No Time 6 7 0.90 78 Full, No Time 9 11 0.91 84 Full, No Time 3 4 0.92 86 Full, No Time 12 13 0.93 6.1.4.2.2 Loneliness (tuning_param %&gt;% filter(model == &quot;biscwit&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Lonely Using BISCWIT ID Group # Items # Features Accuracy 08 Full, No Time 3 5 0.92 10 Psychological, No Time 3 4 0.91 103 Full, No Time 33 31 0.94 105 Psychological, No Time 3 4 0.80 106 Full, No Time 3 4 0.94 108 Psychological, No Time 3 4 0.80 112 Full, No Time 6 7 0.69 118 Psychological, Time 15 16 0.70 119 Full, No Time 15 16 0.93 123 Full, No Time 9 10 0.93 129 Situations, No Time 9 12 0.67 133 Full, No Time 36 34 0.90 135 Full, No Time 3 5 0.93 146 Psychological, No Time 3 4 0.95 149 Full, No Time 33 24 0.95 150 Full, No Time 3 4 0.96 152 Full, No Time 3 4 0.92 154 Full, No Time 36 35 0.94 155 Psychological, No Time 6 7 0.78 157 Full, No Time 3 4 0.90 158 Full, No Time 18 19 0.93 160 Full, No Time 3 4 0.83 164 Full, No Time 3 4 0.96 167 Full, No Time 3 4 0.91 168 Psychological, Time 9 10 0.77 170 Psychological, No Time 3 4 0.95 18 Full, No Time 12 13 0.95 185 Full, No Time 3 4 0.70 186 Full, No Time 24 21 0.93 196 Full, No Time 18 19 0.92 20 Full, No Time 3 4 0.93 201 Situations, No Time 3 4 0.82 206 Full, No Time 3 4 0.89 207 Psychological, No Time 3 4 0.86 211 Full, No Time 3 4 0.81 212 Full, No Time 6 7 0.95 214 Full, No Time 6 7 0.75 220 Full, No Time 6 7 0.92 25 Psychological, No Time 15 14 0.75 27 Psychological, Time 33 33 0.82 32 Full, No Time 3 4 0.93 35 Psychological, No Time 9 10 0.75 36 Full, No Time 3 4 0.92 43 Situations, No Time 12 13 0.91 51 Psychological, Time 24 25 0.75 53 Full, No Time 3 4 0.93 63 Full, No Time 15 16 0.91 71 Full, No Time 3 4 0.91 73 Full, No Time 6 7 0.93 84 Full, No Time 3 4 0.92 6.1.4.3 Random Forest Random forest used two tuning parameters, the number of features sampled from the feature set to train the model and the minimum sample size in each group needed for a binary split. The number of features sampled in each small tree tended to be smaller than the final number of features selected but varied widely across people. The minimum N for a split also varied quite widely. However, 10 was the most frequent number, which logically makes sense given the sample sizes in the present study. Because we used time delay embedding to preserve the “order” of the time series, the final number of features here tended to be larger than other methods (the number of possible features was doubled using an embedding dimension of 1). Each of these appeared to unrelated to accuracy. ##### Procrastination (tuning_param %&gt;% filter(model == &quot;rf&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Procrastinating Using Random Forest ID Group # Features Sampled Min N for Split # Features Accuracy 01 Psychological, No Time 17 10 29 0.91 02 Full, No Time 32 10 80 0.83 05 Psychological, No Time 17 10 46 0.92 09 Psychological, No Time 17 10 0 0.90 10 Psychological, No Time 16 10 27 0.78 103 Full, No Time 32 10 80 0.64 105 Full, No Time 32 10 74 0.92 106 Full, Time 2 40 105 0.87 107 Situations, No Time 6 35 29 0.93 108 Situations, No Time 1 40 35 0.58 110 Situations, No Time 15 10 39 0.92 111 Situations, No Time 1 40 0 0.90 112 Psychological, Time 23 10 67 0.83 118 Psychological, No Time 17 10 50 0.62 119 Full, No Time 33 10 74 0.57 123 Full, No Time 32 10 82 0.79 124 Psychological, No Time 38 33 0 0.60 126 Full, Time 39 10 97 0.73 129 Situations, No Time 8 3 45 0.83 133 Full, No Time 32 10 0 0.90 135 Situations, No Time 16 10 40 0.93 146 Full, Time 36 10 28 0.67 148 Situations, No Time 33 21 38 0.64 149 Situations, No Time 1 40 42 0.63 150 Situations, No Time 6 3 33 0.83 152 Situations, No Time 1 40 40 0.88 155 Full, No Time 31 10 87 0.59 156 Situations, No Time 32 28 39 0.55 157 Full, No Time 31 10 76 0.95 158 Situations, No Time 7 3 36 0.93 159 Situations, No Time 15 10 26 0.90 160 Full, No Time 2 40 87 0.89 162 Situations, Time 44 21 16 0.92 164 Situations, No Time 12 10 24 0.96 165 Situations, No Time 15 10 41 0.70 166 Situations, No Time 10 9 39 0.82 167 Full, No Time 88 5 82 0.78 168 Full, No Time 63 28 61 0.85 169 Psychological, No Time 17 10 41 0.93 17 Situations, Time 15 9 66 0.77 170 Full, No Time 32 10 49 0.83 171 Situations, No Time 9 10 17 0.96 174 Situations, No Time 9 10 23 0.96 185 Full, No Time 30 10 78 0.95 186 Situations, No Time 13 10 26 0.71 188 Full, No Time 33 10 63 0.92 189 Situations, No Time 15 10 38 0.82 190 Psychological, Time 23 10 62 0.93 192 Full, No Time 32 10 28 0.92 195 Situations, No Time 15 10 23 0.94 196 Situations, No Time 1 40 0 0.83 20 Situations, No Time 8 3 45 0.79 206 Situations, No Time 17 10 42 0.88 207 Full, No Time 31 10 57 0.85 21 Full, Time 2 40 107 0.57 211 Situations, No Time 10 10 20 0.96 212 Situations, No Time 8 3 47 0.95 214 Situations, No Time 7 35 35 0.93 216 Full, No Time 24 10 56 0.83 22 Full, No Time 2 40 0 0.82 220 Full, No Time 2 40 0 0.91 25 Situations, No Time 1 40 0 0.75 26 Full, Time 2 40 0 0.85 27 Full, Time 37 10 97 0.89 30 Full, Time 37 10 32 0.92 31 Psychological, No Time 15 10 23 0.92 32 Situations, No Time 1 40 0 0.86 33 Situations, No Time 14 10 16 0.91 35 Full, No Time 30 10 65 0.82 38 Situations, No Time 15 10 45 0.64 40 Full, No Time 31 10 49 0.91 43 Full, No Time 32 10 72 0.90 49 Psychological, No Time 1 40 0 0.50 51 Psychological, No Time 8 35 0 0.91 52 Situations, No Time 14 10 2 0.91 53 Situations, No Time 15 10 31 0.93 61 Psychological, No Time 17 10 0 0.91 63 Full, No Time 33 10 38 0.91 66 Situations, No Time 16 10 25 0.91 67 Situations, No Time 15 10 32 0.69 71 Full, No Time 14 35 0 0.70 73 Full, No Time 2 40 0 0.92 74 Situations, No Time 15 10 20 0.92 77 Situations, No Time 15 10 30 0.90 78 Full, No Time 32 10 32 0.90 84 Situations, No Time 15 10 24 0.92 86 Full, No Time 31 10 57 0.93 6.1.4.3.1 Loneliness (tuning_param %&gt;% filter(model == &quot;biscwit&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] Table 6.3: Table XTuning Parameters, Final Number of Non-Zero Features, and Classifications Accuracy for Each Participants’ Best Model of Lonely Using BISCWIT ID Group # Items # Features Accuracy 08 Full, No Time 3 5 0.92 10 Psychological, No Time 3 4 0.91 103 Full, No Time 33 31 0.94 105 Psychological, No Time 3 4 0.80 106 Full, No Time 3 4 0.94 108 Psychological, No Time 3 4 0.80 112 Full, No Time 6 7 0.69 118 Psychological, Time 15 16 0.70 119 Full, No Time 15 16 0.93 123 Full, No Time 9 10 0.93 129 Situations, No Time 9 12 0.67 133 Full, No Time 36 34 0.90 135 Full, No Time 3 5 0.93 146 Psychological, No Time 3 4 0.95 149 Full, No Time 33 24 0.95 150 Full, No Time 3 4 0.96 152 Full, No Time 3 4 0.92 154 Full, No Time 36 35 0.94 155 Psychological, No Time 6 7 0.78 157 Full, No Time 3 4 0.90 158 Full, No Time 18 19 0.93 160 Full, No Time 3 4 0.83 164 Full, No Time 3 4 0.96 167 Full, No Time 3 4 0.91 168 Psychological, Time 9 10 0.77 170 Psychological, No Time 3 4 0.95 18 Full, No Time 12 13 0.95 185 Full, No Time 3 4 0.70 186 Full, No Time 24 21 0.93 196 Full, No Time 18 19 0.92 20 Full, No Time 3 4 0.93 201 Situations, No Time 3 4 0.82 206 Full, No Time 3 4 0.89 207 Psychological, No Time 3 4 0.86 211 Full, No Time 3 4 0.81 212 Full, No Time 6 7 0.95 214 Full, No Time 6 7 0.75 220 Full, No Time 6 7 0.92 25 Psychological, No Time 15 14 0.75 27 Psychological, Time 33 33 0.82 32 Full, No Time 3 4 0.93 35 Psychological, No Time 9 10 0.75 36 Full, No Time 3 4 0.92 43 Situations, No Time 12 13 0.91 51 Psychological, Time 24 25 0.75 53 Full, No Time 3 4 0.93 63 Full, No Time 15 16 0.91 71 Full, No Time 3 4 0.91 73 Full, No Time 6 7 0.93 84 Full, No Time 3 4 0.92 param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config)) %&gt;% pivot_wider(names_from = &quot;.metric&quot;, values_from = &quot;.estimate&quot;) %&gt;% select(-coefs, -set) %&gt;% unnest(params) %&gt;% select(-.config, -merror, -time) %&gt;% pivot_longer(cols = c(-(model:group)) , names_to = &quot;param&quot; , values_to = &quot;value&quot; , values_drop_na = T) %&gt;% group_by(model, outcome, group, param) %&gt;% summarize_at(vars(value), lst(mean, sd, min, max)) %&gt;% ungroup() %&gt;% mutate(mean = sprintf(&quot;%.2f (%.2f)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max)) %&gt;% select(-sd, -min, -max) %&gt;% pivot_wider(names_from = c(&quot;outcome&quot;, &quot;group&quot;) , values_from = c(&quot;mean&quot;, &quot;range&quot;) , names_glue = &quot;{outcome}_{group}_{.value}&quot;) %&gt;% mutate(model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)) , param = factor(param, c(&quot;accuracy&quot;, &quot;roc_auc&quot;, &quot;penalty&quot;, &quot;mixture&quot;, &quot;nitem&quot;, &quot;min_n&quot;, &quot;mtry&quot;, &quot;nvars&quot;), c(&quot;Accuracy&quot;, &quot;AUC&quot;, &quot;Penalty&quot;, &quot;Mixture&quot;, &quot;# Items&quot;, &quot;Min N Split&quot;, &quot;# Predictors Samples&quot;, &quot;# Features Selected&quot;))) %&gt;% arrange(model, param) ## # A tibble: 14 x 14 ## model param lonely_full_mean lonely_psychologic… lonely_situations… prcrst_full_mean prcrst_psychologic… ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Elastic… Accuracy 0.89 (0.08) 0.84 (0.11) 0.79 (0.13) 0.84 (0.12) 0.78 (0.15) ## 2 Elastic… AUC 0.73 (0.25) 0.75 (0.15) 0.73 (0.18) 0.74 (0.17) 0.62 (0.21) ## 3 Elastic… Penalty 0.10 (0.29) 0.19 (0.39) 0.24 (0.42) 0.18 (0.38) 0.32 (0.46) ## 4 Elastic… Mixture 0.05 (0.18) 0.08 (0.25) 0.05 (0.13) 0.08 (0.19) 0.08 (0.22) ## 5 Elastic… # Feature… 47.82 (8.01) 30.19 (8.64) 28.18 (8.52) 46.74 (9.10) 29.28 (8.10) ## 6 BISCWIT Accuracy 0.90 (0.07) 0.81 (0.08) 0.80 (0.12) 0.83 (0.11) 0.79 (0.14) ## 7 BISCWIT AUC 0.77 (0.18) 0.70 (0.21) 0.69 (0.22) 0.79 (0.12) 0.69 (0.20) ## 8 BISCWIT # Items 10.69 (11.03) 8.88 (7.52) 6.50 (4.90) 12.08 (11.58) 10.47 (8.34) ## 9 BISCWIT # Feature… 11.18 (9.98) 9.44 (7.08) 7.42 (4.21) 12.28 (9.85) 10.35 (6.72) ## 10 Random … Accuracy 0.84 (0.09) 0.83 (0.14) 0.91 (0.06) 0.83 (0.11) 0.82 (0.15) ## 11 Random … AUC 0.82 (0.11) 0.79 (0.16) 0.70 (0.19) 0.73 (0.17) 0.64 (0.20) ## 12 Random … Min N Spl… 16.91 (13.34) 19.18 (14.82) 13.66 (10.65) 16.84 (12.30) 17.21 (12.35) ## 13 Random … # Predict… 25.73 (15.98) 12.18 (7.60) 14.71 (7.45) 28.02 (15.91) 16.52 (9.50) ## 14 Random … # Feature… 58.82 (28.28) 38.88 (22.69) 28.09 (13.39) 53.12 (34.54) 35.28 (24.01) ## # … with 7 more variables: prcrst_situations_mean &lt;chr&gt;, lonely_full_range &lt;chr&gt;, ## # lonely_psychological_range &lt;chr&gt;, lonely_situations_range &lt;chr&gt;, prcrst_full_range &lt;chr&gt;, ## # prcrst_psychological_range &lt;chr&gt;, prcrst_situations_range &lt;chr&gt; 6.2 Question 2: Are there individual differences in the idiographic range of prediction across people? Next, rather than grouping performance information by the feature sets, we’ll group the feature sets by participant, demonstrating the mean, standard deviation, median, and range for each person to answer the range of prediction across people. In the manuscript, we include a subset of this as a figure of a sample of 25 participants for each outcome. But below, we’ll create tables for each outcome, where each participant is a row to describe their results. 6.2.1 The Range of Prediction 6.2.1.1 Table px_sum_tab &lt;- function(d, outcome){ # clean up the outcome names o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) # create the caption cap &lt;- sprintf(&quot;&lt;strong&gt;Table SX&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Descriptive Statistics of Model Performance for Each Participant for %s&quot;, outcome) # create the span headers for the table h1 &lt;- c(1, rep(2, 6)); names(h1) &lt;- c(&quot; &quot;, rep(c(&quot;Accuracy&quot;, &quot;AUC&quot;), times = 3)) h2 &lt;- c(1, rep(4, 3)); names(h2) &lt;- c(&quot; &quot;, &quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;) # call the kable table tab &lt;- d %&gt;% kable(. , &quot;html&quot; , col.names = c(&quot;ID&quot;, rep(c(&quot;M (SD)&quot;, &quot;Range&quot;), times = 6)) , align = c(&quot;r&quot;, rep(&quot;c&quot;, 12)) , caption = cap ) %&gt;% kable_styling(full_width = F) %&gt;% add_header_above(h1) %&gt;% add_header_above(h2) save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/04-participant-sum/%s.html&quot;, res_path, outcome)) return(tab) } # indexing the preferred column order ord &lt;- paste(rep(c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), each = 6) , rep(c(&quot;accuracy&quot;, &quot;roc_auc&quot;), each = 3, times = 3) , rep(c(&quot;mean&quot;, &quot;median&quot;, &quot;range&quot;), times = 6) , sep = &quot;_&quot;) px_tabs &lt;- sum_res %&gt;% unnest(data) %&gt;% group_by(SID, outcome, model, .metric) %&gt;% # summaries for each participant, outcome, model, and metric combinations summarize_at(vars(.estimate), lst(mean, median, sd, min, max), na.rm = T) %&gt;% ungroup() %&gt;% mutate(sd = ifelse(sd &lt; .01, &quot;&lt;.01&quot;, sprintf(&quot;%.2f&quot;, sd)), mean = sprintf(&quot;%.2f (%s)&quot;, mean, sd), range = sprintf(&quot;%.2f-%.2f&quot;, min, max), median = sprintf(&quot;%.2f&quot;, median)) %&gt;% select(-sd, -min, -max) %&gt;% pivot_wider(names_from = c(&quot;model&quot;, &quot;.metric&quot;) , values_from = c(&quot;mean&quot;, &quot;median&quot;, &quot;range&quot;) , names_glue = &quot;{model}_{.metric}_{.value}&quot;) %&gt;% select(SID, outcome, ord) %&gt;% select(-contains(&quot;median&quot;)) %&gt;% group_by(outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = map2(data, outcome, px_sum_tab)) 6.2.1.1.1 Procrastination px_tabs$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;)## procrastination Table 6.4: Table SXDescriptive Statistics of Model Performance for Each Participant for prcrst Elastic Net BISCWIT Random Forest Accuracy AUC Accuracy AUC Accuracy AUC ID M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range 01 0.90 (0.04) 0.82-0.92 0.51 (0.20) 0.20-0.80 0.91 (&lt;.01) 0.91-0.92 0.66 (0.20) 0.45-1.00 0.89 (0.03) 0.83-0.91 0.43 (0.21) 0.18-0.80 02 0.78 (0.10) 0.54-0.85 0.40 (0.17) 0.18-0.82 0.75 (0.07) 0.62-0.85 0.64 (0.11) 0.45-0.86 0.83 (&lt;.01) 0.83-0.85 0.43 (0.12) 0.18-0.50 03 0.86 (0.09) 0.73-0.91 0.57 (0.28) 0.22-0.80 0.69 (0.11) 0.60-0.82 0.42 (0.35) 0.10-0.78 0.83 (0.15) 0.60-0.91 0.46 (0.16) 0.22-0.60 05 0.91 (0.02) 0.85-0.92 0.45 (0.30) 0.09-0.91 0.90 (0.05) 0.83-1.00 0.57 (0.28) 0.00-0.95 0.92 (0.02) 0.91-1.00 0.43 (0.25) 0.00-1.00 09 0.90 (0.03) 0.83-0.92 0.39 (0.32) 0.10-1.00 0.90 (0.03) 0.83-0.92 0.48 (0.20) 0.20-0.82 0.90 (&lt;.01) 0.89-0.91 0.26 (0.20) 0.06-0.67 10 0.78 (0.04) 0.75-0.82 0.39 (0.22) 0.06-0.69 0.75 (0.04) 0.70-0.80 0.54 (0.14) 0.37-0.81 103 0.62 (0.02) 0.56-0.69 0.54 (0.14) 0.33-0.78 0.61 (0.05) 0.50-0.69 0.51 (0.05) 0.42-0.60 0.62 (0.02) 0.57-0.64 0.62 (0.08) 0.49-0.72 105 0.85 (0.06) 0.73-0.92 0.59 (0.21) 0.17-0.92 0.82 (0.06) 0.73-0.92 0.31 (0.15) 0.12-0.69 0.86 (0.05) 0.75-0.92 0.58 (0.20) 0.33-1.00 106 0.80 (0.12) 0.50-0.88 0.61 (0.12) 0.43-0.79 0.74 (0.08) 0.56-0.81 0.41 (0.14) 0.21-0.79 0.79 (0.12) 0.56-0.94 0.62 (0.10) 0.42-0.79 107 0.88 (0.09) 0.67-0.93 0.60 (0.34) 0.00-1.00 0.92 (0.03) 0.87-0.93 0.34 (0.30) 0.00-0.93 0.93 (&lt;.01) 0.92-0.93 0.56 (0.27) 0.00-1.00 108 0.62 (0.04) 0.60-0.70 0.55 (0.12) 0.39-0.85 0.56 (0.09) 0.40-0.75 0.44 (0.16) 0.09-0.70 0.57 (0.01) 0.56-0.60 0.52 (0.14) 0.32-0.83 11 0.74 (0.12) 0.64-0.91 0.53 (0.45) 0.00-1.00 0.71 (0.09) 0.60-0.82 0.34 (0.31) 0.10-0.78 0.74 (0.31) 0.30-1.00 0.42 (0.12) 0.33-0.50 110 0.85 (0.10) 0.58-0.92 0.52 (0.28) 0.18-1.00 0.83 (0.07) 0.75-0.92 0.39 (0.28) 0.00-1.00 0.85 (0.10) 0.58-0.92 0.60 (0.22) 0.36-1.00 111 0.76 (0.16) 0.50-0.90 0.46 (0.31) 0.00-1.00 0.77 (0.10) 0.60-0.90 0.50 (0.32) 0.00-1.00 0.89 (0.03) 0.80-0.90 0.43 (0.20) 0.00-0.88 112 0.64 (0.12) 0.38-0.77 0.48 (0.21) 0.22-0.81 0.67 (0.07) 0.54-0.77 0.50 (0.27) 0.14-0.87 0.58 (0.20) 0.23-0.83 0.60 (0.14) 0.38-0.85 113 0.86 (0.05) 0.80-0.90 0.37 (0.18) 0.11-0.61 118 0.64 (0.08) 0.55-0.80 0.67 (0.16) 0.36-0.83 0.69 (0.06) 0.64-0.80 0.37 (0.14) 0.25-0.68 0.56 (0.16) 0.25-0.82 0.58 (0.21) 0.33-0.89 119 0.57 (0.04) 0.43-0.64 0.53 (0.12) 0.31-0.73 0.54 (0.07) 0.36-0.64 0.48 (0.12) 0.29-0.68 0.57 (&lt;.01) 0.57-0.57 0.46 (0.08) 0.27-0.54 121 0.92 (&lt;.01) 0.92-0.92 0.57 (0.44) 0.00-0.91 0.92 (&lt;.01) 0.92-0.92 0.56 (0.38) 0.09-1.00 0.90 (&lt;.01) 0.90-0.91 0.57 (0.33) 0.11-0.89 123 0.74 (0.11) 0.47-0.87 0.49 (0.15) 0.25-0.82 0.68 (0.11) 0.53-0.93 0.45 (0.16) 0.18-0.72 0.78 (0.03) 0.71-0.80 0.46 (0.24) 0.00-0.72 124 0.49 (0.11) 0.30-0.64 0.40 (0.11) 0.21-0.59 0.42 (0.08) 0.33-0.64 0.61 (0.09) 0.49-0.79 0.48 (0.11) 0.27-0.60 0.44 (0.13) 0.21-0.71 126 0.48 (0.10) 0.25-0.58 0.39 (0.18) 0.09-0.71 0.48 (0.10) 0.33-0.58 0.60 (0.14) 0.29-0.77 0.64 (0.08) 0.42-0.73 0.61 (0.12) 0.46-0.79 129 0.36 (0.09) 0.17-0.50 0.38 (0.14) 0.06-0.56 0.46 (0.18) 0.25-0.75 0.58 (0.22) 0.20-0.88 0.65 (0.15) 0.42-0.92 0.63 (0.23) 0.28-0.94 133 0.90 (&lt;.01) 0.90-0.90 0.56 (0.33) 0.11-1.00 0.91 (0.04) 0.90-1.00 0.45 (0.36) 0.00-0.89 0.90 (&lt;.01) 0.90-0.90 0.58 (0.40) 0.00-1.00 135 0.87 (0.13) 0.43-0.93 0.48 (0.32) 0.00-1.00 0.88 (0.05) 0.79-0.93 0.48 (0.30) 0.08-1.00 0.93 (0.02) 0.92-1.00 0.56 (0.27) 0.08-1.00 146 0.62 (0.05) 0.53-0.68 0.46 (0.13) 0.24-0.68 0.61 (0.05) 0.53-0.67 0.56 (0.13) 0.32-0.76 0.61 (0.06) 0.53-0.68 0.39 (0.16) 0.16-0.63 148 0.56 (0.08) 0.33-0.67 0.45 (0.12) 0.24-0.74 0.54 (0.10) 0.33-0.67 0.55 (0.13) 0.29-0.77 0.53 (0.09) 0.36-0.64 0.47 (0.14) 0.28-0.71 149 0.53 (0.08) 0.44-0.63 0.36 (0.14) 0.09-0.67 0.46 (0.12) 0.28-0.68 0.65 (0.13) 0.36-0.82 0.59 (0.05) 0.53-0.68 0.46 (0.16) 0.25-0.75 15 0.55 (0.07) 0.45-0.64 0.51 (0.12) 0.37-0.67 0.52 (0.14) 0.36-0.64 0.55 (0.16) 0.40-0.77 0.50 (&lt;.01) 0.50-0.50 0.49 (0.09) 0.36-0.56 150 0.70 (0.23) 0.20-0.88 0.56 (0.14) 0.32-0.73 0.67 (0.05) 0.60-0.76 0.48 (0.10) 0.30-0.65 0.78 (0.08) 0.62-0.83 0.74 (0.16) 0.35-0.89 152 0.87 (0.01) 0.83-0.88 0.42 (0.21) 0.16-0.77 0.88 (&lt;.01) 0.88-0.88 0.60 (0.16) 0.30-0.83 0.88 (&lt;.01) 0.87-0.88 0.65 (0.14) 0.37-0.86 155 0.57 (0.04) 0.56-0.67 0.55 (0.07) 0.50-0.71 0.60 (0.05) 0.50-0.67 0.45 (0.11) 0.27-0.58 0.55 (0.06) 0.47-0.67 0.56 (0.14) 0.32-0.79 156 0.48 (0.01) 0.48-0.52 0.48 (0.08) 0.36-0.61 0.48 (0.03) 0.43-0.52 0.53 (0.11) 0.37-0.71 0.49 (0.05) 0.45-0.57 0.58 (0.15) 0.32-0.79 157 0.90 (0.02) 0.85-0.90 0.34 (0.12) 0.11-0.58 0.90 (&lt;.01) 0.90-0.90 0.64 (0.09) 0.45-0.79 0.93 (0.02) 0.90-0.95 0.68 (0.22) 0.28-1.00 158 0.51 (0.16) 0.27-0.73 0.53 (0.14) 0.30-0.73 0.52 (0.10) 0.33-0.67 0.57 (0.14) 0.31-0.75 0.64 (0.15) 0.40-0.93 0.65 (0.25) 0.14-1.00 159 0.86 (0.08) 0.65-0.90 0.43 (0.11) 0.19-0.54 0.89 (0.02) 0.84-0.90 0.54 (0.14) 0.29-0.79 0.89 (&lt;.01) 0.88-0.90 0.43 (0.18) 0.21-0.72 160 0.89 (&lt;.01) 0.89-0.89 0.65 (0.23) 0.22-0.94 0.87 (0.03) 0.83-0.89 0.43 (0.29) 0.06-0.88 0.89 (&lt;.01) 0.88-0.89 0.71 (0.18) 0.28-0.94 162 0.90 (0.05) 0.77-0.93 0.50 (0.33) 0.08-1.00 0.90 (0.05) 0.77-0.93 0.58 (0.38) 0.04-1.00 0.95 (0.04) 0.90-1.00 0.63 (0.34) 0.11-1.00 164 0.96 (&lt;.01) 0.96-0.96 0.44 (0.34) 0.00-0.98 0.95 (&lt;.01) 0.95-0.96 0.67 (0.32) 0.09-1.00 165 0.61 (0.08) 0.36-0.64 0.54 (0.14) 0.29-0.71 0.61 (0.08) 0.45-0.73 0.46 (0.18) 0.25-0.71 0.61 (0.05) 0.56-0.70 0.65 (0.20) 0.50-1.00 166 0.67 (0.05) 0.58-0.75 0.71 (0.10) 0.47-0.81 0.60 (0.07) 0.42-0.67 0.38 (0.12) 0.20-0.59 0.70 (0.11) 0.55-0.83 0.64 (0.13) 0.43-0.80 167 0.58 (0.08) 0.45-0.73 0.64 (0.12) 0.30-0.77 0.56 (0.08) 0.36-0.64 0.40 (0.11) 0.23-0.60 0.61 (0.10) 0.44-0.78 0.51 (0.12) 0.22-0.67 168 0.57 (0.04) 0.54-0.62 0.51 (0.05) 0.43-0.66 0.51 (0.11) 0.31-0.62 0.54 (0.10) 0.40-0.70 0.69 (0.13) 0.50-0.92 0.74 (0.15) 0.50-0.90 169 0.94 (0.02) 0.89-0.95 0.51 (0.35) 0.06-0.94 0.94 (0.02) 0.89-0.95 0.56 (0.36) 0.00-1.00 0.95 (0.02) 0.93-1.00 0.67 (0.26) 0.14-0.93 17 0.40 (0.14) 0.23-0.64 0.58 (0.21) 0.14-0.88 0.47 (0.10) 0.31-0.62 0.50 (0.17) 0.20-0.75 0.46 (0.20) 0.15-0.77 0.57 (0.13) 0.33-0.78 170 0.81 (0.06) 0.75-0.94 0.78 (0.14) 0.50-0.97 0.79 (0.03) 0.75-0.85 0.25 (0.11) 0.13-0.50 0.68 (0.22) 0.17-0.83 0.67 (0.15) 0.39-0.86 171 0.96 (0.01) 0.92-0.96 0.51 (0.33) 0.00-1.00 0.96 (0.01) 0.92-0.96 0.58 (0.30) 0.12-0.91 0.96 (0.02) 0.95-1.00 0.55 (0.28) 0.19-1.00 174 0.96 (&lt;.01) 0.96-0.96 0.53 (0.30) 0.04-0.98 0.96 (0.01) 0.92-0.96 0.46 (0.30) 0.04-0.88 0.96 (0.02) 0.94-1.00 0.34 (0.38) 0.00-0.96 177 0.92 (&lt;.01) 0.92-0.92 0.52 (0.04) 0.50-0.58 0.83 (0.10) 0.69-0.92 0.31 (0.20) 0.17-0.58 0.94 (0.08) 0.83-1.00 0.52 (0.03) 0.50-0.55 185 0.94 (0.03) 0.84-0.95 0.52 (0.30) 0.00-1.00 0.92 (0.03) 0.89-0.95 0.50 (0.32) 0.00-0.89 0.95 (&lt;.01) 0.94-0.95 0.36 (0.27) 0.00-0.89 186 0.72 (0.04) 0.67-0.80 0.83 (0.09) 0.66-0.95 0.75 (0.04) 0.67-0.80 0.15 (0.11) 0.00-0.43 0.70 (0.02) 0.64-0.73 0.75 (0.17) 0.56-1.00 188 0.89 (0.05) 0.75-0.92 0.25 (0.19) 0.00-0.55 0.86 (0.05) 0.75-0.92 0.73 (0.17) 0.41-1.00 0.91 (0.02) 0.83-0.92 0.48 (0.11) 0.36-0.82 189 0.61 (0.21) 0.00-0.83 0.24 (0.14) 0.00-0.55 0.72 (0.09) 0.58-0.83 0.72 (0.21) 0.45-1.00 0.78 (0.05) 0.70-0.83 0.57 (0.16) 0.33-0.89 190 0.93 (&lt;.01) 0.93-0.94 0.44 (0.24) 0.06-0.93 0.89 (0.07) 0.71-0.94 0.62 (0.29) 0.07-1.00 0.91 (0.04) 0.82-0.94 0.47 (0.23) 0.00-0.86 192 0.88 (0.08) 0.69-0.92 0.49 (0.30) 0.08-1.00 0.92 (0.02) 0.85-0.92 0.46 (0.35) 0.00-1.00 0.92 (&lt;.01) 0.92-0.92 0.43 (0.23) 0.00-0.67 195 0.94 (&lt;.01) 0.94-0.94 0.58 (0.32) 0.19-1.00 0.94 (&lt;.01) 0.94-0.94 0.46 (0.31) 0.00-0.75 0.94 (0.03) 0.93-1.00 0.42 (0.40) 0.03-1.00 196 0.82 (0.04) 0.67-0.83 0.41 (0.17) 0.05-0.70 0.69 (0.12) 0.58-1.00 0.51 (0.22) 0.00-0.70 0.83 (0.03) 0.82-0.92 0.55 (0.11) 0.50-0.90 20 0.60 (0.18) 0.36-0.79 0.49 (0.17) 0.12-0.76 0.71 (0.06) 0.64-0.79 0.46 (0.24) 0.23-0.94 0.76 (0.04) 0.71-0.86 0.69 (0.18) 0.42-0.97 206 0.89 (&lt;.01) 0.89-0.89 0.30 (0.14) 0.12-0.56 0.89 (&lt;.01) 0.89-0.89 0.66 (0.16) 0.36-0.81 0.93 (0.06) 0.88-1.00 0.55 (0.22) 0.17-0.80 207 0.85 (&lt;.01) 0.85-0.86 0.38 (0.31) 0.00-0.96 0.85 (&lt;.01) 0.85-0.86 0.56 (0.35) 0.02-0.86 0.85 (&lt;.01) 0.85-0.85 0.80 (0.16) 0.50-1.00 21 0.52 (0.12) 0.20-0.67 0.49 (0.16) 0.20-0.72 0.57 (0.06) 0.47-0.67 0.42 (0.13) 0.22-0.70 0.57 (0.06) 0.47-0.71 0.51 (0.17) 0.33-0.83 211 0.96 (&lt;.01) 0.93-0.96 0.34 (0.26) 0.08-0.96 0.96 (&lt;.01) 0.96-0.96 0.34 (0.32) 0.04-0.88 0.98 (0.02) 0.96-1.00 0.66 (0.24) 0.36-1.00 212 0.93 (0.04) 0.86-0.95 0.53 (0.14) 0.28-0.80 0.92 (0.03) 0.86-0.95 0.35 (0.22) 0.00-0.74 0.95 (&lt;.01) 0.95-0.95 0.44 (0.34) 0.00-0.95 214 0.92 (0.04) 0.81-0.94 0.61 (0.26) 0.13-1.00 0.93 (0.02) 0.88-0.94 0.40 (0.29) 0.00-0.87 0.93 (0.02) 0.91-1.00 0.44 (0.33) 0.00-0.93 216 0.78 (&lt;.01) 0.78-0.78 0.44 (0.14) 0.25-0.68 0.78 (&lt;.01) 0.78-0.78 0.58 (0.13) 0.38-0.83 0.80 (0.03) 0.78-0.83 0.42 (0.12) 0.27-0.69 22 0.65 (0.07) 0.58-0.71 0.47 (0.08) 0.20-0.50 0.66 (0.11) 0.50-0.79 0.40 (0.14) 0.20-0.60 0.70 (0.07) 0.64-0.82 0.51 (0.04) 0.50-0.64 220 0.80 (0.13) 0.58-0.92 0.58 (0.27) 0.18-1.00 0.67 (0.13) 0.50-0.92 0.39 (0.31) 0.00-0.90 0.90 (0.02) 0.82-0.92 0.44 (0.21) 0.00-0.90 25 0.65 (0.12) 0.50-0.75 0.60 (0.13) 0.43-0.78 0.54 (0.11) 0.42-0.67 0.46 (0.15) 0.22-0.61 0.71 (0.06) 0.60-0.75 0.58 (0.14) 0.50-0.85 26 0.80 (0.01) 0.79-0.81 0.55 (0.09) 0.33-0.67 0.77 (0.03) 0.71-0.81 0.60 (0.09) 0.44-0.73 0.81 (0.02) 0.79-0.85 0.62 (0.13) 0.46-0.81 27 0.66 (0.13) 0.42-0.83 0.67 (0.17) 0.34-0.89 0.54 (0.21) 0.17-0.75 0.35 (0.28) 0.00-0.77 0.59 (0.18) 0.30-0.89 0.74 (0.16) 0.39-0.95 30 0.85 (0.10) 0.54-0.92 0.47 (0.28) 0.12-0.92 0.86 (0.05) 0.79-0.92 0.38 (0.28) 0.00-0.79 0.89 (0.04) 0.85-0.92 0.84 (0.27) 0.18-1.00 31 0.90 (0.05) 0.79-0.93 0.49 (0.36) 0.00-1.00 0.93 (&lt;.01) 0.93-0.93 0.53 (0.31) 0.00-1.00 0.92 (&lt;.01) 0.92-0.93 0.35 (0.21) 0.00-0.67 32 0.83 (0.07) 0.64-0.86 0.60 (0.16) 0.33-0.79 0.80 (0.05) 0.71-0.86 0.36 (0.16) 0.17-0.71 0.84 (0.03) 0.77-0.86 0.51 (0.06) 0.41-0.68 33 0.91 (&lt;.01) 0.91-0.91 0.69 (0.30) 0.10-1.00 0.90 (0.02) 0.82-0.91 0.32 (0.24) 0.00-0.85 0.92 (0.04) 0.89-1.00 0.72 (0.32) 0.00-1.00 35 0.73 (0.04) 0.67-0.75 0.54 (0.09) 0.44-0.67 0.68 (0.06) 0.58-0.75 0.42 (0.10) 0.28-0.54 0.80 (0.04) 0.73-0.82 0.56 (0.25) 0.11-0.89 38 0.58 (0.11) 0.36-0.73 0.38 (0.13) 0.15-0.62 0.53 (0.08) 0.45-0.67 0.59 (0.18) 0.15-0.80 0.52 (0.11) 0.30-0.64 0.43 (0.16) 0.08-0.65 40 0.90 (0.02) 0.82-0.91 0.51 (0.31) 0.00-0.90 0.90 (0.02) 0.82-0.91 0.55 (0.34) 0.05-1.00 0.91 (&lt;.01) 0.91-0.91 0.39 (0.31) 0.00-0.90 43 0.86 (0.10) 0.64-0.91 0.74 (0.21) 0.40-1.00 0.83 (0.08) 0.73-0.91 0.32 (0.19) 0.10-0.70 0.87 (0.08) 0.70-0.90 0.42 (0.33) 0.00-0.89 44 0.48 (0.11) 0.36-0.64 0.42 (0.15) 0.20-0.53 0.55 (0.07) 0.45-0.64 0.46 (0.15) 0.33-0.60 0.43 (0.03) 0.40-0.45 0.53 (0.07) 0.50-0.63 49 0.60 (0.07) 0.50-0.67 0.76 (0.10) 0.63-0.87 0.61 (0.05) 0.55-0.67 0.23 (0.11) 0.10-0.39 0.57 (0.08) 0.50-0.67 0.59 (0.12) 0.50-0.78 51 0.92 (0.02) 0.91-1.00 0.48 (0.25) 0.09-1.00 0.75 (0.12) 0.55-0.92 0.70 (0.29) 0.10-1.00 0.89 (0.09) 0.58-1.00 0.55 (0.16) 0.36-1.00 52 0.91 (&lt;.01) 0.91-0.91 0.56 (0.33) 0.00-1.00 0.91 (&lt;.01) 0.91-0.91 0.52 (0.37) 0.00-1.00 0.92 (0.04) 0.89-1.00 0.53 (0.30) 0.10-1.00 53 0.88 (0.11) 0.58-0.93 0.55 (0.29) 0.09-1.00 0.92 (0.04) 0.83-1.00 0.47 (0.35) 0.00-1.00 0.93 (0.02) 0.91-1.00 0.76 (0.27) 0.20-1.00 56 0.90 (&lt;.01) 0.90-0.91 0.33 (0.46) 0.00-1.00 0.90 (&lt;.01) 0.90-0.91 0.68 (0.46) 0.00-1.00 0.90 (&lt;.01) 0.90-0.91 0.40 (0.45) 0.00-1.00 61 0.91 (&lt;.01) 0.91-0.92 0.66 (0.37) 0.10-1.00 0.91 (&lt;.01) 0.91-0.92 0.55 (0.41) 0.00-1.00 0.91 (&lt;.01) 0.91-0.92 0.60 (0.31) 0.09-0.90 63 0.91 (&lt;.01) 0.91-0.91 0.46 (0.34) 0.00-1.00 0.90 (0.03) 0.82-0.91 0.58 (0.25) 0.00-0.90 0.90 (&lt;.01) 0.90-0.91 0.54 (0.24) 0.11-1.00 66 0.91 (&lt;.01) 0.91-0.91 0.49 (0.37) 0.00-1.00 0.89 (0.02) 0.88-0.91 0.42 (0.40) 0.00-1.00 67 0.62 (0.08) 0.36-0.64 0.23 (0.22) 0.00-0.60 0.64 (0.02) 0.57-0.64 0.73 (0.26) 0.42-1.00 0.65 (0.04) 0.62-0.69 0.39 (0.10) 0.22-0.50 71 0.73 (0.04) 0.64-0.82 0.76 (0.21) 0.42-1.00 0.78 (0.16) 0.55-1.00 0.41 (0.40) 0.00-1.00 0.70 (&lt;.01) 0.70-0.73 0.50 (0.21) 0.14-0.81 73 0.93 (&lt;.01) 0.92-0.93 0.22 (0.20) 0.00-0.62 0.92 (0.03) 0.85-0.93 0.70 (0.27) 0.25-1.00 0.92 (0.02) 0.85-0.92 0.56 (0.13) 0.50-0.92 74 0.93 (&lt;.01) 0.93-0.93 0.70 (0.33) 0.00-1.00 0.92 (&lt;.01) 0.92-0.92 0.45 (0.33) 0.04-0.92 77 0.86 (0.11) 0.60-0.90 0.46 (0.40) 0.00-1.00 0.90 (0.06) 0.80-1.00 0.52 (0.37) 0.00-0.89 0.88 (0.04) 0.80-0.90 0.51 (0.24) 0.11-0.71 78 0.91 (&lt;.01) 0.91-0.91 0.74 (0.36) 0.10-1.00 0.90 (&lt;.01) 0.90-0.90 0.85 (0.26) 0.44-1.00 81 0.39 (&lt;.01) 0.38-0.40 0.57 (0.19) 0.37-0.78 0.39 (&lt;.01) 0.38-0.40 0.45 (0.19) 0.26-0.62 0.42 (0.03) 0.38-0.46 0.45 (0.27) 0.20-0.80 84 0.92 (&lt;.01) 0.92-0.92 0.39 (0.24) 0.00-0.96 0.92 (&lt;.01) 0.92-0.92 0.63 (0.19) 0.12-0.88 0.92 (&lt;.01) 0.91-0.92 0.46 (0.33) 0.00-0.88 85 0.85 (&lt;.01) 0.85-0.85 0.47 (0.13) 0.27-0.66 86 0.81 (0.10) 0.69-0.92 0.32 (0.35) 0.00-0.92 0.92 (0.02) 0.85-0.93 0.44 (0.38) 0.00-1.00 0.93 (&lt;.01) 0.92-0.93 0.50 (0.29) 0.08-1.00 88 0.45 (0.24) 0.20-0.70 0.42 (0.29) 0.11-0.67 0.70 (0.12) 0.60-0.80 0.58 (0.29) 0.33-0.89 0.48 (0.30) 0.10-0.80 0.18 (0.14) 0.00-0.33 6.2.1.1.2 Loneliness px_tabs$tab[[2]] %&gt;% scroll_box(height = &quot;750px&quot;)## loneliness Table 6.4: Table SXDescriptive Statistics of Model Performance for Each Participant for lonely Elastic Net BISCWIT Random Forest Accuracy AUC Accuracy AUC Accuracy AUC ID M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range M (SD) Range 03 0.90 (&lt;.01) 0.90-0.91 0.21 (0.10) 0.10-0.33 0.90 (&lt;.01) 0.90-0.91 0.69 (0.15) 0.56-0.90 0.90 (&lt;.01) 0.90-0.91 0.46 (0.05) 0.40-0.50 08 0.49 (0.22) 0.17-0.83 0.62 (0.24) 0.15-1.00 0.79 (0.11) 0.58-0.92 0.25 (0.14) 0.05-0.45 0.48 (0.31) 0.17-0.82 0.52 (0.14) 0.22-0.78 10 0.88 (0.10) 0.64-0.92 0.61 (0.26) 0.09-0.91 0.91 (&lt;.01) 0.91-0.92 0.43 (0.24) 0.18-0.82 0.90 (&lt;.01) 0.89-0.92 0.60 (0.32) 0.11-0.89 103 0.92 (0.03) 0.88-0.94 0.63 (0.27) 0.13-0.93 0.92 (0.04) 0.81-0.94 0.42 (0.24) 0.07-0.87 0.93 (&lt;.01) 0.93-0.94 0.59 (0.22) 0.27-0.92 105 0.74 (0.09) 0.47-0.82 0.59 (0.14) 0.33-0.83 0.79 (0.07) 0.62-0.88 0.42 (0.13) 0.23-0.75 0.83 (0.06) 0.73-0.92 0.73 (0.14) 0.50-0.95 106 0.93 (0.02) 0.88-0.94 0.54 (0.25) 0.20-1.00 0.92 (0.03) 0.88-0.94 0.51 (0.25) 0.07-0.97 0.93 (&lt;.01) 0.93-0.94 0.69 (0.29) 0.00-0.93 108 0.77 (0.10) 0.45-0.80 0.32 (0.14) 0.10-0.50 0.74 (0.07) 0.55-0.80 0.62 (0.11) 0.45-0.86 0.80 (0.01) 0.79-0.81 0.32 (0.10) 0.23-0.55 11 0.88 (0.04) 0.82-0.91 0.59 (0.29) 0.30-0.89 0.90 (&lt;.01) 0.90-0.91 0.75 (0.43) 0.11-1.00 0.95 (0.06) 0.89-1.00 0.71 (0.06) 0.67-0.75 112 0.68 (0.04) 0.54-0.69 0.46 (0.19) 0.17-0.72 0.69 (&lt;.01) 0.69-0.69 0.38 (0.24) 0.11-0.88 0.68 (0.01) 0.67-0.69 0.66 (0.12) 0.47-0.83 118 0.59 (0.21) 0.30-0.82 0.37 (0.16) 0.11-0.61 0.66 (0.08) 0.55-0.82 0.47 (0.19) 0.22-0.75 0.66 (0.11) 0.50-0.82 0.38 (0.24) 0.00-0.73 119 0.93 (&lt;.01) 0.93-0.93 0.46 (0.25) 0.08-0.85 0.93 (&lt;.01) 0.93-0.93 0.59 (0.26) 0.15-1.00 0.93 (&lt;.01) 0.93-0.93 0.49 (0.17) 0.23-0.92 121 0.90 (0.04) 0.83-0.92 0.32 (0.40) 0.09-0.91 0.88 (0.05) 0.83-0.92 0.78 (0.18) 0.64-1.00 0.93 (0.05) 0.90-1.00 0.24 (0.23) 0.10-0.50 123 0.91 (0.06) 0.73-1.00 0.76 (0.20) 0.43-1.00 0.93 (&lt;.01) 0.93-0.93 0.24 (0.25) 0.00-0.64 0.93 (&lt;.01) 0.93-0.93 0.71 (0.31) 0.15-1.00 129 0.60 (0.06) 0.50-0.75 0.76 (0.11) 0.49-0.94 0.60 (0.04) 0.58-0.67 0.27 (0.17) 0.06-0.66 0.59 (0.09) 0.50-0.83 0.81 (0.12) 0.60-0.97 133 0.87 (0.08) 0.70-0.90 0.52 (0.42) 0.00-1.00 0.90 (&lt;.01) 0.90-0.90 0.42 (0.33) 0.00-0.94 0.90 (&lt;.01) 0.90-0.90 0.56 (0.38) 0.11-1.00 135 0.88 (0.11) 0.64-0.93 0.55 (0.25) 0.15-0.92 0.93 (&lt;.01) 0.93-0.93 0.60 (0.24) 0.15-0.92 0.96 (0.04) 0.93-1.00 0.53 (0.15) 0.27-0.77 146 0.93 (0.02) 0.86-0.95 0.62 (0.27) 0.07-0.90 0.93 (0.01) 0.91-0.95 0.36 (0.19) 0.05-0.75 0.93 (0.01) 0.90-0.95 0.45 (0.30) 0.00-1.00 149 0.94 (0.01) 0.89-0.95 0.54 (0.19) 0.12-0.89 0.95 (&lt;.01) 0.94-0.95 0.61 (0.25) 0.22-0.94 0.94 (&lt;.01) 0.93-0.95 0.45 (0.28) 0.06-0.86 15 0.59 (0.09) 0.55-0.73 0.38 (0.21) 0.17-0.56 0.50 (0.12) 0.36-0.64 0.60 (0.14) 0.44-0.78 0.60 (0.22) 0.30-0.80 0.42 (0.19) 0.25-0.69 150 0.95 (0.02) 0.88-0.96 0.48 (0.27) 0.08-0.83 0.95 (0.01) 0.92-0.96 0.42 (0.27) 0.04-0.88 0.96 (&lt;.01) 0.95-0.96 0.86 (0.16) 0.48-1.00 152 0.92 (0.01) 0.92-0.96 0.70 (0.15) 0.48-0.91 0.92 (&lt;.01) 0.92-0.92 0.24 (0.15) 0.05-0.59 0.92 (&lt;.01) 0.91-0.92 0.77 (0.15) 0.45-0.96 154 0.87 (0.10) 0.71-1.00 0.63 (0.36) 0.00-1.00 0.93 (0.02) 0.88-0.94 0.46 (0.37) 0.03-1.00 0.94 (&lt;.01) 0.93-0.94 0.74 (0.22) 0.17-1.00 155 0.53 (0.11) 0.28-0.67 0.47 (0.09) 0.32-0.64 0.58 (0.12) 0.28-0.78 0.47 (0.09) 0.30-0.64 0.61 (0.06) 0.47-0.71 0.64 (0.13) 0.43-0.86 157 0.90 (0.01) 0.86-0.90 0.28 (0.18) 0.06-0.74 0.90 (&lt;.01) 0.90-0.90 0.77 (0.21) 0.21-1.00 0.90 (&lt;.01) 0.89-0.90 0.82 (0.19) 0.44-1.00 158 0.87 (0.11) 0.67-1.00 0.56 (0.39) 0.00-1.00 0.90 (0.06) 0.80-1.00 0.45 (0.39) 0.00-1.00 0.93 (&lt;.01) 0.93-0.93 0.72 (0.27) 0.15-1.00 160 0.83 (&lt;.01) 0.83-0.83 0.39 (0.26) 0.02-0.79 0.83 (&lt;.01) 0.83-0.83 0.54 (0.33) 0.08-1.00 0.83 (&lt;.01) 0.82-0.83 0.65 (0.15) 0.36-0.83 164 0.96 (&lt;.01) 0.96-0.96 0.53 (0.33) 0.00-1.00 0.96 (&lt;.01) 0.96-0.96 0.63 (0.30) 0.09-1.00 0.95 (&lt;.01) 0.95-0.96 0.44 (0.27) 0.05-1.00 167 0.91 (0.04) 0.82-1.00 0.67 (0.31) 0.20-1.00 0.90 (0.04) 0.82-1.00 0.39 (0.30) 0.00-0.90 0.93 (0.05) 0.89-1.00 0.56 (0.40) 0.00-1.00 168 0.52 (0.16) 0.31-0.77 0.59 (0.15) 0.35-0.81 0.65 (0.12) 0.46-0.85 0.31 (0.10) 0.17-0.47 0.66 (0.17) 0.38-0.92 0.86 (0.09) 0.75-1.00 170 0.93 (0.01) 0.92-0.95 0.80 (0.28) 0.22-1.00 0.94 (0.04) 0.88-1.00 0.21 (0.28) 0.00-0.80 0.92 (0.02) 0.88-0.94 0.64 (0.19) 0.38-0.94 18 0.95 (&lt;.01) 0.95-0.95 0.57 (0.29) 0.00-1.00 0.95 (&lt;.01) 0.95-0.95 0.52 (0.31) 0.00-0.94 0.94 (0.03) 0.88-1.00 0.46 (0.32) 0.06-1.00 185 0.64 (0.02) 0.58-0.65 0.37 (0.13) 0.17-0.52 0.61 (0.07) 0.47-0.70 0.57 (0.12) 0.41-0.77 0.63 (0.02) 0.61-0.65 0.51 (0.12) 0.29-0.73 186 0.92 (0.03) 0.87-0.93 0.46 (0.34) 0.00-1.00 0.93 (0.02) 0.87-0.93 0.43 (0.34) 0.00-1.00 0.93 (&lt;.01) 0.92-0.93 0.44 (0.37) 0.00-1.00 196 0.90 (0.05) 0.75-0.92 0.67 (0.25) 0.27-1.00 0.86 (0.09) 0.75-1.00 0.24 (0.26) 0.00-0.82 0.88 (0.05) 0.75-0.92 0.65 (0.16) 0.50-0.91 20 0.93 (&lt;.01) 0.93-0.93 0.15 (0.15) 0.00-0.46 0.93 (&lt;.01) 0.93-0.93 0.91 (0.12) 0.58-1.00 0.91 (0.13) 0.62-1.00 0.43 (0.09) 0.31-0.54 201 0.66 (0.15) 0.36-0.82 0.51 (0.18) 0.28-0.83 0.69 (0.15) 0.45-0.82 0.52 (0.23) 0.11-0.81 0.76 (0.17) 0.60-1.00 0.73 (0.28) 0.28-1.00 206 0.89 (&lt;.01) 0.89-0.89 0.54 (0.07) 0.44-0.69 0.88 (0.02) 0.83-0.89 0.44 (0.15) 0.28-0.72 0.85 (0.04) 0.81-0.88 0.43 (0.11) 0.27-0.64 207 0.76 (0.13) 0.46-0.86 0.71 (0.18) 0.38-0.95 0.85 (0.02) 0.79-0.86 0.42 (0.17) 0.18-0.71 0.85 (0.04) 0.77-0.92 0.68 (0.23) 0.27-1.00 211 0.79 (0.03) 0.69-0.81 0.38 (0.05) 0.27-0.46 0.80 (0.02) 0.77-0.81 0.65 (0.09) 0.50-0.82 0.77 (0.13) 0.35-0.83 0.39 (0.07) 0.28-0.57 212 0.95 (&lt;.01) 0.95-0.95 0.40 (0.26) 0.05-0.74 0.95 (&lt;.01) 0.95-0.95 0.58 (0.25) 0.00-0.95 0.96 (0.02) 0.95-1.00 0.54 (0.28) 0.08-0.94 214 0.75 (&lt;.01) 0.75-0.75 0.50 (0.10) 0.33-0.62 0.74 (0.02) 0.69-0.75 0.43 (0.05) 0.31-0.48 0.75 (0.02) 0.73-0.80 0.67 (0.14) 0.48-0.93 220 0.91 (0.03) 0.82-0.92 0.34 (0.29) 0.00-0.90 0.91 (&lt;.01) 0.91-0.92 0.61 (0.28) 0.09-1.00 0.91 (&lt;.01) 0.91-0.92 0.45 (0.21) 0.14-0.70 25 0.73 (0.06) 0.67-0.83 0.53 (0.19) 0.26-0.78 0.71 (0.08) 0.58-0.83 0.47 (0.15) 0.15-0.59 0.69 (0.06) 0.58-0.75 0.64 (0.09) 0.50-0.74 27 0.83 (0.03) 0.75-0.91 0.64 (0.15) 0.50-0.94 0.73 (0.08) 0.64-0.91 0.35 (0.21) 0.06-0.62 0.81 (0.02) 0.78-0.83 0.49 (0.03) 0.40-0.50 32 0.70 (0.29) 0.07-0.93 0.33 (0.31) 0.00-0.92 0.92 (0.03) 0.86-0.93 0.57 (0.30) 0.23-1.00 0.87 (0.07) 0.77-0.93 0.62 (0.27) 0.00-1.00 35 0.80 (0.11) 0.58-0.92 0.43 (0.33) 0.12-1.00 0.74 (0.11) 0.58-0.92 0.57 (0.30) 0.05-0.93 0.79 (0.05) 0.73-0.83 0.67 (0.15) 0.50-0.83 36 0.86 (0.10) 0.64-1.00 0.59 (0.30) 0.18-1.00 0.92 (0.02) 0.91-1.00 0.43 (0.35) 0.00-1.00 0.90 (0.04) 0.82-1.00 0.43 (0.34) 0.00-1.00 43 0.81 (0.06) 0.73-0.91 0.62 (0.21) 0.33-0.83 0.83 (0.06) 0.73-0.91 0.42 (0.31) 0.06-0.94 0.81 (0.04) 0.80-0.90 0.66 (0.15) 0.50-0.88 51 0.66 (0.12) 0.33-0.82 0.59 (0.18) 0.27-0.83 0.72 (0.12) 0.45-0.92 0.33 (0.18) 0.07-0.70 0.73 (0.06) 0.60-0.83 0.54 (0.09) 0.48-0.78 53 0.92 (0.03) 0.83-0.93 0.53 (0.29) 0.08-1.00 0.92 (&lt;.01) 0.92-0.93 0.46 (0.28) 0.00-0.77 0.93 (0.02) 0.91-1.00 0.53 (0.34) 0.00-0.93 58 0.92 (&lt;.01) 0.92-0.92 0.37 (0.23) 0.17-0.64 0.91 (&lt;.01) 0.91-0.92 0.50 (0.35) 0.14-0.91 63 0.90 (0.03) 0.82-0.91 0.51 (0.25) 0.20-0.90 0.90 (0.02) 0.82-0.91 0.53 (0.29) 0.10-0.90 0.93 (0.04) 0.90-1.00 0.73 (0.20) 0.40-1.00 67 0.86 (0.02) 0.86-0.93 0.37 (0.18) 0.15-0.62 68 0.89 (0.05) 0.82-0.91 0.42 (0.44) 0.00-0.90 0.91 (&lt;.01) 0.91-0.91 0.59 (0.34) 0.10-0.90 0.90 (&lt;.01) 0.90-0.91 0.46 (0.42) 0.00-1.00 71 0.91 (&lt;.01) 0.91-0.91 0.59 (0.39) 0.00-1.00 0.90 (0.05) 0.73-0.91 0.47 (0.37) 0.00-1.00 0.89 (0.04) 0.80-0.91 0.57 (0.25) 0.28-1.00 73 0.93 (&lt;.01) 0.92-0.93 0.56 (0.31) 0.15-1.00 0.93 (&lt;.01) 0.92-0.93 0.52 (0.32) 0.00-0.92 0.92 (&lt;.01) 0.92-0.92 0.85 (0.16) 0.50-1.00 84 0.92 (&lt;.01) 0.92-0.92 0.60 (0.19) 0.38-1.00 0.92 (&lt;.01) 0.92-0.92 0.29 (0.21) 0.08-0.71 0.92 (&lt;.01) 0.91-0.92 0.51 (0.34) 0.00-0.90 88 0.90 (&lt;.01) 0.90-0.90 0.42 (0.50) 0.00-1.00 0.90 (&lt;.01) 0.90-0.90 0.50 (0.28) 0.11-0.78 0.89 (0.01) 0.88-0.90 0.72 (0.26) 0.44-1.00 6.2.1.2 Figure (Figure 2) Now, we’ll create the figure that samples 25 participants ranges. We’ll create separate figures for each outcome (Procrastination, Loneliness) and metric (accuracy, AUC), which will be in Supplemental Materials (05-results/05/figures/01-px-sum-dist). Then, we’ll create a combined version for accuracy and both outcomes that will become Figure 2 in the manuscript. px_sum_plot &lt;- function(d, metric, outcome, model){ m &lt;- if(metric == &quot;accuracy&quot;) &quot;Accuracy&quot; else &quot;AUC&quot; mod &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;), warn_missing = F) # set.seed(6) d %&gt;% # filter(SID %in% sample(SID, 25)) %&gt;% mutate(SID = forcats::fct_reorder(SID, .estimate, median)) %&gt;% ggplot(aes(x = SID, y = .estimate)) + scale_y_continuous(limits = c(0,1), breaks = seq(0,1,.5)) + stat_pointinterval() + labs(x = NULL, y = m, title = mod) + coord_flip() + # facet_grid(. ~ , scales = &quot;free&quot;, space = &quot;free&quot;) + theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;, size = rel(1.2), hjust = .5) , axis.text = element_text(face = &quot;bold&quot;, color = &quot;black&quot;) , axis.title = element_text(face = &quot;bold&quot;, size = rel(1.1)) , axis.line = element_blank() , panel.background = element_rect(color = &quot;black&quot;, size = 1) # , plot.margin = margin(1,.1,.1,.1, unit = &quot;cm&quot;) ) } combine_px_plots &lt;- function(d, outcome, metric){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) my_theme &lt;- function(...) { theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) } title_theme &lt;- calc_element(&quot;plot.title&quot;, my_theme()) ttl &lt;- ggdraw() + draw_label( o, fontfamily = title_theme$family, fontface = title_theme$face, size = title_theme$size ) p1 &lt;- d$p[[1]] + labs(y = &quot;&quot;); p2 &lt;- d$p[[2]]; p3 &lt;- d$p[[3]] + labs(y = &quot;&quot;) p &lt;- cowplot::plot_grid(p1, p2, p3, nrow = 1, axis = &quot;b&quot;) p &lt;- plot_grid(ttl, p, nrow = 2, rel_heights = c(.05,.95)) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/01-px-sum-dist/%s_%s.pdf&quot; , res_path, outcome, metric) , width = 8 , height = 5) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/01-px-sum-dist/png/%s_%s.png&quot; , res_path, outcome, metric) , width = 8 , height = 5) return(p) } set.seed(8) px_plots_sum &lt;- sum_res %&gt;% unnest(data) %&gt;% group_by(outcome) %&gt;% filter(SID %in% sample(unique(SID), 25)) %&gt;% group_by(outcome, .metric, model) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;)) , p = pmap(list(data, .metric, outcome, model), px_sum_plot)) %&gt;% arrange(outcome, model, .metric) %&gt;% group_by(outcome, .metric) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = pmap(list(data, outcome, .metric), combine_px_plots)) p &lt;- cowplot::plot_grid(px_plots_sum$p[[3]], px_plots_sum$p[[1]] , nrow = 2) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-2-accuracy.pdf&quot; , res_path) , width = 8, height = 10) 6.2.1.2.1 Procrastination, Accuracy px_plots_sum$p[[3]] 6.2.1.2.2 Procrastination, AUC px_plots_sum$p[[4]] 6.2.1.2.3 Loneliness, Accuracy px_plots_sum$p[[1]] 6.2.1.2.4 Loneliness, AUC px_plots_sum$p[[2]] 6.3 Question 3: Do Psychological, Situational, or Full Feature Sets Perform Best? To answer the question of whether psychological, situational, or full feature sets (with or without time) perform best, we’ll pull the performance metric data we’ve been working with and combine it with information about specific coefficinets. 6.3.1 Psychological Features, Situations, or Time? 6.3.1.1 Table ord &lt;- paste(rep(c(&quot;lonely&quot;, &quot;prcrst&quot;), each = 6) , rep(c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), each = 2, times = 2) , rep(c(&quot;n&quot;, &quot;perc&quot;), times = 6) , sep = &quot;_&quot;) fps_tab &lt;- best_mods %&gt;% group_by(model, outcome, group, time, .metric) %&gt;% tally() %&gt;% group_by(model, outcome, .metric) %&gt;% mutate(perc = n/(sum(n, na.rm = T))*100 , perc = sprintf(&quot;%.1f%%&quot;,perc)) %&gt;% ungroup() %&gt;% pivot_wider(names_from = c(&quot;outcome&quot;, &quot;model&quot;) , values_from = c(&quot;n&quot;, &quot;perc&quot;) , names_glue = &quot;{outcome}_{model}_{.value}&quot; , names_sort = T) %&gt;% mutate_all(~ifelse(is.na(.), &quot;0&quot;, .)) %&gt;% arrange(.metric, group, time) %&gt;% mutate(group = factor(str_to_title(group)) , time = factor(time, c(&quot;no time&quot;, &quot;time&quot;), c(&quot;No&quot;, &quot;Yes&quot;))) %&gt;% select(group, time, ord) %&gt;% kable(. , &quot;html&quot; , escape = F , col.names = c(&quot;Set&quot;, &quot;Time&quot;, rep(c(&quot;#&quot;, &quot;%&quot;), times = 6)) , align = c(&quot;r&quot;, &quot;r&quot;, rep(&quot;c&quot;, 12)) , cap = &quot;&lt;strong&gt;Table X&lt;/strong&gt;&lt;br&gt;&lt;em&gt;Frequencies of the Full, Psychological, and Situation Feature Sets with or Without Time Being the Best Model for a Participant&lt;/em&gt;&quot; ) %&gt;% kable_styling(full_width = F) %&gt;% collapse_rows(1) %&gt;% kableExtra::group_rows(&quot;Accuracy&quot;, 1, 6) %&gt;% kableExtra::group_rows(&quot;AUC&quot;, 7, 12) %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2, &quot;Elastic Net&quot; = 2, &quot;BISCWIT&quot; = 2, &quot;Random Forest&quot; = 2)) %&gt;% add_header_above(c(&quot; &quot; = 2, &quot;Loneliness&quot; = 6, &quot;Procrastination&quot; = 6)) save_kable(fps_tab, file = sprintf(&quot;%s/05-results/04-tables/02-feature-perf-tab.html&quot;, res_path)) fps_tab Table 6.5: Table XFrequencies of the Full, Psychological, and Situation Feature Sets with or Without Time Being the Best Model for a Participant Loneliness Procrastination Elastic Net BISCWIT Random Forest Elastic Net BISCWIT Random Forest Set Time # % # % # % # % # % # % Accuracy Full No 33 66.0% 34 66.7% 16 32.0% 49 59.8% 45 50.6% 27 31.0% Full Yes 4 8.0% 0 0 2 4.0% 7 8.5% 11 12.4% 7 8.0% Psychological No 6 12.0% 9 17.6% 4 8.0% 12 14.6% 15 16.9% 11 12.6% Psychological Yes 3 6.0% 4 7.8% 1 2.0% 6 7.3% 8 9.0% 2 2.3% Situations No 3 6.0% 4 7.8% 26 52.0% 6 7.3% 7 7.9% 38 43.7% Situations Yes 1 2.0% 0 0 1 2.0% 2 2.4% 3 3.4% 2 2.3% AUC Full No 7 14.0% 9 17.6% 13 26.0% 19 23.2% 19 21.3% 17 19.8% Full Yes 5 10.0% 3 5.9% 6 12.0% 7 8.5% 9 10.1% 8 9.3% Psychological No 11 22.0% 11 21.6% 9 18.0% 20 24.4% 16 18.0% 13 15.1% Psychological Yes 9 18.0% 4 7.8% 6 12.0% 8 9.8% 16 18.0% 10 11.6% Situations No 10 20.0% 15 29.4% 12 24.0% 19 23.2% 18 20.2% 27 31.4% Situations Yes 8 16.0% 9 17.6% 4 8.0% 9 11.0% 11 12.4% 11 12.8% 6.3.1.2 Figure (Figure 3) Next, to demonstrate the relative performance of feature sets and coefficients within those feature sets, we’ll create a series of sequence plots that show the proportion of features from each category for each participant. This will become Figure 3. seq_plot_fun &lt;- function(d, outcome, model){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name) m &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;) , c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;) , warn_missing = F) ord &lt;- (d %&gt;% select(-n) %&gt;% pivot_wider(names_from = &quot;category&quot;, values_from = &quot;perc&quot;) %&gt;% arrange(desc(psychological)))$SID p &lt;- d %&gt;% mutate(SID = factor(SID, levels = ord), category = factor(category,rev(unique(category)) , str_to_title(rev(unique(category))))) %&gt;% ggplot(aes(x = SID , y = perc , rev = T) ) + scale_fill_manual(values = c(&quot;lightgoldenrod1&quot;, &quot;seagreen3&quot;, &quot;deepskyblue4&quot;)) + geom_bar(aes(fill = category) , stat = &quot;identity&quot; ) + labs(x = &quot;Participant ID&quot; , y = &quot;Percentage&quot; , fill = &quot;Feature Category&quot; , title = m) + coord_flip() + theme_classic() + theme(legend.position = &quot;bottom&quot; , axis.text.y = element_blank() , axis.ticks.y = element_blank() , axis.text.x = element_text(face = &quot;bold&quot;, size = rel(1.2), color = &quot;black&quot;) , axis.title = element_text(face = &quot;bold&quot;, size = rel(1.2)) , legend.text = element_text(face = &quot;bold&quot;) , legend.title = element_text(face = &quot;bold&quot;) , plot.title = element_text(face = &quot;bold&quot;, hjust = .5) ) return(p) } ## first get counts and percentages and create each plot for each outcome and model seq_plot &lt;- param_res %&gt;% right_join(best_mods %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% select(model:time)) %&gt;% select(-params) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;)))) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;)) %&gt;% filter(coef != 0) %&gt;% left_join(ftrs %&gt;% select(category = group, Variable = old_name, new_name)) %&gt;% filter(!is.na(category)) %&gt;% group_by(SID, outcome, model, category) %&gt;% tally() %&gt;% group_by(SID, outcome, model) %&gt;% mutate(perc = n/sum(n)*100) %&gt;% group_by(outcome, model) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(p = pmap(list(data, outcome, model), seq_plot_fun)) # seq_plot &lt;- seq_plot %&gt;% # unnest(data) %&gt;% # group_by(outcome, SID, category) %&gt;% # summarize(perc = mean(perc, na.rm = T)) %&gt;% # ungroup() %&gt;% # mutate(model = &quot;combined&quot;) %&gt;% # group_by(outcome, model) %&gt;% # nest() %&gt;% # ungroup() %&gt;% # full_join(seq_plot) %&gt;% # mutate(p = pmap(list(data, outcome), seq_plot_fun)) my_theme &lt;- function(...) { theme_classic() + theme(plot.title = element_text(face = &quot;bold&quot;)) } title_theme &lt;- calc_element(&quot;plot.title&quot;, my_theme()) legend &lt;- get_legend(seq_plot$p[[1]]) seq_plot &lt;- seq_plot %&gt;% mutate(p = map(p, ~(.) + theme(legend.position = &quot;none&quot;))) p1 &lt;- plot_grid( (seq_plot %&gt;% filter(outcome == &quot;prcrst&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] + labs(y = NULL) , (seq_plot %&gt;% filter(outcome == &quot;prcrst&quot; &amp; model == &quot;biscwit&quot;))$p[[1]]+ labs(x = NULL, y = NULL) , (seq_plot %&gt;% filter(outcome == &quot;prcrst&quot; &amp; model == &quot;rf&quot;))$p[[1]]+ labs(x = NULL, y = NULL) , nrow = 1 , axis = &quot;b&quot; , align = &quot;hv&quot; ) ttl &lt;- ggdraw() + draw_label(&quot;Procrastination&quot;, fontface = title_theme$face) p1 &lt;- plot_grid(ttl, p1, nrow = 2, rel_heights = c(.05, .95)) p2 &lt;- plot_grid( (seq_plot %&gt;% filter(outcome == &quot;lonely&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] + labs(y = NULL, title = NULL) , (seq_plot %&gt;% filter(outcome == &quot;lonely&quot; &amp; model == &quot;biscwit&quot;))$p[[1]]+ labs(x = NULL, title = NULL) , (seq_plot %&gt;% filter(outcome == &quot;lonely&quot; &amp; model == &quot;rf&quot;))$p[[1]]+ labs(x = NULL, y = NULL, title = NULL) , nrow = 1 , axis = &quot;b&quot; , align = &quot;hv&quot; ) ttl &lt;- ggdraw() + draw_label(&quot;Loneliness&quot;, fontface = title_theme$face) p2 &lt;- plot_grid(ttl, p2, nrow = 2, rel_heights = c(.05, .95)) p &lt;- plot_grid(p1, p2, nrow = 2, rel_heights = c(.6, .4)) p &lt;- plot_grid(p, legend, nrow = 2, rel_heights = c(.95, .05)); p Figure 6.2: Figure 3. Sequence plots of the percentage of features from the Psychological, Situational, and Time Features Sets for each participant for each outcome. ggsave(p , file = sprintf(&quot;%s/05-results/05-figures/fig-3-seq-plot.pdf&quot; , res_path) , width = 8, height = 8) ggsave(p , file = sprintf(&quot;%s/05-results/05-figures/fig-3-seq-plot.png&quot; , res_path) , width = 8, height = 8) 6.4 Question 4: Which features are most associated with Procrastination and Loneliness? 6.4.1 Feature Frequency: Psychological Features, Situations, or Time? 6.4.1.1 Figure (Figure 4) var_freq &lt;- param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config) %&gt;% filter(.metric == &quot;accuracy&quot;) ) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;)))) %&gt;% select(-params) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;)) %&gt;% filter(coef != 0) %&gt;% # select(-.) %&gt;% group_by(model, SID, outcome) %&gt;% arrange(desc(abs(coef))) %&gt;% slice_max(abs(coef), n = 5) %&gt;% group_by(model, outcome) %&gt;% mutate(N = length(unique(SID))) %&gt;% group_by(model, outcome, Variable, N) %&gt;% tally() %&gt;% ungroup() %&gt;% mutate(n = n/N*100) %&gt;% left_join(ftrs %&gt;% select(group, Variable = old_name, new_name) %&gt;% mutate(group = str_to_title(group)) %&gt;% group_by(group) %&gt;% mutate(ni = 1:n(), ni = ifelse(ni &lt; 10, paste0(&quot;0&quot;, ni), ni), Variable2 = paste0(substr(group, 1, 1), ni) ) %&gt;% ungroup() ) %&gt;% filter(!is.na(group)) %&gt;% distinct() p1 &lt;- var_freq %&gt;% filter(outcome == &quot;prcrst&quot;) %&gt;% mutate(model2 = as.numeric(mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), seq(5,1,-2))), # model2 = ifelse(outcome == &quot;prcrst&quot;, model2 + 1, model2), model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), new_name = factor(new_name, ftrs$new_name), outcome = factor(outcome, outcomes$trait, outcomes$long_name)) %&gt;% arrange(model) %&gt;% ggplot(aes(x = Variable2 , y = model2 , group = factor(Variable2) , fill = model # , color = model # , shape = group , rev=F )) + geom_line(size = .2) + #keep this here, otherwise there is an error xlab(&quot;&quot;) + ylab(&quot;&quot;) + # Generate the grid lines geom_hline(yintercept = 1:7 , colour = &quot;grey80&quot; , size = .2) + geom_vline(xintercept = 1:67 , colour = &quot;grey80&quot; , size = .2) + # Points and lines geom_line(colour=&quot;grey80&quot;, size = .2) + geom_point(aes(size = n, alpha = n) , color = &quot;black&quot; , shape = 21) + # Fill the middle space with a white blank rectangle geom_rect(xmin=-Inf ,xmax=Inf ,ymin=-Inf ,ymax=0 ,fill=&quot;white&quot; , color=NA) + scale_y_continuous(limits=c(-5,5.5) , expand=c(0,0) , breaks=seq(1,5,2) , labels = NULL) + scale_size_continuous(range = c(.5,8) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 28) # 5 features , breaks = c(5, 10, 15, 20, 25)) + # 5 features # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_alpha_continuous(range = c(.3, 1) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 28) # 5 features , breaks = c(5, 10, 15, 20, 25)) + # 5 features # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_fill_manual( values = c(&quot;deepskyblue4&quot;, &quot;seagreen3&quot;, &quot;lightgoldenrod1&quot;) , drop = F ) + # Polar coordinates coord_polar() + # facet_wrap(~outcome, nrow = 2) + # The angle for the symptoms and remove the default grid lines theme_classic()+ theme(axis.text.x = element_text(angle = 360/(2*pi)*rev( pi/2 + seq(pi/67, 2*pi-pi/67, len=67)) + c(rep(0,floor(67/2)), rep(180,ceiling(67/2))), size = rel(1.1), face = &quot;bold&quot;) , panel.border = element_blank() , axis.line = element_blank() , axis.ticks = element_blank() , panel.grid = element_blank() , panel.background = element_blank() , legend.position=&quot;bottom&quot; # , legend.position = &quot;none&quot; # legend.direction = &quot;vertical&quot;, , plot.margin = margin(t = .5, r = 0, l = 0, b = 0, unit = &quot;cm&quot;) , plot.title = element_text(face = &quot;bold&quot;, hjust = .5) , strip.background = element_blank() , strip.text = element_text(face = &quot;bold&quot;, size = rel(1.2)) ) + labs(size = &quot;% Participants&quot; # , fill = &quot;Model&quot; , alpha = &quot;% Participants&quot; , title = &quot;Procrastination&quot;) + guides(size = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5) , fill = &quot;none&quot;#guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5) , alpha = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5) , shape = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5)) legend &lt;- cowplot::get_legend(p1) p1 &lt;- p1 + theme(legend.position = &quot;none&quot;) p1 &lt;- plot_grid(p1, legend, nrow = 2, rel_heights = c(.9, .1)) mx &lt;- max((var_freq %&gt;%filter(outcome == &quot;lonely&quot;))$n) p2 &lt;- var_freq %&gt;% filter(outcome == &quot;lonely&quot;) %&gt;% mutate(model2 = as.numeric(mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), seq(5,1,-2))), # model2 = ifelse(outcome == &quot;prcrst&quot;, model2 + 1, model2), model = factor(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), new_name = factor(new_name, ftrs$new_name), outcome = factor(outcome, outcomes$trait, outcomes$long_name)) %&gt;% arrange(model) %&gt;% ggplot(aes(x = Variable2 , y = model2 , group = factor(Variable2) , fill = model # , color = model # , shape = group , rev=F )) + geom_line(size = .2) + #keep this here, otherwise there is an error xlab(&quot;&quot;) + ylab(&quot;&quot;) + # Generate the grid lines geom_hline(yintercept = 1:7 , colour = &quot;grey80&quot; , size = .2) + geom_vline(xintercept = 1:67 , colour = &quot;grey80&quot; , size = .2) + # Points and lines geom_line(colour=&quot;grey80&quot;, size = .2) + geom_point(aes(size = n, alpha = n) , color = &quot;black&quot; , shape = 21) + # Fill the middle space with a white blank rectangle geom_rect(xmin=-Inf ,xmax=Inf ,ymin=-Inf ,ymax=0 ,fill=&quot;white&quot; , color=NA) + scale_y_continuous(limits=c(-8,5.5) , expand=c(0,0) , breaks=seq(1,5,2) , labels = NULL) + scale_size_continuous(range = c(.5,8) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 35) # 5 features , breaks = c(5, 12.5, 20, 27.5, 35) # 5 features , labels = c(&quot;5&quot;, &quot;12.5&quot;, &quot;20&quot;, &quot;27.5&quot;, &quot;35&quot;)) + # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_alpha_continuous(range = c(.3, 1) # , limits = c(0, 50) # 10 features # , breaks = c(5, 15, 25, 35, 45)) + # 10 features , limits = c(0, 35) # 5 features , breaks = c(5, 12.5, 20, 27.5, 35) # 5 features , labels = c(&quot;5&quot;, &quot;12.5&quot;, &quot;20&quot;, &quot;27.5&quot;, &quot;35&quot;)) + # 5 features # , limits = c(0, 26.5) # 3 features # , breaks = c(5, 10, 15, 20, 25)) + # 3 features scale_fill_manual( values = c(&quot;deepskyblue4&quot;, &quot;seagreen3&quot;, &quot;lightgoldenrod1&quot;) , drop = F ) + # Polar coordinates coord_polar() + # facet_wrap(~outcome, nrow = 2) + # The angle for the symptoms and remove the default grid lines theme_classic()+ theme(axis.text.x = element_text(angle = 360/(2*pi)*rev( pi/2 + seq(pi/67, 2*pi-pi/67, len=67)) + c(rep(0,floor(67/2)), rep(180,ceiling(67/2))), size = rel(1.1), face = &quot;bold&quot;) , panel.border = element_blank() , axis.line = element_blank() , axis.ticks = element_blank() , panel.grid = element_blank() , panel.background = element_blank() , legend.position=&quot;bottom&quot; # legend.direction = &quot;vertical&quot;, , plot.margin = margin(t = .5, r = 0, l = 0, b = 0, unit = &quot;cm&quot;) , plot.title = element_text(face = &quot;bold&quot;, hjust = .5) , strip.background = element_blank() , strip.text = element_text(face = &quot;bold&quot;, size = rel(1.2)) ) + labs(size = &quot;% Participants&quot; , fill = &quot;Model&quot; , alpha = &quot;% Participants&quot; , title = &quot;Loneliness&quot;) + # guides(size = &quot;none&quot; # , fill = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, label.hjust = 0) # , alpha = &quot;none&quot; # , shape = &quot;none&quot;) guides(size = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, order = 1, label.hjust = 0.1) , fill = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, label.hjust = 0) , alpha = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1, label.hjust = 0.1) , shape = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1, label.hjust = 0.1)) legend &lt;- cowplot::get_legend(p2) p2 &lt;- p2 + theme(legend.position = &quot;none&quot;) # guides(size = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1) # , fill = &quot;none&quot; # , alpha = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1) # , shape = guide_legend(title.position=&quot;top&quot;, title.hjust = 0.5, order = 1)) p3 &lt;- var_freq %&gt;% select(new_name, Variable2) %&gt;% distinct() %&gt;% mutate(new_name = factor(new_name, ftrs$new_name), names = paste0(Variable2, &quot;: &quot;, new_name)) %&gt;% arrange(Variable2) %&gt;% mutate(names = factor(names, .$names)) %&gt;% ggplot(aes(x = 1, y = 1:66)) + geom_text(aes(label = rev(names)), hjust = 0, size = 3) + scale_x_continuous(limits = c(.9999, 1.1))+ theme_classic() + theme(axis.line = element_blank() , axis.text = element_blank() , axis.ticks = element_blank() , axis.title = element_blank()) # p3 &lt;- plot_grid(p3, legend, nrow = 2, rel_heights = c(.95, .05)) + # theme(plot.margin = margin(.1,.5,.5,-1, unit = &quot;cm&quot;)) p &lt;- plot_grid(p1, p2, nrow = 2, rel_heights = c(.53, .47)) p &lt;- plot_grid(p, p3, nrow = 1, rel_widths = c(.65, .35)) p &lt;- plot_grid(p, legend, nrow = 2, rel_heights = c(.95, .05)); p ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-4-combined_top5.pdf&quot;, res_path), height = 12 , width = 9) ggsave(p, file = sprintf(&quot;%s/05-results/05-figures/fig-4-combined_top5.png&quot;, res_path), height = 12 , width = 9) 6.5 Question 5: Do people vary in the which features are most important? Next we want to address not just general frequencies of important coefficients but also patterns of coefficients at the participant level. To do this, we’ll (1) make tables of all coefficients for each participant’s best model for all outcomes and models, (2) make a figure that displays this graphically, and (3) examine whether there are patterns of coefficients across people. 6.5.1 Participant Coefficients 6.5.1.1 Table First, let’s create tables for each participant and outcome combination of the coefficients from their models. As we’ve previously selected the feature set with the best performance, features from other sets will automatically be set to 0. In addition, as each of the models we used have feature selection procedures, features not chosen by the model will also be 0. px_coef_tab_fun &lt;- function(d, SID, outcome){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) mchar &lt;- d %&gt;% select(model, group, accuracy) %&gt;% distinct() %&gt;% mutate(model = mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;)), tmp = sprintf(&quot;%s: best model was %s with accuracy %.2f&quot;, model, group, accuracy)) note &lt;- paste(mchar$tmp, collapse = &quot;; &quot;); note &lt;- paste0(note, &quot;.&quot;) d2 &lt;- d %&gt;% select(-group, -accuracy) %&gt;% pivot_wider(names_from = &quot;model&quot; , values_from = &quot;coef&quot; , values_fn = mean) %&gt;% mutate_at(vars(-Variable), ~ifelse(abs(.) &gt; .01, sprintf(&quot;%.2f&quot;, .), ifelse(. == 0, &quot;0&quot;, ifelse(. &gt; -.01 &amp; . &lt; 0, sprintf(&quot;&gt; -.01&quot;, .), sprintf(&quot;&lt; .01&quot;, .))))) %&gt;% full_join(ftrs %&gt;% select(group, Variable = old_name, new_name)) %&gt;% filter(!is.na(group)) %&gt;% select(-Variable) %&gt;% mutate(new_name = factor(new_name, ftrs$new_name) , group = str_to_title(group)) %&gt;% mutate_at(vars(glmnet, biscwit, rf), ~ifelse(is.na(.), 0, .)) %&gt;% arrange(new_name) rs &lt;- d2 %&gt;% group_by(group) %&gt;% tally() %&gt;% mutate(end = cumsum(n), start = lag(end) + 1, start = ifelse(is.na(start), 1, start)) tab &lt;- d2 %&gt;% select(new_name, glmnet, biscwit, rf) %&gt;% kable(. , &quot;html&quot; , escape = F , col.names = c(&quot;Variable&quot;, &quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;) , align = c(&quot;r&quot;, rep(&quot;c&quot;, 3)) , cap = sprintf(&quot;%s Model Coefficients for Participant %s&quot;, o, SID)) %&gt;% kable_styling(full_width = F) %&gt;% add_footnote(note, label = NULL) for (i in 1:nrow(rs)){ tab &lt;- tab %&gt;% kableExtra::group_rows(rs$group[i], rs$start[i], rs$end[i]) } save_kable(tab, file = sprintf(&quot;%s/05-results/04-tables/05-participant-coef/%s_%s.html&quot; , res_path, SID, outcome)) return(tab) } px_coef &lt;- param_res %&gt;% right_join(best_mods %&gt;% select(-.estimator, -.config) %&gt;% filter(.metric == &quot;accuracy&quot;) ) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;)))) %&gt;% select(-params) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;), group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) %&gt;% select(-set, -.metric, -time) %&gt;% rename(accuracy = .estimate) %&gt;% group_by(SID, outcome) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(tab = pmap(list(data, SID, outcome), possibly(px_coef_tab_fun, NA_real_))); px_coef ## # A tibble: 140 x 4 ## SID outcome data tab ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 01 prcrst &lt;tibble[,5] [149 × 5]&gt; &lt;kablExtr [1]&gt; ## 2 02 prcrst &lt;tibble[,5] [225 × 5]&gt; &lt;kablExtr [1]&gt; ## 3 05 prcrst &lt;tibble[,5] [143 × 5]&gt; &lt;kablExtr [1]&gt; ## 4 08 lonely &lt;tibble[,5] [187 × 5]&gt; &lt;kablExtr [1]&gt; ## 5 09 prcrst &lt;tibble[,5] [101 × 5]&gt; &lt;kablExtr [1]&gt; ## 6 10 lonely &lt;tibble[,5] [94 × 5]&gt; &lt;kablExtr [1]&gt; ## 7 103 lonely &lt;tibble[,5] [141 × 5]&gt; &lt;kablExtr [1]&gt; ## 8 103 prcrst &lt;tibble[,5] [166 × 5]&gt; &lt;kablExtr [1]&gt; ## 9 105 lonely &lt;tibble[,5] [97 × 5]&gt; &lt;kablExtr [1]&gt; ## 10 105 prcrst &lt;tibble[,5] [185 × 5]&gt; &lt;kablExtr [1]&gt; ## # … with 130 more rows All of these tables will be contained with the R shiny web app. However, the three sample participants from the manuscript are shown below. 6.5.1.1.1 Participant 169 (px_coef %&gt;% filter(SID == &quot;169&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.6: Procrastinating Model Coefficients for Participant 169 Variable Elastic Net BISCWIT Random Forest Psychological Extraversion: Sociability 0.12 0 &gt; -.01 Extraversion: Assertiveness -0.73 0 &lt; .01 Extraversion: Energy Level 0.68 0.12 &gt; -.01 Agreeableness: Compassion 0.50 0.13 &gt; -.01 Agreeableness: Respectfulness -0.15 0 &lt; .01 Agreeableness: Trust 0.15 0 &lt; .01 Conscientiousness: Organization 0.53 0 &gt; -.01 Conscientiousness: Productiveness -0.74 -0.17 &gt; -.01 Conscientiousness: Responsibility 0.62 0.15 &lt; .01 Neuroticism: Anxiety 0.13 0.12 &gt; -.01 Neuroticism: Depression -0.02 0 &gt; -.01 Neuroticism: Emotional Volatility 0.08 0 &lt; .01 Openness: Intellectual Curiosity -1.07 0 &gt; -.01 Openness: Aesthetic Sensitivity 0.14 0.15 &gt; -.01 Openness: Creative Imagination 1.80 0.29 &lt; .01 Negative: Angry -0.51 0 &lt; .01 Negative: Afraid 1.42 0.26 &lt; .01 Positive: Happy -0.38 0 &gt; -.01 Positive: Excited -0.54 0 &gt; -.01 Positive: Proud 0.33 0.11 &gt; -.01 Negative: Guilty 0.21 0.12 &lt; .01 Positive: Attentive -0.60 0 &gt; -.01 Positive: Content 0.33 0 &gt; -.01 Neutral: Purposeful 0.63 0 &gt; -.01 Neutral: Goal-directed 0.05 0 &lt; .01 Situations Duty 0 0 0 Intellect 0 0 0 Adversity 0 0 0 Mating 0 0 0 pOsitivity 0 0 0 Negativity 0 0 0 Deception 0 0 0 Sociability 0 0 0 Studying 0 0 0 Argument 0 0 0 Interacted 0 0 0 Lost something 0 0 0 Late 0 0 0 Forgot something 0 0 0 Bored with schoolwork 0 0 0 Excited about schoolwork 0 0 0 Anxious about schoolwork 0 0 0 Tired 0 0 0 Sick 0 0 0 Sleeping 0 0 0 In Class 0 0 0 Listening to music 0 0 0 On the internet 0 0 0 Watching TV 0 0 0 Time Monday 0 0 0 Tuesday 0 0 0 Wednesday 0 0 0 Thursday 0 0 0 Friday 0 0 0 Saturday 0 0 0 Sunday 0 0 0 Morning 0 0 0 Midday 0 0 0 Evening 0 0 0 Night 0 0 0 Linear Trend 0 0 0 Quadratic Trend 0 0 0 Cubic Trend 0 0 0 24 hour Sinusoidal Cycle 0 0 0 12 hour Sinusoidal Cycle 0 0 0 24 hour Cosinusoidal Cycle 0 0 0 12 hour Cosinusoidal Cycle 0 0 0 6.5.1.1.2 Participant 43 (px_coef %&gt;% filter(SID == &quot;43&quot; &amp; outcome == &quot;lonely&quot;))$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.6: Lonely Model Coefficients for Participant 43 Variable Elastic Net BISCWIT Random Forest Psychological Extraversion: Sociability 0 0 0 Extraversion: Assertiveness 0 0 0 Extraversion: Energy Level 0 0 0 Agreeableness: Compassion 0 0 0 Agreeableness: Respectfulness 0 0 0 Agreeableness: Trust 0 0 0 Conscientiousness: Organization 0 0 0 Conscientiousness: Productiveness 0 0 0 Conscientiousness: Responsibility 0 0 0 Neuroticism: Anxiety 0 0 0 Neuroticism: Depression 0 0 0 Neuroticism: Emotional Volatility 0 0 0 Openness: Intellectual Curiosity 0 0 0 Openness: Aesthetic Sensitivity 0 0 0 Openness: Creative Imagination 0 0 0 Negative: Angry 0 0 0 Negative: Afraid 0 0 0 Positive: Happy 0 0 0 Positive: Excited 0 0 0 Positive: Proud 0 0 0 Negative: Guilty 0 0 0 Positive: Attentive 0 0 0 Positive: Content 0 0 0 Neutral: Purposeful 0 0 0 Neutral: Goal-directed 0 0 0 Situations Duty -5.45 -0.62 &lt; .01 Intellect 3.02 0 &gt; -.01 Adversity -0.37 -0.17 &lt; .01 Mating 0 0 0 pOsitivity 0.31 0 &gt; -.01 Negativity 0.40 -0.13 &gt; -.01 Deception 0 0 0 Sociability -1.86 -0.15 &lt; .01 Studying -0.49 -0.21 &gt; -.01 Argument -1.63 0 &lt; .01 Interacted -1.22 -0.45 &lt; .01 Lost something 0 0 &gt; -.01 Late 0.75 0.29 &lt; .01 Forgot something 0 -0.17 0 Bored with schoolwork 0.20 0 &gt; -.01 Excited about schoolwork -1.50 -0.20 &gt; -.01 Anxious about schoolwork 2.56 0 &gt; -.01 Tired 0 0 &gt; -.01 Sick 4.58 0.38 &lt; .01 Sleeping -1.58 0 &gt; -.01 In Class -3.03 -0.28 &lt; .01 Listening to music 0 0 &gt; -.01 On the internet -0.43 -0.25 &lt; .01 Watching TV -0.10 -0.20 &gt; -.01 Time Monday 0 0 0 Tuesday 0 0 0 Wednesday 0 0 0 Thursday 0 0 0 Friday 0 0 0 Saturday 0 0 0 Sunday 0 0 0 Morning 0 0 0 Midday 0 0 0 Evening 0 0 0 Night 0 0 0 Linear Trend 0 0 0 Quadratic Trend 0 0 0 Cubic Trend 0 0 0 24 hour Sinusoidal Cycle 0 0 0 12 hour Sinusoidal Cycle 0 0 0 24 hour Cosinusoidal Cycle 0 0 0 12 hour Cosinusoidal Cycle 0 0 0 6.5.1.1.3 Participant 160 (px_coef %&gt;% filter(SID == &quot;160&quot; &amp; outcome == &quot;prcrst&quot;))$tab[[1]] %&gt;% scroll_box(height = &quot;750px&quot;) Table 6.6: Procrastinating Model Coefficients for Participant 160 Variable Elastic Net BISCWIT Random Forest Psychological Extraversion: Sociability 0.03 0 &lt; .01 Extraversion: Assertiveness 0.91 0.30 &gt; -.01 Extraversion: Energy Level -0.18 0 &gt; -.01 Agreeableness: Compassion 0.16 0 &gt; -.01 Agreeableness: Respectfulness 0.39 0.12 &gt; -.01 Agreeableness: Trust -0.39 0 &lt; .01 Conscientiousness: Organization 0.09 0 &gt; -.01 Conscientiousness: Productiveness -0.51 -0.17 &lt; .01 Conscientiousness: Responsibility 0.23 0 &lt; .01 Neuroticism: Anxiety 0.15 0 &gt; -.01 Neuroticism: Depression -0.17 -0.20 &lt; .01 Neuroticism: Emotional Volatility -0.13 -0.18 &lt; .01 Openness: Intellectual Curiosity 0.18 0 &gt; -.01 Openness: Aesthetic Sensitivity 0.76 0.28 &lt; .01 Openness: Creative Imagination 0.48 0 &lt; .01 Negative: Angry -0.19 0 &gt; -.01 Negative: Afraid -0.26 -0.12 &lt; .01 Positive: Happy -0.25 -0.13 &gt; -.01 Positive: Excited 0.11 0 &lt; .01 Positive: Proud -0.20 0 &lt; .01 Negative: Guilty 0.31 0 &lt; .01 Positive: Attentive -0.22 -0.27 &gt; -.01 Positive: Content 0.49 0 &gt; -.01 Neutral: Purposeful -0.06 0 &lt; .01 Neutral: Goal-directed -0.24 -0.16 &lt; .01 Situations Duty 0.29 0 &gt; -.01 Intellect -0.47 -0.25 &gt; -.01 Adversity 0 0 0 Mating -0.29 0 &gt; -.01 pOsitivity -0.07 0 &gt; -.01 Negativity -0.16 0 &lt; .01 Deception 0 0 0 Sociability -0.97 -0.42 &lt; .01 Studying -0.10 0 &lt; .01 Argument 0.34 0.17 &lt; .01 Interacted -0.05 -0.14 &lt; .01 Lost something -0.14 0 &gt; -.01 Late 0.25 0.12 &lt; .01 Forgot something 0.55 0.17 &gt; -.01 Bored with schoolwork 0 0 0 Excited about schoolwork -0.15 0 0 Anxious about schoolwork 0.11 0 &gt; -.01 Tired -0.06 0 &lt; .01 Sick -0.22 0 &gt; -.01 Sleeping 0.78 0.44 &lt; .01 In Class -0.14 -0.14 &lt; .01 Listening to music 0.13 0 &lt; .01 On the internet 0.27 0 &gt; -.01 Watching TV -0.47 -0.17 &lt; .01 Time Monday 0 0 0 Tuesday 0 0 0 Wednesday 0 0 0 Thursday 0 0 0 Friday 0 0 0 Saturday 0 0 0 Sunday 0 0 0 Morning 0 0 0 Midday 0 0 0 Evening 0 0 0 Night 0 0 0 Linear Trend 0 0 0 Quadratic Trend 0 0 0 Cubic Trend 0 0 0 24 hour Sinusoidal Cycle 0 0 0 12 hour Sinusoidal Cycle 0 0 0 24 hour Cosinusoidal Cycle 0 0 0 12 hour Cosinusoidal Cycle 0 0 0 6.5.1.2 Figure Now we’ll create some figures that display the same information but make relative comparisons easier. coef_plot_fun &lt;- function(d, outcome, gr, SID, model){ o &lt;- mapvalues(outcome, outcomes$trait, outcomes$long_name, warn_missing = F) mod &lt;- mapvalues(model, c(&quot;glmnet&quot;, &quot;biscwit&quot;, &quot;rf&quot;), c(&quot;Elastic Net&quot;, &quot;BISCWIT&quot;, &quot;Random Forest&quot;), warn_missing = F) ttl &lt;- sprintf(&quot;Best %s Model (%s) Predicting \\n%s for Participant %s&quot;, mod, gr, o, SID) d &lt;- d %&gt;% mutate(coef = abs(coef)) # Set a number of &#39;empty bar&#39; to add at the end of each group empty_bar &lt;- 4 to_add &lt;- data.frame(matrix(NA, empty_bar*nlevels(factor(d$category)), ncol(d)) ) colnames(to_add) &lt;- colnames(d) to_add$category &lt;- rep(levels(factor(d$category)), each=empty_bar) d &lt;- rbind(d, to_add) d &lt;- d %&gt;% arrange(category, desc(coef)) d$id &lt;- seq(1, nrow(d)) breaks &lt;- round(seq(0, max(d$coef, na.rm = T), length.out = 5),2) rng &lt;- c(-1*(breaks[5]-.01), breaks[5]+.01) # Get the name and the y position of each label label_data &lt;- d number_of_bar &lt;- nrow(label_data) angle &lt;- 90 - 360 * (label_data$id-0.5) /number_of_bar # I substract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0) label_data$hjust &lt;- ifelse( angle &lt; -90, 1, 0) label_data$angle &lt;- ifelse(angle &lt; -90, angle+180, angle) label_data &lt;- label_data %&gt;% mutate(y = ifelse(is.na(coef) | coef &lt; 0, 0, coef + rng[2]/20) , lab = ifelse(!is.na(coef) &amp; coef &gt; 0, str_wrap(`short name`, 20), `short name`)) rng &lt;- c(round(-1*max(label_data$y),2)-.01, round(max(label_data$y),2)+.01) # prepare a data frame for base lines base_data &lt;- d %&gt;% group_by(category) %&gt;% summarize(start=min(id), end=max(id) - empty_bar) %&gt;% rowwise() %&gt;% mutate(title=mean(c(start, end))) breaks &lt;- breaks[1:4] # rng &lt;- c(-1*breaks[4], breaks[4]) # prepare a data frame for grid (scales) grid_data &lt;- base_data grid_data$end &lt;- grid_data$end[ c( nrow(grid_data), 1:nrow(grid_data)-1)] + 1 grid_data$start &lt;- grid_data$start - 1 grid_data &lt;- grid_data[-1,] # grid_data &lt;- grid_data %&gt;% crossing(breaks) p &lt;- d %&gt;% ggplot(aes( x = as.factor(id) , y = coef , fill = category )) + geom_bar(aes(na.rm = F), stat=&quot;identity&quot;, alpha=0.5) + scale_x_discrete(drop=FALSE) + scale_fill_manual( values = c(&quot;deepskyblue4&quot;, &quot;seagreen3&quot;, &quot;lightgoldenrod1&quot;) , drop = F ) + # Add a val=100/75/50/25 lines. I do it at the beginning to make sur barplots are OVER it. geom_segment(data = grid_data , aes(x = end, y = breaks[1], xend = start, yend = breaks[1]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + geom_segment(data = grid_data , aes(x = end, y = breaks[2], xend = start, yend = breaks[2]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + geom_segment(data = grid_data , aes(x = end, y = breaks[3], xend = start, yend = breaks[3]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + geom_segment(data = grid_data , aes(x = end, y = breaks[4], xend = start, yend = breaks[4]) , colour = &quot;grey&quot; , alpha=1 , size=0.3 , inherit.aes = FALSE ) + # # Add text showing the value of each 100/75/50/25 lines annotate(&quot;text&quot; , x = rep(max(d$id),4) , y = breaks , label = paste0(breaks, &quot;-&quot;) , color=&quot;grey&quot; , size=3 , angle=0 , fontface=&quot;bold&quot; , hjust=1) + ylim(rng[1], rng[2]) + theme_minimal() + theme( # legend.position = &quot;none&quot;, legend.position = &quot;bottom&quot;, axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank(), plot.title = element_text(hjust = .5, size = rel(1)) # plot.margin = unit(rep(-1,4), &quot;cm&quot;) ) + coord_polar() + labs(fill = &quot;Feature Category&quot;, title = ttl) + guides(fill = guide_legend(title.position=&quot;top&quot;, title.hjust = .5, label.hjust = 0)) + geom_text(data = label_data , aes(x = id , y = y#coef , label = lab , hjust = hjust) , color=&quot;black&quot; , fontface=&quot;bold&quot; , lineheight = .6 , alpha=0.6 , size=2.5 , angle= label_data$angle , inherit.aes = FALSE) + # Add base line information geom_segment(data = base_data , aes(x = start , y = rng[1]/20 , xend = end , yend = rng[1]/20) , colour = &quot;black&quot; , alpha=0.8 , size=0.6 , inherit.aes = FALSE ) ggsave(p, filename = sprintf(&quot;%s/05-results/05-figures/02-participant-coef/%s/%s_%s.pdf&quot; , res_path, model, SID, outcome) , width = 6, height = 8) ggsave(p, filename = sprintf(&quot;%s/05-results/05-figures/02-participant-coef/%s/png/%s_%s.png&quot; , res_path, model, SID, outcome) , width = 6, height = 8) return(p) } px_coef_fig &lt;- param_res %&gt;% right_join(best_mods %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% select(model:time)) %&gt;% select(-params) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;))) ) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;), group = sprintf(&quot;%s, %s&quot;, str_to_title(group), str_to_title(time))) %&gt;% select(-set, -time) %&gt;% group_by(model, SID, outcome, group, Variable) %&gt;% summarize(coef = mean(coef)) %&gt;% ungroup() %&gt;% group_by(model, outcome, SID, group) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(data = map(data, ~(.) %&gt;% full_join(ftrs %&gt;% select(category = group, Variable = old_name, `short name`) ) %&gt;% mutate(coef = ifelse(coef == 0, NA_real_, coef) , category = factor(str_to_title(category)))), p = pmap(list(data, outcome, group, SID, model) , possibly(coef_plot_fun, NA_real_))) 6.5.1.2.1 Participant 169 (Figure 5) (px_coef_fig %&gt;% filter(outcome == &quot;prcrst&quot; &amp; SID == &quot;169&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-5-px-169.pdf&quot;, res_path) , width = 6, height = 8) 6.5.1.2.2 Participant 43 (Figure 6) (px_coef_fig %&gt;% filter(outcome == &quot;lonely&quot; &amp; SID == &quot;43&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-6-px-43.pdf&quot;, res_path) , width = 6, height = 8) 6.5.1.2.3 Participant 160 (Figure 7) (px_coef_fig %&gt;% filter(outcome == &quot;prcrst&quot; &amp; SID == &quot;160&quot; &amp; model == &quot;glmnet&quot;))$p[[1]] ggsave(file = sprintf(&quot;%s/05-results/05-figures/fig-7-px-160.pdf&quot;, res_path) , width = 6, height = 8) 6.5.2 Profile Similarity procor_fun &lt;- function(d){ m &lt;- d %&gt;% select(-SID) %&gt;% mutate_all(~ifelse(is.na(.), 0, .)) %&gt;% as.matrix(); rownames(m) &lt;- d$SID r &lt;- cor(t(m)) diag(r) &lt;- NA rd &lt;- r %&gt;% data.frame() %&gt;% rownames_to_column(&quot;SID1&quot;) %&gt;% pivot_longer(cols = -SID1 , names_to = &quot;SID2&quot; , values_to = &quot;r&quot; , values_drop_na = T) %&gt;% mutate(SID2 = str_remove_all(SID2, &quot;X&quot;)) r %&gt;% mutate(r = fisherz(r)) %&gt;% group_by(SID1) %&gt;% summarize_at(vars(r), lst(mean, min, max)) } profile_sim &lt;- param_res %&gt;% select(-params) %&gt;% right_join(best_mods %&gt;% select(model:.metric)) %&gt;% filter(map_lgl(coefs, is.null) == F) %&gt;% mutate(coefs = map(coefs, ~(.) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;Variable&quot;) %&gt;% setNames(c(&quot;Variable&quot;, &quot;coef&quot;))) ) %&gt;% unnest(coefs) %&gt;% mutate(Variable = str_remove_all(Variable, &quot;_X1&quot;), Variable = str_remove_all(Variable, &quot;_1&quot;), Variable = str_remove_all(Variable, &quot;_2&quot;), Variable = str_replace_all(Variable, &quot;[.]&quot;, &quot;_&quot;)) %&gt;% filter(Variable %in% ftrs$old_name) %&gt;% group_by(model, SID, outcome, .metric, Variable) %&gt;% summarize(coef = mean(coef)) %&gt;% ungroup() profile_sim %&gt;% left_join(ftrs %&gt;% select(group, Variable = old_name, new_name)) %&gt;% filter(model == &quot;glmnet&quot; &amp; .metric == &quot;accuracy&quot; &amp; outcome == &quot;prcrst&quot;) %&gt;% mutate(coef = ifelse(coef == 0, NA, coef) , coef = ifelse(coef &gt; 5, 5, ifelse(coef &lt; -5, -5, coef)) , group = str_to_title(group) , new_name = factor(new_name, rev(ftrs$new_name))) %&gt;% drop_na() %&gt;% ggplot(aes(x = SID, y = new_name, color = coef)) + scale_color_gradient2(low = &quot;blue&quot; , mid = &quot;white&quot; , high = &quot;red&quot; # , limits = c(-5,5) ) + geom_point() + labs(x = &quot;Participant ID&quot;, y = NULL, color = &quot;Coefficient&quot;) + facet_grid(group ~ ., space = &quot;free&quot;, scale = &quot;free&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 90, face = &quot;bold&quot;) , axis.text.y = element_text(face = &quot;bold&quot;) , legend.position = &quot;bottom&quot; , strip.background = element_rect(fill = &quot;black&quot;) , strip.text = element_text(face = &quot;bold&quot;, color = &quot;white&quot;, size = rel(1.2))) "],["references.html", "References", " References "]]
