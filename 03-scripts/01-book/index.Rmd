--- 
title: "Idiographic prediction of loneliness and procrastination"
author: "Emorie D Beck"
institution: "Northwestern University Feinberg School of Medicine"
date: "`r Sys.Date()`"
site: 
  bookdown::bookdown_site:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    numbering: false
    number_sections: false
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "A longstanding goal of psychology is to predict the things people do, but tools to predict accurately future behaviors remain elusive. In the present study, we used intensive longitudinal data (N = 104; total assessments = 5,971) and three machine learning approaches to investigate the degree to which two behaviors – loneliness and procrastination – could be predicted from psychological (i.e. personality and affective states), situational (i.e. objective situations and psychological situation cues), and time (i.e. trends, diurnal cycles, time of day, and day of the week) phenomena from an idiographic, person-centered perspective. Rather than pitting persons against situations, such an approach allows psychological phenomena, situations, and time to jointly inform prediction. We find (1) a striking degree of accuracy across participants, (2) that a majority of participants models are best informed by both person and situation features, and (3) that the most important features vary greatly across people."
---


# Workspace  
## Packages  

```{r, echo = F}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, error = F)
options(knitr.kable.NA = '')
```


```{r packages}
library(knitr)              # creating tables
library(kableExtra)         # formatting and exporting tables
library(rio)                # importing html
library(readxl)             # read excel codebooks and documentation
library(psych)              # biscuit / biscwit
library(glmnet)             # elastic net regression
library(glmnetUtils)        # extension of basic elastic net with CV
library(caret)              # train and test for random forest
library(vip)                # variable importance
library(Amelia)             # multiple imputation (of time series)
library(lubridate)          # date wrangling
library(gtable)             # ggplot friendly tables
library(grid)               # ggplot friendly table rendering 
library(gridExtra)          # more helpful ggplot friendly table updates
library(plyr)               # data wranging
library(tidyverse)          # data wrangling
library(ggdist)             # distributional plots 
library(ggridges)           # more distributional plots 
library(cowplot)            # flexibly arrange multiple ggplot objects
library(tidymodels)         # tidy model workflow and selection
# library(modeltime)          # tidy models for time series
library(furrr)              # mapping many models in parallel 
```


## Directory Path  
```{r path}
res_path <- "https://github.com/emoriebeck/behavior-prediction/raw/main"
local_path <- "~/Box/network/other projects/idio prediction"
```

## Codebook  
Each study has a separate codebook indexing matching, covariate, personality, and outcome variables. Moreover, these codebooks contain information about the original scale of the variable, any recoding of the variable (including binarizing outcomes, changing the scale, and removing missing data), reverse coding of scale variables, categories, etc.  
```{r codebook}
# list of all codebook sheets
ipcs_codebook <- import(file = sprintf("%s/01-codebooks/codebook.xlsx", res_path), which = 2) %>%
  as_tibble()
ipcs_codebook

outcomes <- ipcs_codebook %>% filter(category == "outcome") %>% select(trait, long_name)

ftrs <- import(file = sprintf("%s/01-codebooks/codebook.xlsx", res_path), which = 3) %>%
  as_tibble()
```

### Measures  
Participants responded to a large battery of trait and ESM measures as part of the larger study. The present study focuses on ESM measures whose use we preregistered. A full list of the collected measures for the study can be found in supplementary codebooks in the online materials on the OSF and GitHub. The measures collected at each wave were identical. ESM measures were used to estimate idiographic personality prediction models.  

#### ESM Measures  
##### Personality  
Personality was assessed using the full BFI-2 (Soto & John, 2017). The scale was administered using a planned missing data design (Revelle et al., 2016). We have previously demonstrated both the between- and within-person construct validity of assessing personality using planned missing designs using the BFI-2 (https://osf.io/pj9sy/). The planned missingness was done within each Big Five trait separately, with three items from each trait included at each timepoint (75% missingness). Each item was answered relative to what a participant was just doing on a 5-point Likert-like scale from 1 "disagree strongly" to 5 "agree strongly." Items for each person at each assessment were determined by pulling 3 numbers (1 to 12) from a uniform distribution. The order of the resulting 15 items were then randomized before being displayed to participants.  
```{r}
ipcs_codebook %>% filter(category == "BFI-2")
```


##### Affect  
Items capturing affect were initially pulled from the PANAS-X (Watson & Clark, 1994). In order to reduce redundancy, these were cross-referenced with the BFI-2 and duplicated items (e.g., "excited" were only asked once. Because we were not interested in scale score but in items, we further had research participants examine remaining items and asked them to indicate items that were not relevant to their experience. Finally, we added two "neutral" affect-related terms – goal-directed and purposeful. Each of these were rated on a 1 "disagree strongly" to 5 "agree strongly."  

```{r}
ipcs_codebook %>% filter(category == "Affect")
```

##### Binary Situations  
Binary situation indicators were derived by asking undergraduate research assistants to provide list of the common social, academic, and personal situations in which they tended to find themselves. From these, we derived a list of 19 unique situations. Separate items for arguing with or interacting with friends or relatives were composited in overall argument and interaction items. Participants checked a box for each event that occurred in the last hour (1 = occurred, 0 = did not occur).  

```{r}
ipcs_codebook %>% filter(category == "sit")
```

##### DIAMONDS Situation Features  
Psychological features of situations were measured using the ultra brief version of the "Situational Eight" DIAMONDS (Duty, Intellect, Adversity, Mating, pOsitivity, Negativity, Deception, and Sociality) scale (S8-I; Rauthmann  & Sherman, 2015). Items were measured on a 3-point scale from 1 "not at all" to 3 "totally."  

```{r}
ipcs_codebook %>% filter(category == "S8-I")
```

##### Timing Features  
The final set of features were created from the time stamps collected with each survey based on approaches used in other studies of idiographic prediction (Fisher & Soyster, 2019; . To create these, we created time of day (4; morning, midday, evening, night) and day of the week dummy codes (7). Next, we create a cumulative time variable (in hours) from first beep (not used in analyses) that we used to create linear, quadratic, and cubic time trends (3) as well as 1 and 2 period sine and cosine functions across each 24 period (e.g., 2 period sine = \sin{\frac{2\pi}{12}}\ast\ {cumulative\ time}_t and 1 period sine = \sin{\frac{2\pi}{24}}\ast\ {cumulative\ time}_t).  

### Procedure  
Participants in this study were drawn from a larger personality study. All responded to two types of surveys: trait and state (Experience Sampling Method; ESM) measures, for which they were paid separately. Participants completed three waves of trait measures and two waves of state measures. For the first two waves, trait surveys were collected immediately before beginning the ESM protocol.  

#### Main Sample  
For the main sample, participants were recruited from the psychology subject pool at Washington University in St. Louis. Participants were told that the study posted on the recruitment website was the first wave of a longer longitudinal study they would be offered the opportunity to take part in.  

Participants were brought into the lab between October 2018 and December 2019, where a research assistant or the first author explained the study procedure to them and walked them through the consent procedure. If they consented, participants were led to a room where they could fill out a form to opt into the ESM portion of the study. They then completed baseline trait measures using the Qualtrics Survey Platform. After, the participants were debriefed, paid $10 in cash and, if they opted into the ESM portion of the study, the ESM survey procedure was explained to them.  

Participants then received ESM surveys four times per day for two weeks (target n = 56). The survey platform was built by the first author using the jsPsych library (De Leeuw, 2015). Additional JavaScript controllers were written for the purpose of this study and are available on the first author's GitHub. Start times were based on times that participants indicated they would like to receive their first survey based on their personal wake times. Surveys were sent every 4 hours, meaning that the surveys spanned a 12-hour period from the start time participants indicated. Participants received their first survey at their chosen time on the Monday following their in-lab session. They were compensated $.50 for each survey completed for a maximum of $28. To incentivize responding, participants who completed at least 50 surveys received a "bonus" for a total compensation of $30, which was distributed as an Amazon Gift Card.  

### Analytic Plan  

The present study tested three methods of machine learning classification models, some of which have been used for idiographic prediction in other studies (Fisher & Soyster, 2019; Kaiser & Butter, 2020): (1) Elastic Net Regression (Friedman, Hastie, & Tibshirani, 2010), (2) The Best Items Scale that is Cross-validated, Correlation-weighted, Informative and Transparent (BISCWIT; Elleman, McDougald, Condon, & Revelle, 2020), and (3) Random Forest Models (Kim et al., 2019).  

Because we have a large number of indicators to test, each of the methods used have variable selection features and, in some instances, other methods for reducing overfitting, as detailed below. To both reduce the number of indicators used in each test and to test which group of indicators are the most predictive of procrastination and loneliness, we will also test these in several sets: (1) Personality indicators (15), (2) Affective indicators (10), (3) Binary situation indicators (16), (4) DIAMONDS situation indicators (8), (5) Psychological indicators (personality + affect) (25), (6), Situation indicators (binary + DIAMONDS) (24), and (7) Full set (personality + affect + binary situations + DIAMONDS) (49). We will additionally test each of these with and without the 18 timing indicators, for a total set of 14 combinations of the 67 features.  

In each of these methods, we used cumulative rolling origin forecast validation,  which was comprised of the first 75% of the time series, and held out the remaining 25% of the data set for the test set. In the rolling origin forecast validation, we used the first one-third of the time series as the initial set, five observations as the validation set, and set skip to one, which roughly resulted in 10-15 rolling origin “folds.”  

Out of sample prediction was tested based on classification error and area under the ROC (receive operating characteristic) curve (AUC). Classification error is a simple estimate of the percentage of the test sample that was correctly classified by the model. In addition, the AUC will capture the trade-off between sensitivity and specificity across a threshold. In the present study, we used an AUC threshold of .5, which indicates binary classification at chance levels. ROC visualizations plot 1 - specificity (i.e. false positive rate: false positives / (false positives + true negatives)) against sensitivity (i.e. true positive rate: true positives / (true positives + false positives)).

## Demographics  

#### Trait  
```{r trait data combine, eval = F}
participants <- googlesheets4::sheets_read("https://docs.google.com/spreadsheets/d/1r808gQ-LWfG98J9rvt_CRMHtmCFgtdcfThl0XA0HHbM/edit?usp=sharing", sheet = "ESM") %>%
  select(SID, Name, Email) %>%
  mutate(new = seq(1, n(), 1),
         new = ifelse(new < 10, paste("0", new, sep = ""), new))
1

old_names <- trait_codebook$`New #`

# wave 1 trait
baseline <- sprintf("%s/04-data/01-raw-data/baseline_05.07.20.csv", res_path) %>% 
  read_csv() %>%
  filter(!row_number() %in% c(1,2) & !is.na(SID) & SID %in% participants$SID) %>% 
  select(SID, StartDate, gender, YOB, race, ethnicity) %>%
  mutate(SID = mapvalues(SID, participants$SID, participants$new)) %>%
  mutate(wave = 1,
         gender = factor(gender, c(1,2), c("Male", "Female")),
         YOB = substr(YOB, nchar(YOB)-4+1, nchar(YOB)),
         race = mapvalues(race, 1:7, c(0,1,3,2,3,3,3)),
         ethnicity = ifelse(!is.na(ethnicity), 3, NA),
         race = ifelse(is.na(ethnicity), race, ifelse(ethnicity == 3, ethnicity)))  %>%
  select(-ethnicity)

save(baseline, 
     file = sprintf("%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData", res_path))
```

```{r}
load(url(sprintf("%s/04-data/01-raw-data/cleaned_combined_2020-05-06.RData", res_path)))
dem <- baseline %>%
  select(SID:race) %>%
  mutate(age = year(ymd_hms(StartDate)) - as.numeric(YOB),
         StartDate = as.Date(ymd_hms(StartDate)),
         race = factor(race, 0:3, c("White", "Black", "Asian", "Other"))) %>%
  select(-YOB) 

dem %>% 
  summarize(n = length(unique(SID)),
            gender = sprintf("%i (%.2f%%)",sum(gender == "Female"), sum(gender == "Female")/n()*100),
            age = sprintf("%.2f (%.2f)", mean(age, na.rm = T), sd(age, na.rm = T)),
            white = sprintf("%i (%.2f%%)"
                            , sum(race == "White", na.rm = T)
                            , sum(race == "White", na.rm = T)/n()*100),
            black = sprintf("%i (%.2f%%)"
                            , sum(race == "Black", na.rm = T)
                            , sum(race == "Black", na.rm = T)/n()*100),
            asian = sprintf("%i (%.2f%%)"
                            , sum(race == "Asian", na.rm = T)
                            , sum(race == "Asian", na.rm = T)/n()*100),
            other = sprintf("%i (%.2f%%)"
                            , sum(race == "Other", na.rm = T)
                            , sum(race == "Other", na.rm = T)/n()*100),
            StartDate = sprintf("%s (%s - %s)", median(StartDate), 
                                min(StartDate), max(StartDate)))

dem %>%
  kable(., "html"
        , col.names = c("ID", "Start Date", "Gender", "Race/Ethnicity", "Age")
        , align = rep("c", 5)
        , caption = "<strong>Table S1</strong><br><em>Descriptive Statistics of Participants at Baseline<em>") %>%
  kable_styling(full_width = F) %>%
    scroll_box(height = "900px")
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```